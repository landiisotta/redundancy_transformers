{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/landii03/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_likelihood = outputs[0] * trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1621.4019)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1523.1713), tensor(992.5504), tensor(1621.4019)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287644"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 287644])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([idx for idx in target_ids[0] if int(idx) != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1523.1713), tensor(992.5504), tensor(1621.4019)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/562 [00:57<2:57:22, 19.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-96ca4d1d5d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mneg_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    791\u001b[0m                 )\n\u001b[1;32m    792\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "\n",
    "nlls = []\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "    trg_len = end_loc - i    # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:,begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:,:-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: n2c2_dataset/language_model\n",
      "Reusing dataset n2c2_dataset (/Users/landii03/.cache/huggingface/datasets/n2c2_dataset/language_model/0.0.1/34ccee62d89a9c090d608a6043eea469e8600904f5c66bad48e60cfd8122b379)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0a99ce99be49ceb505df99101bff18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from n2c2_dataset import N2c2Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForPreTraining, BertTokenizer, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "from datasets import Dataset, load_dataset\n",
    "import pickle as pkl\n",
    "import utils as ut\n",
    "\n",
    "mydata = load_dataset('./datasets/n2c2_datasets')\n",
    "MaskedLmInstance = namedtuple(\"MaskedLmInstance\",\n",
    "                              [\"index\", \"label\"])\n",
    "checkpoint = ut.checkpoint\n",
    "\n",
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokens,\n",
    "                 segment_ids,\n",
    "                 masked_lm_positions,\n",
    "                 masked_lm_labels,\n",
    "                 is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"tokens: %s\\n\" % (\" \".join(\n",
    "            [x for x in self.tokens]))\n",
    "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "            [str(x) for x in self.masked_lm_positions]))\n",
    "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "            [x for x in self.masked_lm_labels]))\n",
    "        s += \"\\n\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "rng = random.Random(42)\n",
    "\n",
    "input_dataset=mydata['train']\n",
    "max_seq_length=32\n",
    "dupe_factor=1\n",
    "short_seq_prob=0.1\n",
    "masked_lm_prob=0.15 \n",
    "max_predictions_per_seq=1\n",
    "\n",
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "documents = {}\n",
    "# Iterate over Dataset objects\n",
    "for el in input_dataset:\n",
    "    documents.setdefault(el['document'], list()).append(el['sentence'])\n",
    "instances = []\n",
    "\n",
    "# rng.shuffle(documents.items())\n",
    "# for _ in range(dupe_factor):\n",
    "#     for idx in documents.keys():\n",
    "#         instances.extend(\n",
    "#             create_instances_from_document(\n",
    "#                 documents, idx, max_seq_length, short_seq_prob,\n",
    "#                 masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "# rng.shuffle(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for [CLS], [SEP], [SEP]\n",
    "max_num_tokens = max_seq_length - 3\n",
    "\n",
    "# We *usually* want to fill up the entire sequence since we are padding\n",
    "# to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "# computation. However, we *sometimes*\n",
    "# (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "# sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "# The `target_seq_length` is just a rough target however, whereas\n",
    "# `max_seq_length` is a hard limit.\n",
    "target_seq_length = max_num_tokens\n",
    "if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We DON'T just concatenate all of the tokens from a document into a long\n",
    "# sequence and choose an arbitrary split point because this would make the\n",
    "# next sentence prediction task too easy. Instead, we split the input into\n",
    "# segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "# input.\n",
    "instances = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "all_idx = list(documents.keys())\n",
    "idx = '757'\n",
    "# print(all_idx)\n",
    "i = 0\n",
    "document = documents[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['641', '643', '681', '704', '757'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        print(total_length)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "        print(\"TRUNCATING\")\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        print(trunc_tokens)\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()\n",
    "            \n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)\n",
    "\n",
    "    output_tokens = list(tokens)\n",
    "\n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if index in covered_indexes:\n",
    "            continue\n",
    "        covered_indexes.add(index)\n",
    "        # BERT default percentages\n",
    "        masked_token = None\n",
    "        # 80% of the time, replace with [MASK]\n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = \"[MASK]\"\n",
    "        else:\n",
    "            # 10% of the time, keep original\n",
    "            if rng.random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "            # 10% of the time, replace with random word\n",
    "            else:\n",
    "                masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "        output_tokens[index] = masked_token\n",
    "\n",
    "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "\n",
    "    return output_tokens, masked_lm_positions, masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "TRUNCATING\n",
      "['301443520', 'ctmc', '49020928', '448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina', 'in', '1986']\n",
      "56\n",
      "TRUNCATING\n",
      "['ctmc', '49020928', '448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina', 'in', '1986']\n",
      "55\n",
      "TRUNCATING\n",
      "['49020928', '448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina', 'in', '1986']\n",
      "54\n",
      "TRUNCATING\n",
      "['448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina', 'in', '1986']\n",
      "53\n",
      "TRUNCATING\n",
      "['448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina', 'in']\n",
      "52\n",
      "TRUNCATING\n",
      "['448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent', 'angina']\n",
      "51\n",
      "TRUNCATING\n",
      "['448922', '1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent']\n",
      "50\n",
      "TRUNCATING\n",
      "['1/11/1990', 'am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent']\n",
      "49\n",
      "TRUNCATING\n",
      "['am', 'discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent']\n",
      "48\n",
      "TRUNCATING\n",
      "['discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with', 'recurrent']\n",
      "47\n",
      "TRUNCATING\n",
      "['discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984', 'with']\n",
      "46\n",
      "TRUNCATING\n",
      "['discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi', '1984']\n",
      "45\n",
      "TRUNCATING\n",
      "['discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post', 'mi']\n",
      "44\n",
      "TRUNCATING\n",
      "['discharge', 'summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post']\n",
      "43\n",
      "TRUNCATING\n",
      "['summary', 'unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post']\n",
      "42\n",
      "TRUNCATING\n",
      "['unsigned', 'dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post']\n",
      "41\n",
      "TRUNCATING\n",
      "['dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status', 'post']\n",
      "40\n",
      "TRUNCATING\n",
      "['dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt', 'status']\n",
      "39\n",
      "TRUNCATING\n",
      "['dis', 'admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt']\n",
      "38\n",
      "TRUNCATING\n",
      "['admission', 'date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt']\n",
      "37\n",
      "TRUNCATING\n",
      "['date', '01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt']\n",
      "36\n",
      "TRUNCATING\n",
      "['01/11/1990', 'report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt']\n",
      "35\n",
      "TRUNCATING\n",
      "['report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', 'dvt']\n",
      "34\n",
      "TRUNCATING\n",
      "['report', 'status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral']\n",
      "33\n",
      "TRUNCATING\n",
      "['status', 'unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral']\n",
      "32\n",
      "TRUNCATING\n",
      "['unsigned', 'discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral']\n",
      "31\n",
      "TRUNCATING\n",
      "['discharge', 'date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral']\n",
      "30\n",
      "TRUNCATING\n",
      "['date', '01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral']\n",
      "29\n",
      "['01/22/1990', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral'] ['he', 'was', 'edentulous']\n",
      "tokens: [CLS] [MASK] history of present illness this is the second stamgibonnole hospital admission for this 50 year old woman with a history of hypertension crest syndrome bilateral [SEP] he was edentulous [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 1\n",
      "masked_lm_labels: 01/22/1990\n",
      "\n",
      "\n",
      "HERE\n",
      "44\n",
      "TRUNCATING\n",
      "['catheterization', 'report', 'in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of', 'angina', 'with', 'exertion']\n",
      "43\n",
      "TRUNCATING\n",
      "['catheterization', 'report', 'in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of', 'angina', 'with']\n",
      "42\n",
      "TRUNCATING\n",
      "['report', 'in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of', 'angina', 'with']\n",
      "41\n",
      "TRUNCATING\n",
      "['report', 'in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of', 'angina']\n",
      "40\n",
      "TRUNCATING\n",
      "['report', 'in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of']\n",
      "39\n",
      "TRUNCATING\n",
      "['in', '1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of']\n",
      "38\n",
      "TRUNCATING\n",
      "['1988', 'showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of']\n",
      "37\n",
      "TRUNCATING\n",
      "['showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency', 'of']\n",
      "36\n",
      "TRUNCATING\n",
      "['she', 'was', 'referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent', 'of', 'disease']\n",
      "35\n",
      "TRUNCATING\n",
      "['showed', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency']\n",
      "34\n",
      "TRUNCATING\n",
      "['she', 'was', 'referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent', 'of']\n",
      "33\n",
      "TRUNCATING\n",
      "['subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing', 'frequency']\n",
      "32\n",
      "TRUNCATING\n",
      "['was', 'referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent', 'of']\n",
      "31\n",
      "TRUNCATING\n",
      "['subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', 'increasing']\n",
      "30\n",
      "TRUNCATING\n",
      "['referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent', 'of']\n",
      "29\n",
      "['subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had'] ['referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent']\n",
      "tokens: [CLS] subtotal occlusion of the rca with a high grade lad lesion the patient has had [SEP] referred for laser angiography in 11/89 but it was not done secondasry to extent [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 10\n",
      "masked_lm_labels: lad\n",
      "\n",
      "\n",
      "HERE\n",
      "33\n",
      "TRUNCATING\n",
      "['cath', 'report', 'showed', '50%', 'ulcerated', 'lad', '100%', 'circumflex', '100%', 'rca', 'inferior', 'posterior', 'focal', 'inferior', 'apical', 'o.k.', 'ef', '49%']\n",
      "32\n",
      "TRUNCATING\n",
      "['report', 'showed', '50%', 'ulcerated', 'lad', '100%', 'circumflex', '100%', 'rca', 'inferior', 'posterior', 'focal', 'inferior', 'apical', 'o.k.', 'ef', '49%']\n",
      "31\n",
      "TRUNCATING\n",
      "['showed', '50%', 'ulcerated', 'lad', '100%', 'circumflex', '100%', 'rca', 'inferior', 'posterior', 'focal', 'inferior', 'apical', 'o.k.', 'ef', '49%']\n",
      "30\n",
      "TRUNCATING\n",
      "['past', 'medical', 'history', 'significant', 'for', 'as', 'above', 'hypertension', 'crest', 'scleroderma', 'reynaud', 'esophagitis', 'telangectasia', 'and', 'calcinosis']\n",
      "29\n",
      "['50%', 'ulcerated', 'lad', '100%', 'circumflex', '100%', 'rca', 'inferior', 'posterior', 'focal', 'inferior', 'apical', 'o.k.', 'ef', '49%'] ['past', 'medical', 'history', 'significant', 'for', 'as', 'above', 'hypertension', 'crest', 'scleroderma', 'reynaud', 'esophagitis', 'telangectasia', 'and']\n",
      "tokens: [CLS] 50% ulcerated lad 100% circumflex 100% rca inferior posterior focal inferior apical o.k. ef 49% [SEP] past medical history [MASK] for as above hypertension crest scleroderma reynaud esophagitis telangectasia and [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 20\n",
      "masked_lm_labels: significant\n",
      "\n",
      "\n",
      "HERE\n",
      "31\n",
      "TRUNCATING\n",
      "['she', 'has', 'a', 'history', 'of', 'lymphangitis', 'left', 'arm', 'history', 'of', 'migraine', 'history', 'of', 'left', 'shoulder', 'bursitis']\n",
      "30\n",
      "TRUNCATING\n",
      "['has', 'a', 'history', 'of', 'lymphangitis', 'left', 'arm', 'history', 'of', 'migraine', 'history', 'of', 'left', 'shoulder', 'bursitis']\n",
      "29\n",
      "['she', 'is', 'status', 'post', 'bilateral', 'dvt', 'in', '1960', 'and', '1970', 'treated', 'with', 'heparin', 'and', 'coumadin'] ['a', 'history', 'of', 'lymphangitis', 'left', 'arm', 'history', 'of', 'migraine', 'history', 'of', 'left', 'shoulder', 'bursitis']\n",
      "tokens: [CLS] [MASK] is status post bilateral dvt in 1960 and 1970 treated with heparin and coumadin [SEP] a history of lymphangitis left arm history of migraine history of left shoulder bursitis [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 1\n",
      "masked_lm_labels: she\n",
      "\n",
      "\n",
      "HERE\n",
      "48\n",
      "TRUNCATING\n",
      "['at', 'the', 'time', 'she', 'also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal', 'region']\n",
      "47\n",
      "TRUNCATING\n",
      "['the', 'time', 'she', 'also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal', 'region']\n",
      "46\n",
      "TRUNCATING\n",
      "['the', 'time', 'she', 'also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal']\n",
      "45\n",
      "TRUNCATING\n",
      "['time', 'she', 'also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal']\n",
      "44\n",
      "TRUNCATING\n",
      "['she', 'also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal']\n",
      "43\n",
      "TRUNCATING\n",
      "['also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the', 'peritracheal']\n",
      "42\n",
      "TRUNCATING\n",
      "['also', 'had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the']\n",
      "41\n",
      "TRUNCATING\n",
      "['had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in', 'the']\n",
      "40\n",
      "TRUNCATING\n",
      "['had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy', 'in']\n",
      "39\n",
      "TRUNCATING\n",
      "['had', 'symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy']\n",
      "38\n",
      "TRUNCATING\n",
      "['symptoms', 'of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy']\n",
      "37\n",
      "TRUNCATING\n",
      "['of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive', 'lymphadenopathy']\n",
      "36\n",
      "TRUNCATING\n",
      "['of', 'hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive']\n",
      "35\n",
      "TRUNCATING\n",
      "['hemoptysis', 'on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive']\n",
      "34\n",
      "TRUNCATING\n",
      "['on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and', 'extensive']\n",
      "33\n",
      "TRUNCATING\n",
      "['on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass', 'and']\n",
      "32\n",
      "TRUNCATING\n",
      "['on', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass']\n",
      "31\n",
      "TRUNCATING\n",
      "['november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe', 'mass']\n",
      "30\n",
      "TRUNCATING\n",
      "['november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', 'lobe']\n",
      "29\n",
      "['she', 'has', 'never', 'been', 'on', 'steroids', 'past', 'surgical', 'history', 'status', 'post', 'tubal', 'ligation', 'appendectomy'] ['november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper']\n",
      "tokens: [CLS] she has never been on steroids past surgical history status post tubal ligation appendectomy [SEP] november 26 a chest ct revealed multiple bilateral cysts and bilateral nodules a right upper [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 6\n",
      "masked_lm_labels: steroids\n",
      "\n",
      "\n",
      "HERE\n",
      "36\n",
      "TRUNCATING\n",
      "['he', 'lived', 'alone', 'but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care', 'and', 'deconditioning']\n",
      "35\n",
      "TRUNCATING\n",
      "['he', 'lived', 'alone', 'but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care', 'and']\n",
      "34\n",
      "TRUNCATING\n",
      "['he', 'lived', 'alone', 'but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "33\n",
      "TRUNCATING\n",
      "['lived', 'alone', 'but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "32\n",
      "TRUNCATING\n",
      "['alone', 'but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "31\n",
      "TRUNCATING\n",
      "['but', 'was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "30\n",
      "TRUNCATING\n",
      "['was', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "29\n",
      "['medications', 'dilt', '60', 'b.i.d.', 'lopressor', '50', 'b.i.d.', 'motrin', '800', 't.i.d.', 'decreased', 'from', 'q.i.d.', 'zantac', 'prn'] ['driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care']\n",
      "tokens: [CLS] medications dilt 60 b.i.d. lopressor 50 b.i.d. motrin 800 t.i.d. decreased from q.i.d. zantac prn [SEP] driven to the hospital by his son because of reported worsening and general care [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 11\n",
      "masked_lm_labels: decreased\n",
      "\n",
      "\n",
      "HERE\n",
      "35\n",
      "TRUNCATING\n",
      "['social', 'history', 'she', 'drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries', 'on', '1/12/90']\n",
      "34\n",
      "TRUNCATING\n",
      "['history', 'she', 'drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries', 'on', '1/12/90']\n",
      "33\n",
      "TRUNCATING\n",
      "['she', 'drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries', 'on', '1/12/90']\n",
      "32\n",
      "TRUNCATING\n",
      "['she', 'drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries', 'on']\n",
      "31\n",
      "TRUNCATING\n",
      "['drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries', 'on']\n",
      "30\n",
      "TRUNCATING\n",
      "['drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', 'arteries']\n",
      "29\n",
      "['allergies', 'are', 'morphine', 'causes', 'hives'] ['drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary']\n",
      "tokens: [CLS] allergies are morphine landing hives [SEP] drinks alcohol 2-3 times per week 1-2 packs per day hospital course the patient underwent coronary artery bypass grafting x 2 with bilateral mammary [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 4\n",
      "masked_lm_labels: causes\n",
      "\n",
      "\n",
      "HERE\n",
      "29\n",
      "['lma', 'to', 'rca', 'lma', 'to', 'lad', 'the', 'patient', 'was', 'without', 'operative', 'complications'] ['his', 'history', 'began', 'in', 'february', 'of', '1987', 'when', 'he', 'developed', 'streptococcal', 'endocarditis', 'secondary', 'to', 'intravenous', 'drug', 'abuse']\n",
      "tokens: [CLS] lma to rca lma to lad [MASK] patient was without operative complications [SEP] his history began in february of 1987 when he developed streptococcal endocarditis secondary to intravenous drug abuse [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 7\n",
      "masked_lm_labels: the\n",
      "\n",
      "\n",
      "HERE\n",
      "33\n",
      "TRUNCATING\n",
      "['she', 'was', 'extubated', 'on', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued', 'on', 'lopressor']\n",
      "32\n",
      "TRUNCATING\n",
      "['was', 'extubated', 'on', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued', 'on', 'lopressor']\n",
      "31\n",
      "TRUNCATING\n",
      "['was', 'extubated', 'on', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued', 'on']\n",
      "30\n",
      "TRUNCATING\n",
      "['was', 'extubated', 'on', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued']\n",
      "29\n",
      "['she', 'was', 'admitted', 'to', 'tng', 'for', 'blood', 'pressure', 'control'] ['extubated', 'on', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued']\n",
      "tokens: [CLS] she was admitted to tng for blood pressure control [SEP] extubated pen day 1 and diuresis was initiated she continued to do well she was taken off nifedipine and continued [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 12\n",
      "masked_lm_labels: on\n",
      "\n",
      "\n",
      "HERE\n",
      "33\n",
      "TRUNCATING\n",
      "['on', 'martin', 'luther', 'king', 'day', 'she', 'was', 'noted', 'to', 'be', 'in', 'and', 'out', 'of', 'atrial', 'fibrillation', 'without', 'symptoms']\n",
      "32\n",
      "TRUNCATING\n",
      "['on', 'martin', 'luther', 'king', 'day', 'she', 'was', 'noted', 'to', 'be', 'in', 'and', 'out', 'of', 'atrial', 'fibrillation', 'without']\n",
      "31\n",
      "TRUNCATING\n",
      "['on', 'martin', 'luther', 'king', 'day', 'she', 'was', 'noted', 'to', 'be', 'in', 'and', 'out', 'of', 'atrial', 'fibrillation']\n",
      "30\n",
      "TRUNCATING\n",
      "['he', 'was', 'transfused', 'one', 'unit', 'of', 'packed', 'red', 'blood', 'cells', 'for', 'a', 'hematocrit', 'of', '23']\n",
      "29\n",
      "['on', 'martin', 'luther', 'king', 'day', 'she', 'was', 'noted', 'to', 'be', 'in', 'and', 'out', 'of', 'atrial'] ['was', 'transfused', 'one', 'unit', 'of', 'packed', 'red', 'blood', 'cells', 'for', 'a', 'hematocrit', 'of', '23']\n",
      "tokens: [CLS] on martin luther king day she was noted to be in and out of atrial [SEP] was transfused one unit of packed red blood cells for a hematocrit of 23 [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 20\n",
      "masked_lm_labels: unit\n",
      "\n",
      "\n",
      "39\n",
      "TRUNCATING\n",
      "['she', 'was', 'continued', 'on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a', 'large', 'output']\n",
      "38\n",
      "TRUNCATING\n",
      "['was', 'continued', 'on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a', 'large', 'output']\n",
      "37\n",
      "TRUNCATING\n",
      "['continued', 'on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a', 'large', 'output']\n",
      "36\n",
      "TRUNCATING\n",
      "['continued', 'on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a', 'large']\n",
      "35\n",
      "TRUNCATING\n",
      "['on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a', 'large']\n",
      "34\n",
      "TRUNCATING\n",
      "['on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to', 'a']\n",
      "33\n",
      "TRUNCATING\n",
      "['on', 'lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to']\n",
      "32\n",
      "TRUNCATING\n",
      "['lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due', 'to']\n",
      "31\n",
      "TRUNCATING\n",
      "['lopressor', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due']\n",
      "30\n",
      "TRUNCATING\n",
      "['50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', 'due']\n",
      "29\n",
      "['50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', 'lopressor', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in'] ['he', 'did', 'well', 'but', 'continued', 'to', 'use', 'intravenous', 'drugs']\n",
      "tokens: [CLS] 50 b.i.d. it was recommended that her [MASK] dose be increased to 75 b.i.d. her chest tubes were kept in [SEP] he did well but continued to use intravenous drugs [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: True\n",
      "masked_lm_positions: 8\n",
      "masked_lm_labels: lopressor\n",
      "\n",
      "\n",
      "HERE\n",
      "24\n",
      "['chest', 'showed', 'bilateral', 'pneumothorax', 'very', 'small', 'apical', 'pneumothoraxes', 'several', 'days'] ['the', 'chest', 'tubes', 'were', 'maintained', 'until', '1/20/90', 'at', 'which', 'time', 'they', 'were', 'pulled', 'sequentially']\n",
      "tokens: [CLS] chest showed bilateral pneumothorax [MASK] small apical pneumothoraxes several days [SEP] the chest tubes were maintained until 1/20/90 at which time they were pulled sequentially [SEP]\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "is_random_next: False\n",
      "masked_lm_positions: 5\n",
      "masked_lm_labels: very\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while i < len(document):\n",
    "    segment = document[i].split(' ')  # each segment is a dictionary w/ sentence from the document\n",
    "    # print(segment)\n",
    "    current_chunk.append(segment)\n",
    "    # print(current_chunk)\n",
    "    current_length += len(segment)  # Number of tokens?\n",
    "    # print(current_length)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "        if current_chunk:\n",
    "            # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "            # (first) sentence.\n",
    "            a_end = 1\n",
    "            if len(current_chunk) >= 2:\n",
    "                print('HERE')\n",
    "                # print(current_chunk)\n",
    "                a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                # print(a_end)\n",
    "\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # print(tokens_a)\n",
    "            tokens_b = []\n",
    "            # Random next\n",
    "            is_random_next = False\n",
    "            if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                is_random_next = True\n",
    "                target_b_length = target_seq_length - len(tokens_a)\n",
    "                # print(target_b_length)\n",
    "\n",
    "                # This should rarely go for more than one iteration for large\n",
    "                # corpora. However, just to be careful, we try to make sure that\n",
    "                # the random document is not the same as the document\n",
    "                # we're processing.\n",
    "                # print([i for i in all_idx if i != idx])\n",
    "                random_document_index = rng.choice([i for i in all_idx if i != idx])\n",
    "\n",
    "                random_document = documents[random_document_index]\n",
    "                random_start = rng.randint(0, len(random_document) - 1)\n",
    "                for j in range(random_start, len(random_document)):\n",
    "                    tokens_b.extend(random_document[j].split(' '))\n",
    "                    if len(tokens_b) >= target_b_length:\n",
    "                        break\n",
    "                # print(tokens_b)\n",
    "                # We didn't actually use these segments so we \"put them back\" so\n",
    "                # they don't go to waste.\n",
    "                num_unused_segments = len(current_chunk) - a_end\n",
    "                i -= num_unused_segments\n",
    "            # Actual next\n",
    "            else:\n",
    "                is_random_next = False\n",
    "                for j in range(a_end, len(current_chunk)):\n",
    "                    tokens_b.extend(current_chunk[j])\n",
    "            truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "            print(tokens_a, tokens_b)\n",
    "            assert len(tokens_a) >= 1\n",
    "            assert len(tokens_b) >= 1\n",
    "\n",
    "            tokens = []\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in tokens_a:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "            # print(tokens)\n",
    "\n",
    "            (tokens, masked_lm_positions,\n",
    "             masked_lm_labels) = create_masked_lm_predictions(\n",
    "                tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "            instance = TrainingInstance(\n",
    "                tokens=tokens,\n",
    "                segment_ids=segment_ids,\n",
    "                is_random_next=is_random_next,\n",
    "                masked_lm_positions=masked_lm_positions,\n",
    "                masked_lm_labels=masked_lm_labels)\n",
    "            instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        print(instance)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.token_type_ids = []\n",
    "        self.labels = []\n",
    "        self.next_sentence_label = []\n",
    "        for dt in dataset.values():\n",
    "            self.input_ids.append(dt[\"input_ids\"])\n",
    "            self.attention_mask.append(dt[\"attention_mask\"])\n",
    "            self.token_type_ids.append(dt[\"token_type_ids\"])\n",
    "            self.labels.append(dt[\"labels\"])\n",
    "            self.next_sentence_label.append(dt[\"next_sentence_label\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in dataset[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'history', 'of', 'present', 'illness', 'this', 'is', 'the', 'second', 'stamgibonnole', 'hospital', 'admission', 'for', 'this', '50', 'year', 'old', 'woman', 'with', 'a', 'history', 'of', 'hypertension', 'crest', 'syndrome', 'bilateral', '[SEP]', 'he', 'was', 'edentulous', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'subtotal', 'occlusion', 'of', 'the', 'rca', 'with', 'a', 'high', 'grade', 'lad', 'lesion', 'the', 'patient', 'has', 'had', '[SEP]', 'referred', 'for', 'laser', 'angiography', 'in', '11/89', 'but', 'it', 'was', 'not', 'done', 'secondasry', 'to', 'extent', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', '50%', 'ulcerated', 'lad', '100%', 'circumflex', '100%', 'rca', 'inferior', 'posterior', 'focal', 'inferior', 'apical', 'o.k.', 'ef', '49%', '[SEP]', 'past', 'medical', 'history', '[MASK]', 'for', 'as', 'above', 'hypertension', 'crest', 'scleroderma', 'reynaud', 'esophagitis', 'telangectasia', 'and', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', '[MASK]', 'is', 'status', 'post', 'bilateral', 'dvt', 'in', '1960', 'and', '1970', 'treated', 'with', 'heparin', 'and', 'coumadin', '[SEP]', 'a', 'history', 'of', 'lymphangitis', 'left', 'arm', 'history', 'of', 'migraine', 'history', 'of', 'left', 'shoulder', 'bursitis', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'she', 'has', 'never', 'been', 'on', 'steroids', 'past', 'surgical', 'history', 'status', 'post', 'tubal', 'ligation', 'appendectomy', '[SEP]', 'november', '26', 'a', 'chest', 'ct', 'revealed', 'multiple', 'bilateral', 'cysts', 'and', 'bilateral', 'nodules', 'a', 'right', 'upper', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'medications', 'dilt', '60', 'b.i.d.', 'lopressor', '50', 'b.i.d.', 'motrin', '800', 't.i.d.', 'decreased', 'from', 'q.i.d.', 'zantac', 'prn', '[SEP]', 'driven', 'to', 'the', 'hospital', 'by', 'his', 'son', 'because', 'of', 'reported', 'worsening', 'and', 'general', 'care', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'allergies', 'are', 'morphine', 'landing', 'hives', '[SEP]', 'drinks', 'alcohol', '2-3', 'times', 'per', 'week', '1-2', 'packs', 'per', 'day', 'hospital', 'course', 'the', 'patient', 'underwent', 'coronary', 'artery', 'bypass', 'grafting', 'x', '2', 'with', 'bilateral', 'mammary', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'lma', 'to', 'rca', 'lma', 'to', 'lad', '[MASK]', 'patient', 'was', 'without', 'operative', 'complications', '[SEP]', 'his', 'history', 'began', 'in', 'february', 'of', '1987', 'when', 'he', 'developed', 'streptococcal', 'endocarditis', 'secondary', 'to', 'intravenous', 'drug', 'abuse', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'she', 'was', 'admitted', 'to', 'tng', 'for', 'blood', 'pressure', 'control', '[SEP]', 'extubated', 'pen', 'day', '1', 'and', 'diuresis', 'was', 'initiated', 'she', 'continued', 'to', 'do', 'well', 'she', 'was', 'taken', 'off', 'nifedipine', 'and', 'continued', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'on', 'martin', 'luther', 'king', 'day', 'she', 'was', 'noted', 'to', 'be', 'in', 'and', 'out', 'of', 'atrial', '[SEP]', 'was', 'transfused', 'one', 'unit', 'of', 'packed', 'red', 'blood', 'cells', 'for', 'a', 'hematocrit', 'of', '23', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', '50', 'b.i.d.', 'it', 'was', 'recommended', 'that', 'her', '[MASK]', 'dose', 'be', 'increased', 'to', '75', 'b.i.d.', 'her', 'chest', 'tubes', 'were', 'kept', 'in', '[SEP]', 'he', 'did', 'well', 'but', 'continued', 'to', 'use', 'intravenous', 'drugs', '[SEP]']\n",
      "32\n",
      "32\n",
      "['[CLS]', 'chest', 'showed', 'bilateral', 'pneumothorax', '[MASK]', 'small', 'apical', 'pneumothoraxes', 'several', 'days', '[SEP]', 'the', 'chest', 'tubes', 'were', 'maintained', 'until', '1/20/90', 'at', 'which', 'time', 'they', 'were', 'pulled', 'sequentially', '[SEP]']\n",
      "32\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "for (inst_index, instance) in enumerate(instances):\n",
    "    print(instance.tokens)\n",
    "    tokenizer.add_tokens(instance.tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(\n",
    "        instance.tokens)  # update vocab with new words given that the tokenization has been done with the raw split(' ') [to modify]\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    print(max_seq_length)\n",
    "    print(len(input_ids))\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "    labels = [-100] * len(input_ids)\n",
    "    labels[masked_lm_positions[0]] = masked_lm_ids[0]\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "        masked_lm_positions.append(0)\n",
    "        masked_lm_ids.append(0)\n",
    "        masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = OrderedDict()\n",
    "    features[\"input_ids\"] = input_ids\n",
    "    features[\"attention_mask\"] = input_mask\n",
    "    features[\"token_type_ids\"] = segment_ids\n",
    "    features[\"masked_lm_positions\"] = masked_lm_positions\n",
    "    features[\"masked_lm_ids\"] = masked_lm_ids\n",
    "    features[\"masked_lm_weights\"] = masked_lm_weights\n",
    "    features[\"next_sentence_label\"] = [next_sentence_label]\n",
    "    features[\"labels\"] = labels\n",
    "\n",
    "    dataset[inst_index] = features\n",
    "    # dt = MyDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "dt, tokenizers = pkl.load(open('./out/prova.pt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'token_type_ids', 'masked_lm_positions', 'masked_lm_ids', 'masked_lm_weights', 'next_sentence_label', 'labels', 'note_id'],\n",
       "        num_rows: 137\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'token_type_ids', 'masked_lm_positions', 'masked_lm_ids', 'masked_lm_weights', 'next_sentence_label', 'labels', 'note_id'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
       " 'test': PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid magic number; corrupt file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_98084/2664224815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./out/prova.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0mprotocol_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol_version\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPROTOCOL_VERSION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid magic number; corrupt file?"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dt = torch.load(open('./out/prova.pt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dd = Dataset.from_dict(dt['train'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  28996,\n",
       "  1142,\n",
       "  7791,\n",
       "  1218,\n",
       "  1105,\n",
       "  1108,\n",
       "  28997,\n",
       "  1106,\n",
       "  1103,\n",
       "  17688,\n",
       "  6059,\n",
       "  2585,\n",
       "  1205,\n",
       "  1111,\n",
       "  1748,\n",
       "  102,\n",
       "  1113,\n",
       "  28998,\n",
       "  1744,\n",
       "  170,\n",
       "  2229,\n",
       "  28999,\n",
       "  3090,\n",
       "  2967,\n",
       "  20557,\n",
       "  29000,\n",
       "  1105,\n",
       "  103,\n",
       "  29001,\n",
       "  170,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  29002,\n",
       "  102,\n",
       "  29003,\n",
       "  8682,\n",
       "  1127,\n",
       "  2330,\n",
       "  2589,\n",
       "  1111,\n",
       "  1199,\n",
       "  7648,\n",
       "  10496,\n",
       "  29004,\n",
       "  17688,\n",
       "  29005,\n",
       "  1114,\n",
       "  170,\n",
       "  25550,\n",
       "  29006,\n",
       "  29007,\n",
       "  29008,\n",
       "  22895,\n",
       "  7413,\n",
       "  103,\n",
       "  8886,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  3105,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  2229,\n",
       "  11182,\n",
       "  1127,\n",
       "  4441,\n",
       "  1235,\n",
       "  29009,\n",
       "  1120,\n",
       "  1134,\n",
       "  1159,\n",
       "  1152,\n",
       "  1127,\n",
       "  1865,\n",
       "  29010,\n",
       "  102,\n",
       "  1104,\n",
       "  20752,\n",
       "  8936,\n",
       "  29011,\n",
       "  103,\n",
       "  4680,\n",
       "  170,\n",
       "  29012,\n",
       "  29013,\n",
       "  4680,\n",
       "  4091,\n",
       "  29014,\n",
       "  29015,\n",
       "  4680,\n",
       "  29016,\n",
       "  102],\n",
       " [101,\n",
       "  1126,\n",
       "  29017,\n",
       "  3090,\n",
       "  1126,\n",
       "  29008,\n",
       "  13394,\n",
       "  1104,\n",
       "  29018,\n",
       "  102,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  29021,\n",
       "  122,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  1266,\n",
       "  1607,\n",
       "  1103,\n",
       "  103,\n",
       "  3756,\n",
       "  1185,\n",
       "  1266,\n",
       "  1607,\n",
       "  1104,\n",
       "  4182,\n",
       "  1103,\n",
       "  1534,\n",
       "  1452,\n",
       "  102],\n",
       " [101,\n",
       "  29022,\n",
       "  29019,\n",
       "  29023,\n",
       "  103,\n",
       "  1103,\n",
       "  1148,\n",
       "  1160,\n",
       "  2277,\n",
       "  18326,\n",
       "  1106,\n",
       "  2363,\n",
       "  29022,\n",
       "  29019,\n",
       "  29024,\n",
       "  1107,\n",
       "  102,\n",
       "  29022,\n",
       "  29019,\n",
       "  29024,\n",
       "  29025,\n",
       "  2908,\n",
       "  29022,\n",
       "  29019,\n",
       "  29024,\n",
       "  29026,\n",
       "  1406,\n",
       "  29022,\n",
       "  29019,\n",
       "  1120,\n",
       "  29027,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  29028,\n",
       "  29029,\n",
       "  1299,\n",
       "  29030,\n",
       "  29031,\n",
       "  5731,\n",
       "  1659,\n",
       "  1111,\n",
       "  9301,\n",
       "  5300,\n",
       "  1246,\n",
       "  1257,\n",
       "  4942,\n",
       "  3678,\n",
       "  1105,\n",
       "  102,\n",
       "  2999,\n",
       "  29032,\n",
       "  26600,\n",
       "  2330,\n",
       "  2241,\n",
       "  1185,\n",
       "  29033,\n",
       "  103,\n",
       "  1137,\n",
       "  29034,\n",
       "  14701,\n",
       "  2991,\n",
       "  29035,\n",
       "  102],\n",
       " [101,\n",
       "  29036,\n",
       "  1275,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  127,\n",
       "  29037,\n",
       "  29038,\n",
       "  1111,\n",
       "  22882,\n",
       "  102,\n",
       "  186,\n",
       "  29039,\n",
       "  26754,\n",
       "  1118,\n",
       "  29040,\n",
       "  29041,\n",
       "  29042,\n",
       "  29043,\n",
       "  6546,\n",
       "  29044,\n",
       "  29045,\n",
       "  29046,\n",
       "  103,\n",
       "  29047,\n",
       "  15817,\n",
       "  29048,\n",
       "  7448,\n",
       "  29049,\n",
       "  29050,\n",
       "  102],\n",
       " [101,\n",
       "  14115,\n",
       "  29051,\n",
       "  29052,\n",
       "  1235,\n",
       "  29053,\n",
       "  2258,\n",
       "  29054,\n",
       "  1413,\n",
       "  7953,\n",
       "  103,\n",
       "  29055,\n",
       "  29056,\n",
       "  29057,\n",
       "  1105,\n",
       "  29058,\n",
       "  2273,\n",
       "  102,\n",
       "  2812,\n",
       "  1146,\n",
       "  2812,\n",
       "  1146,\n",
       "  1114,\n",
       "  29059,\n",
       "  29060,\n",
       "  29059,\n",
       "  29061,\n",
       "  1105,\n",
       "  2425,\n",
       "  1920,\n",
       "  7454,\n",
       "  102],\n",
       " [101,\n",
       "  29062,\n",
       "  1529,\n",
       "  170,\n",
       "  29063,\n",
       "  29017,\n",
       "  1113,\n",
       "  29064,\n",
       "  2799,\n",
       "  16964,\n",
       "  1104,\n",
       "  1103,\n",
       "  16530,\n",
       "  27535,\n",
       "  3170,\n",
       "  1104,\n",
       "  102,\n",
       "  170,\n",
       "  29065,\n",
       "  10338,\n",
       "  1113,\n",
       "  1103,\n",
       "  11727,\n",
       "  1115,\n",
       "  103,\n",
       "  1154,\n",
       "  1286,\n",
       "  29066,\n",
       "  29067,\n",
       "  14441,\n",
       "  1219,\n",
       "  102],\n",
       " [101,\n",
       "  1104,\n",
       "  29068,\n",
       "  1607,\n",
       "  1104,\n",
       "  1286,\n",
       "  2342,\n",
       "  29069,\n",
       "  1131,\n",
       "  1144,\n",
       "  1309,\n",
       "  1151,\n",
       "  1113,\n",
       "  29070,\n",
       "  1763,\n",
       "  13467,\n",
       "  1607,\n",
       "  2781,\n",
       "  2112,\n",
       "  29071,\n",
       "  29072,\n",
       "  29073,\n",
       "  102,\n",
       "  1126,\n",
       "  29017,\n",
       "  3090,\n",
       "  1126,\n",
       "  29008,\n",
       "  13394,\n",
       "  103,\n",
       "  29018,\n",
       "  102],\n",
       " [101,\n",
       "  29074,\n",
       "  29075,\n",
       "  1110,\n",
       "  4533,\n",
       "  1111,\n",
       "  170,\n",
       "  11727,\n",
       "  103,\n",
       "  102,\n",
       "  29076,\n",
       "  29077,\n",
       "  29078,\n",
       "  29079,\n",
       "  29080,\n",
       "  10507,\n",
       "  29081,\n",
       "  29082,\n",
       "  29083,\n",
       "  171,\n",
       "  1105,\n",
       "  172,\n",
       "  29084,\n",
       "  1107,\n",
       "  2164,\n",
       "  19560,\n",
       "  29085,\n",
       "  29086,\n",
       "  3850,\n",
       "  6704,\n",
       "  29087,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1338,\n",
       "  1106,\n",
       "  1103,\n",
       "  5241,\n",
       "  1395,\n",
       "  1111,\n",
       "  10540,\n",
       "  102,\n",
       "  17688,\n",
       "  29005,\n",
       "  1114,\n",
       "  170,\n",
       "  25550,\n",
       "  29006,\n",
       "  29007,\n",
       "  29008,\n",
       "  22895,\n",
       "  7413,\n",
       "  6478,\n",
       "  8886,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  3105,\n",
       "  29088,\n",
       "  11911,\n",
       "  2652,\n",
       "  29089,\n",
       "  24716,\n",
       "  102],\n",
       " [101,\n",
       "  1131,\n",
       "  26360,\n",
       "  1251,\n",
       "  6272,\n",
       "  1137,\n",
       "  2472,\n",
       "  3850,\n",
       "  1329,\n",
       "  2952,\n",
       "  8179,\n",
       "  9301,\n",
       "  5300,\n",
       "  4143,\n",
       "  1762,\n",
       "  2603,\n",
       "  3078,\n",
       "  1892,\n",
       "  2997,\n",
       "  29090,\n",
       "  19192,\n",
       "  2603,\n",
       "  1407,\n",
       "  102,\n",
       "  7621,\n",
       "  29091,\n",
       "  103,\n",
       "  29092,\n",
       "  1113,\n",
       "  1395,\n",
       "  1586,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  5165,\n",
       "  1114,\n",
       "  29093,\n",
       "  1105,\n",
       "  29094,\n",
       "  1113,\n",
       "  29095,\n",
       "  1119,\n",
       "  1872,\n",
       "  29096,\n",
       "  1107,\n",
       "  1117,\n",
       "  1286,\n",
       "  1981,\n",
       "  1105,\n",
       "  1289,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  1276,\n",
       "  1106,\n",
       "  103,\n",
       "  170,\n",
       "  1286,\n",
       "  29097,\n",
       "  18593,\n",
       "  29098,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  10296,\n",
       "  29099,\n",
       "  1108,\n",
       "  1969,\n",
       "  1653,\n",
       "  5099,\n",
       "  29100,\n",
       "  103,\n",
       "  15059,\n",
       "  15731,\n",
       "  21177,\n",
       "  29101,\n",
       "  1955,\n",
       "  29102,\n",
       "  102,\n",
       "  29103,\n",
       "  1103,\n",
       "  29103,\n",
       "  3090,\n",
       "  29104,\n",
       "  29105,\n",
       "  1120,\n",
       "  11965,\n",
       "  1679,\n",
       "  2517,\n",
       "  1114,\n",
       "  1185,\n",
       "  29106,\n",
       "  14235,\n",
       "  102],\n",
       " [101,\n",
       "  17713,\n",
       "  29019,\n",
       "  29023,\n",
       "  29107,\n",
       "  1626,\n",
       "  17713,\n",
       "  186,\n",
       "  29039,\n",
       "  29108,\n",
       "  29109,\n",
       "  1572,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  29039,\n",
       "  193,\n",
       "  124,\n",
       "  102,\n",
       "  29110,\n",
       "  1620,\n",
       "  17713,\n",
       "  29019,\n",
       "  103,\n",
       "  29111,\n",
       "  29112,\n",
       "  4214,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  29111,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  29113,\n",
       "  1105,\n",
       "  29114,\n",
       "  1120,\n",
       "  1115,\n",
       "  1159,\n",
       "  1105,\n",
       "  1108,\n",
       "  2886,\n",
       "  29115,\n",
       "  102,\n",
       "  1106,\n",
       "  29116,\n",
       "  29117,\n",
       "  1286,\n",
       "  16557,\n",
       "  16610,\n",
       "  29116,\n",
       "  103,\n",
       "  29118,\n",
       "  16557,\n",
       "  16610,\n",
       "  29119,\n",
       "  29117,\n",
       "  1286,\n",
       "  29120,\n",
       "  1105,\n",
       "  29117,\n",
       "  102],\n",
       " [101,\n",
       "  3711,\n",
       "  1106,\n",
       "  1138,\n",
       "  2141,\n",
       "  170,\n",
       "  1374,\n",
       "  2277,\n",
       "  2403,\n",
       "  102,\n",
       "  1114,\n",
       "  1160,\n",
       "  1482,\n",
       "  3189,\n",
       "  1104,\n",
       "  2344,\n",
       "  1119,\n",
       "  1144,\n",
       "  29121,\n",
       "  1113,\n",
       "  29122,\n",
       "  1649,\n",
       "  26360,\n",
       "  29123,\n",
       "  29124,\n",
       "  29125,\n",
       "  29121,\n",
       "  1137,\n",
       "  2211,\n",
       "  29126,\n",
       "  29034,\n",
       "  102],\n",
       " [101,\n",
       "  2765,\n",
       "  1108,\n",
       "  29099,\n",
       "  3236,\n",
       "  29127,\n",
       "  5099,\n",
       "  5692,\n",
       "  126,\n",
       "  4393,\n",
       "  1113,\n",
       "  12630,\n",
       "  29128,\n",
       "  103,\n",
       "  7597,\n",
       "  29129,\n",
       "  102,\n",
       "  13522,\n",
       "  1121,\n",
       "  1142,\n",
       "  7791,\n",
       "  1105,\n",
       "  1119,\n",
       "  1915,\n",
       "  1107,\n",
       "  2999,\n",
       "  29130,\n",
       "  6795,\n",
       "  1235,\n",
       "  1103,\n",
       "  1159,\n",
       "  102],\n",
       " [101,\n",
       "  1763,\n",
       "  2657,\n",
       "  1607,\n",
       "  2418,\n",
       "  1111,\n",
       "  1112,\n",
       "  1807,\n",
       "  29131,\n",
       "  13468,\n",
       "  29132,\n",
       "  29133,\n",
       "  29134,\n",
       "  29135,\n",
       "  1105,\n",
       "  29136,\n",
       "  102,\n",
       "  2363,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  29021,\n",
       "  122,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  103,\n",
       "  1607,\n",
       "  1103,\n",
       "  5351,\n",
       "  3756,\n",
       "  102],\n",
       " [101,\n",
       "  29137,\n",
       "  5538,\n",
       "  103,\n",
       "  29138,\n",
       "  6546,\n",
       "  29139,\n",
       "  1211,\n",
       "  29042,\n",
       "  29140,\n",
       "  15817,\n",
       "  29141,\n",
       "  7448,\n",
       "  29049,\n",
       "  29142,\n",
       "  29143,\n",
       "  173,\n",
       "  29144,\n",
       "  189,\n",
       "  29145,\n",
       "  102,\n",
       "  2628,\n",
       "  12645,\n",
       "  29146,\n",
       "  14441,\n",
       "  8974,\n",
       "  1105,\n",
       "  29147,\n",
       "  1104,\n",
       "  1103,\n",
       "  11911,\n",
       "  102],\n",
       " [101,\n",
       "  1106,\n",
       "  10296,\n",
       "  2781,\n",
       "  2112,\n",
       "  29148,\n",
       "  23897,\n",
       "  1113,\n",
       "  103,\n",
       "  29149,\n",
       "  5190,\n",
       "  29011,\n",
       "  3839,\n",
       "  1266,\n",
       "  1607,\n",
       "  1136,\n",
       "  3836,\n",
       "  1934,\n",
       "  102,\n",
       "  8898,\n",
       "  29150,\n",
       "  29151,\n",
       "  1106,\n",
       "  10116,\n",
       "  9415,\n",
       "  1105,\n",
       "  23261,\n",
       "  29152,\n",
       "  1111,\n",
       "  1242,\n",
       "  1201,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  26360,\n",
       "  2229,\n",
       "  2489,\n",
       "  2793,\n",
       "  10880,\n",
       "  29153,\n",
       "  29154,\n",
       "  29155,\n",
       "  1137,\n",
       "  22882,\n",
       "  102,\n",
       "  29156,\n",
       "  1108,\n",
       "  1982,\n",
       "  103,\n",
       "  170,\n",
       "  1286,\n",
       "  1762,\n",
       "  29156,\n",
       "  1108,\n",
       "  1982,\n",
       "  1134,\n",
       "  3090,\n",
       "  170,\n",
       "  1268,\n",
       "  29104,\n",
       "  2997,\n",
       "  1104,\n",
       "  1275,\n",
       "  102],\n",
       " [101,\n",
       "  3102,\n",
       "  17713,\n",
       "  29086,\n",
       "  29157,\n",
       "  29055,\n",
       "  17713,\n",
       "  29158,\n",
       "  1112,\n",
       "  2002,\n",
       "  29159,\n",
       "  122,\n",
       "  17713,\n",
       "  29158,\n",
       "  103,\n",
       "  29160,\n",
       "  1969,\n",
       "  17713,\n",
       "  29158,\n",
       "  29161,\n",
       "  29162,\n",
       "  29163,\n",
       "  29164,\n",
       "  29165,\n",
       "  29163,\n",
       "  29164,\n",
       "  10296,\n",
       "  102,\n",
       "  29166,\n",
       "  1634,\n",
       "  1108,\n",
       "  102],\n",
       " [101,\n",
       "  1111,\n",
       "  10221,\n",
       "  29167,\n",
       "  1107,\n",
       "  29168,\n",
       "  1133,\n",
       "  1122,\n",
       "  1108,\n",
       "  1136,\n",
       "  1694,\n",
       "  29169,\n",
       "  1106,\n",
       "  103,\n",
       "  1104,\n",
       "  3653,\n",
       "  102,\n",
       "  2799,\n",
       "  29170,\n",
       "  29171,\n",
       "  19122,\n",
       "  29119,\n",
       "  29120,\n",
       "  29119,\n",
       "  29172,\n",
       "  15543,\n",
       "  16530,\n",
       "  17811,\n",
       "  15543,\n",
       "  29173,\n",
       "  29174,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  103,\n",
       "  1141,\n",
       "  2587,\n",
       "  1104,\n",
       "  8733,\n",
       "  1894,\n",
       "  1892,\n",
       "  3652,\n",
       "  1111,\n",
       "  170,\n",
       "  29099,\n",
       "  1104,\n",
       "  1695,\n",
       "  102,\n",
       "  27486,\n",
       "  1105,\n",
       "  29175,\n",
       "  1653,\n",
       "  1892,\n",
       "  3652,\n",
       "  29176,\n",
       "  1894,\n",
       "  1892,\n",
       "  3652,\n",
       "  29177,\n",
       "  15185,\n",
       "  29176,\n",
       "  1653,\n",
       "  102],\n",
       " [101,\n",
       "  129,\n",
       "  177,\n",
       "  29178,\n",
       "  2489,\n",
       "  173,\n",
       "  172,\n",
       "  1546,\n",
       "  103,\n",
       "  10211,\n",
       "  1822,\n",
       "  7930,\n",
       "  1185,\n",
       "  1896,\n",
       "  6870,\n",
       "  102,\n",
       "  1103,\n",
       "  1488,\n",
       "  1150,\n",
       "  1464,\n",
       "  1115,\n",
       "  1119,\n",
       "  1105,\n",
       "  1168,\n",
       "  1266,\n",
       "  1484,\n",
       "  1458,\n",
       "  1106,\n",
       "  29179,\n",
       "  1103,\n",
       "  5351,\n",
       "  102],\n",
       " [101,\n",
       "  1185,\n",
       "  103,\n",
       "  1607,\n",
       "  102,\n",
       "  3653,\n",
       "  3189,\n",
       "  1104,\n",
       "  2344,\n",
       "  4303,\n",
       "  29180,\n",
       "  10296,\n",
       "  23897,\n",
       "  29166,\n",
       "  29022,\n",
       "  29019,\n",
       "  29181,\n",
       "  29027,\n",
       "  29055,\n",
       "  29022,\n",
       "  29019,\n",
       "  29181,\n",
       "  29039,\n",
       "  29182,\n",
       "  1512,\n",
       "  29022,\n",
       "  29019,\n",
       "  29183,\n",
       "  29025,\n",
       "  7690,\n",
       "  29022,\n",
       "  102],\n",
       " [101,\n",
       "  29184,\n",
       "  1108,\n",
       "  29185,\n",
       "  29186,\n",
       "  3090,\n",
       "  2999,\n",
       "  29130,\n",
       "  21344,\n",
       "  1114,\n",
       "  1185,\n",
       "  29187,\n",
       "  2607,\n",
       "  102,\n",
       "  2704,\n",
       "  1736,\n",
       "  1103,\n",
       "  5351,\n",
       "  1915,\n",
       "  1113,\n",
       "  1103,\n",
       "  29188,\n",
       "  6059,\n",
       "  1555,\n",
       "  1111,\n",
       "  1103,\n",
       "  1148,\n",
       "  1210,\n",
       "  1552,\n",
       "  1104,\n",
       "  29189,\n",
       "  102],\n",
       " [101,\n",
       "  26844,\n",
       "  1106,\n",
       "  1609,\n",
       "  29190,\n",
       "  29191,\n",
       "  29192,\n",
       "  2455,\n",
       "  1185,\n",
       "  29193,\n",
       "  29194,\n",
       "  29195,\n",
       "  103,\n",
       "  29196,\n",
       "  29006,\n",
       "  29007,\n",
       "  29008,\n",
       "  102,\n",
       "  2999,\n",
       "  29032,\n",
       "  26600,\n",
       "  2330,\n",
       "  2241,\n",
       "  1185,\n",
       "  29033,\n",
       "  29197,\n",
       "  1137,\n",
       "  29034,\n",
       "  14701,\n",
       "  2991,\n",
       "  29035,\n",
       "  102],\n",
       " [101,\n",
       "  20705,\n",
       "  1127,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  3105,\n",
       "  25163,\n",
       "  1105,\n",
       "  1142,\n",
       "  1108,\n",
       "  1464,\n",
       "  1106,\n",
       "  1129,\n",
       "  9803,\n",
       "  1106,\n",
       "  1129,\n",
       "  8080,\n",
       "  1114,\n",
       "  102,\n",
       "  1113,\n",
       "  29198,\n",
       "  1330,\n",
       "  29199,\n",
       "  29200,\n",
       "  1108,\n",
       "  1982,\n",
       "  24749,\n",
       "  29201,\n",
       "  14402,\n",
       "  8240,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  29083,\n",
       "  29202,\n",
       "  11911,\n",
       "  4290,\n",
       "  12687,\n",
       "  1114,\n",
       "  1117,\n",
       "  29203,\n",
       "  29204,\n",
       "  102,\n",
       "  103,\n",
       "  1145,\n",
       "  10558,\n",
       "  1114,\n",
       "  170,\n",
       "  3968,\n",
       "  1107,\n",
       "  19968,\n",
       "  5964,\n",
       "  1245,\n",
       "  1167,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  1112,\n",
       "  1119,\n",
       "  1872,\n",
       "  1199,\n",
       "  26600,\n",
       "  102],\n",
       " [101,\n",
       "  2999,\n",
       "  2327,\n",
       "  29206,\n",
       "  3807,\n",
       "  102,\n",
       "  9301,\n",
       "  5300,\n",
       "  4143,\n",
       "  1762,\n",
       "  2603,\n",
       "  3078,\n",
       "  1892,\n",
       "  2997,\n",
       "  29090,\n",
       "  19192,\n",
       "  2603,\n",
       "  1407,\n",
       "  7621,\n",
       "  29091,\n",
       "  1110,\n",
       "  29092,\n",
       "  1113,\n",
       "  1395,\n",
       "  1586,\n",
       "  29207,\n",
       "  103,\n",
       "  4463,\n",
       "  1668,\n",
       "  1105,\n",
       "  26844,\n",
       "  102],\n",
       " [101,\n",
       "  2229,\n",
       "  2799,\n",
       "  20557,\n",
       "  29208,\n",
       "  1304,\n",
       "  1353,\n",
       "  29173,\n",
       "  29209,\n",
       "  1317,\n",
       "  1552,\n",
       "  102,\n",
       "  29102,\n",
       "  1489,\n",
       "  1105,\n",
       "  103,\n",
       "  1104,\n",
       "  29210,\n",
       "  29211,\n",
       "  8208,\n",
       "  11911,\n",
       "  3053,\n",
       "  5715,\n",
       "  29212,\n",
       "  1104,\n",
       "  1489,\n",
       "  2904,\n",
       "  1542,\n",
       "  1703,\n",
       "  29213,\n",
       "  5190,\n",
       "  102],\n",
       " [101,\n",
       "  1114,\n",
       "  1104,\n",
       "  1117,\n",
       "  29214,\n",
       "  29215,\n",
       "  27143,\n",
       "  1113,\n",
       "  29058,\n",
       "  1103,\n",
       "  10938,\n",
       "  1112,\n",
       "  1218,\n",
       "  1112,\n",
       "  1210,\n",
       "  1551,\n",
       "  170,\n",
       "  1989,\n",
       "  1107,\n",
       "  1103,\n",
       "  1378,\n",
       "  1989,\n",
       "  1105,\n",
       "  1114,\n",
       "  29059,\n",
       "  102,\n",
       "  1131,\n",
       "  3756,\n",
       "  1515,\n",
       "  1185,\n",
       "  9302,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  29216,\n",
       "  2799,\n",
       "  29175,\n",
       "  29217,\n",
       "  103,\n",
       "  1105,\n",
       "  29175,\n",
       "  1653,\n",
       "  1892,\n",
       "  3652,\n",
       "  29176,\n",
       "  1894,\n",
       "  1892,\n",
       "  3652,\n",
       "  29177,\n",
       "  15185,\n",
       "  29176,\n",
       "  1653,\n",
       "  1892,\n",
       "  3652,\n",
       "  2641,\n",
       "  102,\n",
       "  29218,\n",
       "  7606,\n",
       "  1105,\n",
       "  29219,\n",
       "  29220,\n",
       "  1127,\n",
       "  1982,\n",
       "  102],\n",
       " [101,\n",
       "  1131,\n",
       "  3756,\n",
       "  1515,\n",
       "  1198,\n",
       "  8204,\n",
       "  2988,\n",
       "  1106,\n",
       "  10296,\n",
       "  102,\n",
       "  8087,\n",
       "  2233,\n",
       "  3385,\n",
       "  1111,\n",
       "  170,\n",
       "  29101,\n",
       "  1105,\n",
       "  29102,\n",
       "  1489,\n",
       "  1105,\n",
       "  29221,\n",
       "  1104,\n",
       "  29210,\n",
       "  29211,\n",
       "  8208,\n",
       "  11911,\n",
       "  3053,\n",
       "  5715,\n",
       "  29212,\n",
       "  1104,\n",
       "  1489,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  29055,\n",
       "  1108,\n",
       "  29222,\n",
       "  1111,\n",
       "  29223,\n",
       "  11727,\n",
       "  29224,\n",
       "  1114,\n",
       "  170,\n",
       "  2273,\n",
       "  29225,\n",
       "  1104,\n",
       "  103,\n",
       "  102,\n",
       "  1113,\n",
       "  4910,\n",
       "  2781,\n",
       "  12211,\n",
       "  1119,\n",
       "  1108,\n",
       "  29226,\n",
       "  1133,\n",
       "  29227,\n",
       "  7779,\n",
       "  1106,\n",
       "  1271,\n",
       "  1214,\n",
       "  1105,\n",
       "  2704,\n",
       "  102],\n",
       " [101,\n",
       "  1649,\n",
       "  170,\n",
       "  29228,\n",
       "  14884,\n",
       "  1108,\n",
       "  1982,\n",
       "  1134,\n",
       "  3090,\n",
       "  9533,\n",
       "  9750,\n",
       "  102,\n",
       "  2077,\n",
       "  2041,\n",
       "  1133,\n",
       "  1108,\n",
       "  4940,\n",
       "  1106,\n",
       "  1103,\n",
       "  2704,\n",
       "  1118,\n",
       "  1117,\n",
       "  103,\n",
       "  1272,\n",
       "  1104,\n",
       "  2103,\n",
       "  29204,\n",
       "  1105,\n",
       "  1704,\n",
       "  1920,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  5351,\n",
       "  1125,\n",
       "  1199,\n",
       "  8331,\n",
       "  1107,\n",
       "  1123,\n",
       "  8006,\n",
       "  102,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29229,\n",
       "  1111,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  1131,\n",
       "  1108,\n",
       "  29230,\n",
       "  1113,\n",
       "  1285,\n",
       "  122,\n",
       "  1105,\n",
       "  29231,\n",
       "  24844,\n",
       "  7087,\n",
       "  1131,\n",
       "  1598,\n",
       "  1106,\n",
       "  102],\n",
       " [101,\n",
       "  2635,\n",
       "  1104,\n",
       "  1117,\n",
       "  29232,\n",
       "  1763,\n",
       "  2657,\n",
       "  1607,\n",
       "  103,\n",
       "  21344,\n",
       "  29087,\n",
       "  29131,\n",
       "  29233,\n",
       "  1107,\n",
       "  2294,\n",
       "  29234,\n",
       "  102,\n",
       "  2229,\n",
       "  3090,\n",
       "  2330,\n",
       "  8682,\n",
       "  1105,\n",
       "  1173,\n",
       "  1119,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  1264,\n",
       "  172,\n",
       "  1111,\n",
       "  2635,\n",
       "  102],\n",
       " [101,\n",
       "  4940,\n",
       "  1106,\n",
       "  1103,\n",
       "  2704,\n",
       "  1118,\n",
       "  1117,\n",
       "  1488,\n",
       "  1272,\n",
       "  1104,\n",
       "  2103,\n",
       "  29204,\n",
       "  1105,\n",
       "  1704,\n",
       "  1920,\n",
       "  1105,\n",
       "  102,\n",
       "  1736,\n",
       "  1119,\n",
       "  103,\n",
       "  17428,\n",
       "  1107,\n",
       "  1103,\n",
       "  5241,\n",
       "  1395,\n",
       "  1276,\n",
       "  1106,\n",
       "  1129,\n",
       "  8669,\n",
       "  29028,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  29235,\n",
       "  13093,\n",
       "  3367,\n",
       "  1878,\n",
       "  29236,\n",
       "  10296,\n",
       "  2236,\n",
       "  29237,\n",
       "  2592,\n",
       "  2781,\n",
       "  1878,\n",
       "  12398,\n",
       "  2236,\n",
       "  29238,\n",
       "  103,\n",
       "  102,\n",
       "  8208,\n",
       "  11911,\n",
       "  3053,\n",
       "  5715,\n",
       "  29212,\n",
       "  1104,\n",
       "  1489,\n",
       "  2904,\n",
       "  1542,\n",
       "  1703,\n",
       "  29213,\n",
       "  5190,\n",
       "  29239,\n",
       "  29240,\n",
       "  102],\n",
       " [101,\n",
       "  29241,\n",
       "  12211,\n",
       "  103,\n",
       "  29242,\n",
       "  4366,\n",
       "  29243,\n",
       "  1127,\n",
       "  3258,\n",
       "  1114,\n",
       "  1363,\n",
       "  29244,\n",
       "  27631,\n",
       "  102,\n",
       "  1607,\n",
       "  1104,\n",
       "  29245,\n",
       "  1286,\n",
       "  1981,\n",
       "  1607,\n",
       "  1104,\n",
       "  29068,\n",
       "  1607,\n",
       "  1104,\n",
       "  1286,\n",
       "  2342,\n",
       "  29069,\n",
       "  1131,\n",
       "  1144,\n",
       "  1309,\n",
       "  1151,\n",
       "  102],\n",
       " [101,\n",
       "  1562,\n",
       "  1118,\n",
       "  1103,\n",
       "  20342,\n",
       "  3653,\n",
       "  1555,\n",
       "  1150,\n",
       "  6315,\n",
       "  1119,\n",
       "  3118,\n",
       "  1113,\n",
       "  1117,\n",
       "  29051,\n",
       "  29246,\n",
       "  1105,\n",
       "  102,\n",
       "  7261,\n",
       "  29247,\n",
       "  103,\n",
       "  1103,\n",
       "  17688,\n",
       "  29156,\n",
       "  8087,\n",
       "  1113,\n",
       "  29248,\n",
       "  1134,\n",
       "  3090,\n",
       "  5199,\n",
       "  29249,\n",
       "  29250,\n",
       "  102],\n",
       " [101,\n",
       "  1954,\n",
       "  1329,\n",
       "  5351,\n",
       "  3756,\n",
       "  7957,\n",
       "  6272,\n",
       "  1329,\n",
       "  1144,\n",
       "  1151,\n",
       "  9987,\n",
       "  2324,\n",
       "  1275,\n",
       "  16595,\n",
       "  170,\n",
       "  1285,\n",
       "  102,\n",
       "  29251,\n",
       "  3653,\n",
       "  3694,\n",
       "  1107,\n",
       "  29252,\n",
       "  103,\n",
       "  29253,\n",
       "  29254,\n",
       "  1104,\n",
       "  1117,\n",
       "  29117,\n",
       "  1268,\n",
       "  4422,\n",
       "  29255,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1209,\n",
       "  2760,\n",
       "  29051,\n",
       "  29052,\n",
       "  1235,\n",
       "  29256,\n",
       "  124,\n",
       "  1630,\n",
       "  1106,\n",
       "  170,\n",
       "  1703,\n",
       "  1104,\n",
       "  1210,\n",
       "  2277,\n",
       "  102,\n",
       "  1119,\n",
       "  1110,\n",
       "  29257,\n",
       "  1106,\n",
       "  1114,\n",
       "  29059,\n",
       "  29060,\n",
       "  1117,\n",
       "  29258,\n",
       "  103,\n",
       "  1117,\n",
       "  2425,\n",
       "  1920,\n",
       "  7454,\n",
       "  102],\n",
       " [101,\n",
       "  29259,\n",
       "  29260,\n",
       "  193,\n",
       "  1160,\n",
       "  1107,\n",
       "  2219,\n",
       "  1105,\n",
       "  2164,\n",
       "  4172,\n",
       "  7918,\n",
       "  29261,\n",
       "  29262,\n",
       "  1107,\n",
       "  2115,\n",
       "  1105,\n",
       "  102,\n",
       "  3175,\n",
       "  1106,\n",
       "  170,\n",
       "  29263,\n",
       "  11138,\n",
       "  1313,\n",
       "  1106,\n",
       "  2335,\n",
       "  1117,\n",
       "  1565,\n",
       "  1989,\n",
       "  1736,\n",
       "  1104,\n",
       "  29051,\n",
       "  102],\n",
       " [101,\n",
       "  2704,\n",
       "  1736,\n",
       "  1105,\n",
       "  3252,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  4120,\n",
       "  103,\n",
       "  1103,\n",
       "  29264,\n",
       "  2755,\n",
       "  1104,\n",
       "  2657,\n",
       "  2057,\n",
       "  102,\n",
       "  4910,\n",
       "  2781,\n",
       "  8621,\n",
       "  1106,\n",
       "  6246,\n",
       "  1112,\n",
       "  1119,\n",
       "  1245,\n",
       "  1167,\n",
       "  23024,\n",
       "  1105,\n",
       "  1750,\n",
       "  29227,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  1106,\n",
       "  10296,\n",
       "  1607,\n",
       "  1104,\n",
       "  2302,\n",
       "  6272,\n",
       "  1329,\n",
       "  1150,\n",
       "  2756,\n",
       "  1114,\n",
       "  170,\n",
       "  1160,\n",
       "  1989,\n",
       "  1607,\n",
       "  103,\n",
       "  2869,\n",
       "  29158,\n",
       "  14741,\n",
       "  2841,\n",
       "  2445,\n",
       "  102,\n",
       "  1131,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29229,\n",
       "  1111,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  102],\n",
       " [101,\n",
       "  103,\n",
       "  29265,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  1105,\n",
       "  29266,\n",
       "  1107,\n",
       "  1103,\n",
       "  1286,\n",
       "  8561,\n",
       "  1108,\n",
       "  9507,\n",
       "  1206,\n",
       "  5356,\n",
       "  1105,\n",
       "  8183,\n",
       "  1119,\n",
       "  1108,\n",
       "  29267,\n",
       "  1117,\n",
       "  19192,\n",
       "  2603,\n",
       "  102,\n",
       "  29091,\n",
       "  1108,\n",
       "  29268,\n",
       "  1107,\n",
       "  1395,\n",
       "  1586,\n",
       "  102],\n",
       " [101,\n",
       "  103,\n",
       "  29269,\n",
       "  29270,\n",
       "  29165,\n",
       "  29270,\n",
       "  123,\n",
       "  29271,\n",
       "  29272,\n",
       "  29164,\n",
       "  29052,\n",
       "  29273,\n",
       "  9416,\n",
       "  17713,\n",
       "  29086,\n",
       "  29274,\n",
       "  29275,\n",
       "  20159,\n",
       "  123,\n",
       "  29276,\n",
       "  29158,\n",
       "  6875,\n",
       "  29277,\n",
       "  15059,\n",
       "  102,\n",
       "  1131,\n",
       "  1144,\n",
       "  1309,\n",
       "  1151,\n",
       "  1113,\n",
       "  29070,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  5762,\n",
       "  1111,\n",
       "  170,\n",
       "  5807,\n",
       "  7439,\n",
       "  102,\n",
       "  29278,\n",
       "  2332,\n",
       "  1113,\n",
       "  29279,\n",
       "  1105,\n",
       "  1973,\n",
       "  1113,\n",
       "  29246,\n",
       "  1105,\n",
       "  29280,\n",
       "  170,\n",
       "  29063,\n",
       "  103,\n",
       "  1113,\n",
       "  29281,\n",
       "  2799,\n",
       "  1936,\n",
       "  10338,\n",
       "  1113,\n",
       "  1103,\n",
       "  11727,\n",
       "  1105,\n",
       "  170,\n",
       "  102],\n",
       " [101,\n",
       "  1110,\n",
       "  2781,\n",
       "  2112,\n",
       "  103,\n",
       "  29282,\n",
       "  1107,\n",
       "  2761,\n",
       "  1105,\n",
       "  2459,\n",
       "  5165,\n",
       "  1114,\n",
       "  29283,\n",
       "  1105,\n",
       "  29055,\n",
       "  1131,\n",
       "  1144,\n",
       "  170,\n",
       "  1607,\n",
       "  1104,\n",
       "  102,\n",
       "  29284,\n",
       "  29285,\n",
       "  29286,\n",
       "  29287,\n",
       "  29288,\n",
       "  1821,\n",
       "  29289,\n",
       "  1104,\n",
       "  1103,\n",
       "  29290,\n",
       "  102],\n",
       " [101,\n",
       "  2229,\n",
       "  103,\n",
       "  2330,\n",
       "  8682,\n",
       "  1105,\n",
       "  1173,\n",
       "  1119,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  1264,\n",
       "  172,\n",
       "  1111,\n",
       "  2635,\n",
       "  102,\n",
       "  1142,\n",
       "  1110,\n",
       "  1103,\n",
       "  1248,\n",
       "  29291,\n",
       "  2704,\n",
       "  10296,\n",
       "  1111,\n",
       "  1142,\n",
       "  1851,\n",
       "  1214,\n",
       "  1385,\n",
       "  1590,\n",
       "  1114,\n",
       "  170,\n",
       "  102],\n",
       " [101,\n",
       "  29156,\n",
       "  2592,\n",
       "  1107,\n",
       "  2115,\n",
       "  2799,\n",
       "  29292,\n",
       "  29293,\n",
       "  1104,\n",
       "  1103,\n",
       "  29172,\n",
       "  1114,\n",
       "  170,\n",
       "  1344,\n",
       "  3654,\n",
       "  19122,\n",
       "  29294,\n",
       "  1103,\n",
       "  5351,\n",
       "  1144,\n",
       "  1125,\n",
       "  4138,\n",
       "  5625,\n",
       "  1104,\n",
       "  29295,\n",
       "  102,\n",
       "  14701,\n",
       "  3112,\n",
       "  29206,\n",
       "  3807,\n",
       "  103,\n",
       "  102],\n",
       " [101,\n",
       "  6145,\n",
       "  1114,\n",
       "  1103,\n",
       "  1488,\n",
       "  1150,\n",
       "  1464,\n",
       "  1115,\n",
       "  1119,\n",
       "  1105,\n",
       "  1168,\n",
       "  1266,\n",
       "  1484,\n",
       "  1458,\n",
       "  1106,\n",
       "  29179,\n",
       "  1103,\n",
       "  5351,\n",
       "  29296,\n",
       "  1105,\n",
       "  3644,\n",
       "  17047,\n",
       "  102,\n",
       "  1131,\n",
       "  26360,\n",
       "  1251,\n",
       "  6272,\n",
       "  1137,\n",
       "  2472,\n",
       "  103,\n",
       "  1329,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  29297,\n",
       "  1131,\n",
       "  1125,\n",
       "  4138,\n",
       "  29121,\n",
       "  29298,\n",
       "  2229,\n",
       "  2489,\n",
       "  1105,\n",
       "  10496,\n",
       "  29299,\n",
       "  29300,\n",
       "  29098,\n",
       "  1108,\n",
       "  102,\n",
       "  29005,\n",
       "  1114,\n",
       "  170,\n",
       "  25550,\n",
       "  29006,\n",
       "  29007,\n",
       "  103,\n",
       "  22895,\n",
       "  7413,\n",
       "  1108,\n",
       "  8886,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  102],\n",
       " [101,\n",
       "  2781,\n",
       "  2112,\n",
       "  1268,\n",
       "  29301,\n",
       "  1607,\n",
       "  1104,\n",
       "  20752,\n",
       "  8936,\n",
       "  29011,\n",
       "  29302,\n",
       "  103,\n",
       "  170,\n",
       "  29012,\n",
       "  29013,\n",
       "  4680,\n",
       "  102,\n",
       "  2988,\n",
       "  1106,\n",
       "  10296,\n",
       "  1119,\n",
       "  1144,\n",
       "  1125,\n",
       "  1210,\n",
       "  3426,\n",
       "  1104,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  1114,\n",
       "  16320,\n",
       "  102],\n",
       " [101,\n",
       "  2241,\n",
       "  1108,\n",
       "  29029,\n",
       "  102,\n",
       "  1489,\n",
       "  1105,\n",
       "  29221,\n",
       "  1104,\n",
       "  29210,\n",
       "  29211,\n",
       "  103,\n",
       "  11911,\n",
       "  3053,\n",
       "  5715,\n",
       "  29212,\n",
       "  1104,\n",
       "  1489,\n",
       "  2904,\n",
       "  1542,\n",
       "  1703,\n",
       "  29213,\n",
       "  5190,\n",
       "  29239,\n",
       "  29240,\n",
       "  21121,\n",
       "  23651,\n",
       "  29303,\n",
       "  29304,\n",
       "  29305,\n",
       "  29306,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  27019,\n",
       "  1131,\n",
       "  1310,\n",
       "  4172,\n",
       "  22212,\n",
       "  29307,\n",
       "  7606,\n",
       "  1114,\n",
       "  29308,\n",
       "  1105,\n",
       "  29309,\n",
       "  102,\n",
       "  1552,\n",
       "  1229,\n",
       "  29308,\n",
       "  1108,\n",
       "  1598,\n",
       "  1111,\n",
       "  1421,\n",
       "  1552,\n",
       "  1235,\n",
       "  29310,\n",
       "  1103,\n",
       "  5351,\n",
       "  28996,\n",
       "  29311,\n",
       "  1304,\n",
       "  1218,\n",
       "  1114,\n",
       "  102],\n",
       " [101,\n",
       "  1218,\n",
       "  1235,\n",
       "  29312,\n",
       "  1104,\n",
       "  1630,\n",
       "  103,\n",
       "  1119,\n",
       "  1245,\n",
       "  29313,\n",
       "  1892,\n",
       "  8708,\n",
       "  1127,\n",
       "  3112,\n",
       "  1111,\n",
       "  29045,\n",
       "  102,\n",
       "  1565,\n",
       "  1989,\n",
       "  1736,\n",
       "  1104,\n",
       "  29093,\n",
       "  1105,\n",
       "  29314,\n",
       "  1108,\n",
       "  8318,\n",
       "  1892,\n",
       "  8708,\n",
       "  1127,\n",
       "  3112,\n",
       "  1111,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  29315,\n",
       "  1108,\n",
       "  8779,\n",
       "  1105,\n",
       "  2714,\n",
       "  1127,\n",
       "  1189,\n",
       "  1106,\n",
       "  1838,\n",
       "  1140,\n",
       "  1113,\n",
       "  29316,\n",
       "  102,\n",
       "  1285,\n",
       "  2988,\n",
       "  1106,\n",
       "  10296,\n",
       "  1119,\n",
       "  1144,\n",
       "  1125,\n",
       "  1210,\n",
       "  3426,\n",
       "  1104,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  103,\n",
       "  16320,\n",
       "  29317,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  1814,\n",
       "  1106,\n",
       "  1103,\n",
       "  17688,\n",
       "  6059,\n",
       "  12885,\n",
       "  1920,\n",
       "  2587,\n",
       "  1107,\n",
       "  6111,\n",
       "  3879,\n",
       "  102,\n",
       "  29318,\n",
       "  1285,\n",
       "  103,\n",
       "  1119,\n",
       "  1108,\n",
       "  1276,\n",
       "  1106,\n",
       "  1138,\n",
       "  1185,\n",
       "  2900,\n",
       "  6795,\n",
       "  1177,\n",
       "  1113,\n",
       "  29318,\n",
       "  1285,\n",
       "  123,\n",
       "  102],\n",
       " [101,\n",
       "  3090,\n",
       "  170,\n",
       "  1268,\n",
       "  29104,\n",
       "  2997,\n",
       "  1104,\n",
       "  1275,\n",
       "  170,\n",
       "  1268,\n",
       "  29319,\n",
       "  2997,\n",
       "  1104,\n",
       "  29320,\n",
       "  170,\n",
       "  26600,\n",
       "  29321,\n",
       "  2997,\n",
       "  1104,\n",
       "  1969,\n",
       "  170,\n",
       "  102,\n",
       "  1131,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29229,\n",
       "  24289,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  102],\n",
       " [101,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29229,\n",
       "  1111,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  1131,\n",
       "  1108,\n",
       "  29230,\n",
       "  1113,\n",
       "  1285,\n",
       "  122,\n",
       "  1105,\n",
       "  102,\n",
       "  1131,\n",
       "  1598,\n",
       "  1106,\n",
       "  1498,\n",
       "  1218,\n",
       "  1131,\n",
       "  1108,\n",
       "  1678,\n",
       "  1228,\n",
       "  29322,\n",
       "  1105,\n",
       "  1598,\n",
       "  1113,\n",
       "  29323,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  29324,\n",
       "  29325,\n",
       "  2226,\n",
       "  1285,\n",
       "  1131,\n",
       "  1108,\n",
       "  2382,\n",
       "  1106,\n",
       "  1129,\n",
       "  1107,\n",
       "  1105,\n",
       "  1149,\n",
       "  1104,\n",
       "  29104,\n",
       "  29105,\n",
       "  1443,\n",
       "  8006,\n",
       "  102,\n",
       "  23897,\n",
       "  1113,\n",
       "  103,\n",
       "  29149,\n",
       "  5190,\n",
       "  29011,\n",
       "  3839,\n",
       "  1266,\n",
       "  1607,\n",
       "  1136,\n",
       "  3836,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  2919,\n",
       "  1111,\n",
       "  3288,\n",
       "  2657,\n",
       "  2635,\n",
       "  1114,\n",
       "  25161,\n",
       "  1104,\n",
       "  1117,\n",
       "  29326,\n",
       "  103,\n",
       "  10745,\n",
       "  1113,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  8243,\n",
       "  1113,\n",
       "  29316,\n",
       "  1229,\n",
       "  1223,\n",
       "  10900,\n",
       "  1114,\n",
       "  17688,\n",
       "  8804,\n",
       "  1105,\n",
       "  1108,\n",
       "  27933,\n",
       "  102],\n",
       " [101,\n",
       "  11727,\n",
       "  5627,\n",
       "  1105,\n",
       "  22590,\n",
       "  1104,\n",
       "  1286,\n",
       "  29319,\n",
       "  1106,\n",
       "  29249,\n",
       "  29327,\n",
       "  29328,\n",
       "  1168,\n",
       "  8826,\n",
       "  29249,\n",
       "  7261,\n",
       "  102,\n",
       "  2853,\n",
       "  1736,\n",
       "  1119,\n",
       "  1108,\n",
       "  17428,\n",
       "  1107,\n",
       "  1103,\n",
       "  5241,\n",
       "  1395,\n",
       "  1276,\n",
       "  1106,\n",
       "  1129,\n",
       "  8669,\n",
       "  29028,\n",
       "  102],\n",
       " [101,\n",
       "  29055,\n",
       "  126,\n",
       "  29022,\n",
       "  29019,\n",
       "  29329,\n",
       "  29330,\n",
       "  4214,\n",
       "  29022,\n",
       "  29019,\n",
       "  103,\n",
       "  29331,\n",
       "  24854,\n",
       "  123,\n",
       "  1107,\n",
       "  29332,\n",
       "  1121,\n",
       "  29333,\n",
       "  1235,\n",
       "  29027,\n",
       "  25622,\n",
       "  1119,\n",
       "  1108,\n",
       "  15207,\n",
       "  1106,\n",
       "  1313,\n",
       "  102,\n",
       "  3879,\n",
       "  1113,\n",
       "  12398,\n",
       "  6111,\n",
       "  102],\n",
       " [101,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  1103,\n",
       "  29334,\n",
       "  2332,\n",
       "  1107,\n",
       "  29335,\n",
       "  1111,\n",
       "  15443,\n",
       "  29295,\n",
       "  1105,\n",
       "  8006,\n",
       "  1104,\n",
       "  29336,\n",
       "  102,\n",
       "  170,\n",
       "  29063,\n",
       "  29017,\n",
       "  1113,\n",
       "  29281,\n",
       "  2799,\n",
       "  1936,\n",
       "  10338,\n",
       "  1113,\n",
       "  103,\n",
       "  11727,\n",
       "  1105,\n",
       "  170,\n",
       "  10496,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  1607,\n",
       "  1310,\n",
       "  1107,\n",
       "  29337,\n",
       "  1104,\n",
       "  2164,\n",
       "  1165,\n",
       "  1119,\n",
       "  1872,\n",
       "  29338,\n",
       "  29085,\n",
       "  3718,\n",
       "  1106,\n",
       "  29051,\n",
       "  102,\n",
       "  170,\n",
       "  1353,\n",
       "  29339,\n",
       "  1856,\n",
       "  1119,\n",
       "  1108,\n",
       "  5165,\n",
       "  1114,\n",
       "  1210,\n",
       "  2277,\n",
       "  1104,\n",
       "  29340,\n",
       "  1105,\n",
       "  1173,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  4910,\n",
       "  2781,\n",
       "  12211,\n",
       "  1119,\n",
       "  1108,\n",
       "  29226,\n",
       "  1133,\n",
       "  29227,\n",
       "  7779,\n",
       "  1106,\n",
       "  1271,\n",
       "  103,\n",
       "  1105,\n",
       "  2704,\n",
       "  102,\n",
       "  186,\n",
       "  29111,\n",
       "  29112,\n",
       "  4214,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  29111,\n",
       "  29036,\n",
       "  1275,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  127,\n",
       "  102],\n",
       " [101,\n",
       "  1117,\n",
       "  9840,\n",
       "  1108,\n",
       "  1851,\n",
       "  4842,\n",
       "  1114,\n",
       "  1126,\n",
       "  13975,\n",
       "  15119,\n",
       "  3392,\n",
       "  3510,\n",
       "  102,\n",
       "  1106,\n",
       "  1138,\n",
       "  2012,\n",
       "  8006,\n",
       "  1104,\n",
       "  29336,\n",
       "  1762,\n",
       "  4290,\n",
       "  1118,\n",
       "  1607,\n",
       "  1134,\n",
       "  1127,\n",
       "  8080,\n",
       "  1114,\n",
       "  170,\n",
       "  13306,\n",
       "  103,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  1982,\n",
       "  8387,\n",
       "  170,\n",
       "  1664,\n",
       "  103,\n",
       "  29341,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  3175,\n",
       "  1166,\n",
       "  1106,\n",
       "  1103,\n",
       "  1555,\n",
       "  1187,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  29002,\n",
       "  2455,\n",
       "  1108,\n",
       "  29003,\n",
       "  8682,\n",
       "  1127,\n",
       "  2330,\n",
       "  2589,\n",
       "  1111,\n",
       "  1199,\n",
       "  7648,\n",
       "  10496,\n",
       "  102],\n",
       " [101,\n",
       "  2704,\n",
       "  1736,\n",
       "  1103,\n",
       "  5351,\n",
       "  9315,\n",
       "  29342,\n",
       "  18593,\n",
       "  13981,\n",
       "  29343,\n",
       "  193,\n",
       "  123,\n",
       "  1114,\n",
       "  20557,\n",
       "  29344,\n",
       "  29345,\n",
       "  1113,\n",
       "  103,\n",
       "  29346,\n",
       "  1106,\n",
       "  29172,\n",
       "  29346,\n",
       "  1106,\n",
       "  19122,\n",
       "  102,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  1443,\n",
       "  13035,\n",
       "  13522,\n",
       "  102],\n",
       " [101,\n",
       "  1107,\n",
       "  29347,\n",
       "  2229,\n",
       "  3090,\n",
       "  103,\n",
       "  29348,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  3105,\n",
       "  25163,\n",
       "  102,\n",
       "  1127,\n",
       "  3258,\n",
       "  1114,\n",
       "  1363,\n",
       "  29244,\n",
       "  27631,\n",
       "  29349,\n",
       "  12211,\n",
       "  1108,\n",
       "  1664,\n",
       "  17811,\n",
       "  29219,\n",
       "  10846,\n",
       "  25550,\n",
       "  1194,\n",
       "  29350,\n",
       "  1127,\n",
       "  29351,\n",
       "  102],\n",
       " [101,\n",
       "  1142,\n",
       "  1159,\n",
       "  1119,\n",
       "  1598,\n",
       "  1106,\n",
       "  19073,\n",
       "  1104,\n",
       "  7957,\n",
       "  2229,\n",
       "  2489,\n",
       "  1134,\n",
       "  1108,\n",
       "  29352,\n",
       "  7375,\n",
       "  1118,\n",
       "  29353,\n",
       "  29354,\n",
       "  29051,\n",
       "  29355,\n",
       "  1105,\n",
       "  29356,\n",
       "  102,\n",
       "  1175,\n",
       "  103,\n",
       "  1185,\n",
       "  29186,\n",
       "  2607,\n",
       "  1219,\n",
       "  1292,\n",
       "  3426,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  18855,\n",
       "  29249,\n",
       "  29357,\n",
       "  1134,\n",
       "  1108,\n",
       "  3055,\n",
       "  29358,\n",
       "  3975,\n",
       "  102,\n",
       "  1310,\n",
       "  1107,\n",
       "  29337,\n",
       "  1104,\n",
       "  2164,\n",
       "  1165,\n",
       "  1119,\n",
       "  1872,\n",
       "  29338,\n",
       "  29085,\n",
       "  3718,\n",
       "  1106,\n",
       "  29051,\n",
       "  3850,\n",
       "  6704,\n",
       "  1119,\n",
       "  1125,\n",
       "  5199,\n",
       "  29249,\n",
       "  29359,\n",
       "  102],\n",
       " [101,\n",
       "  1607,\n",
       "  29152,\n",
       "  1111,\n",
       "  3407,\n",
       "  1190,\n",
       "  1620,\n",
       "  5246,\n",
       "  1201,\n",
       "  29360,\n",
       "  16777,\n",
       "  1679,\n",
       "  1285,\n",
       "  193,\n",
       "  2588,\n",
       "  1201,\n",
       "  102,\n",
       "  29217,\n",
       "  27486,\n",
       "  1105,\n",
       "  29175,\n",
       "  1653,\n",
       "  1892,\n",
       "  3652,\n",
       "  29176,\n",
       "  1894,\n",
       "  1892,\n",
       "  3652,\n",
       "  29177,\n",
       "  15185,\n",
       "  29176,\n",
       "  102],\n",
       " [101,\n",
       "  103,\n",
       "  29361,\n",
       "  13585,\n",
       "  29362,\n",
       "  1106,\n",
       "  1268,\n",
       "  29342,\n",
       "  18593,\n",
       "  1108,\n",
       "  8581,\n",
       "  1117,\n",
       "  29361,\n",
       "  13585,\n",
       "  29362,\n",
       "  1106,\n",
       "  102,\n",
       "  29363,\n",
       "  170,\n",
       "  2587,\n",
       "  1295,\n",
       "  29364,\n",
       "  10296,\n",
       "  2236,\n",
       "  29288,\n",
       "  12398,\n",
       "  2236,\n",
       "  29365,\n",
       "  3981,\n",
       "  12645,\n",
       "  29289,\n",
       "  102],\n",
       " [101,\n",
       "  13093,\n",
       "  29366,\n",
       "  1108,\n",
       "  1982,\n",
       "  8387,\n",
       "  29367,\n",
       "  29289,\n",
       "  29368,\n",
       "  1120,\n",
       "  1103,\n",
       "  1159,\n",
       "  1131,\n",
       "  1145,\n",
       "  1125,\n",
       "  8006,\n",
       "  102,\n",
       "  28999,\n",
       "  3090,\n",
       "  2967,\n",
       "  20557,\n",
       "  29000,\n",
       "  1105,\n",
       "  20557,\n",
       "  29001,\n",
       "  170,\n",
       "  1268,\n",
       "  3105,\n",
       "  25163,\n",
       "  3367,\n",
       "  103,\n",
       "  102],\n",
       " [101,\n",
       "  1607,\n",
       "  1103,\n",
       "  5351,\n",
       "  2491,\n",
       "  1107,\n",
       "  29369,\n",
       "  1118,\n",
       "  1941,\n",
       "  1649,\n",
       "  1123,\n",
       "  6508,\n",
       "  2491,\n",
       "  1131,\n",
       "  3756,\n",
       "  1515,\n",
       "  103,\n",
       "  9302,\n",
       "  15640,\n",
       "  1103,\n",
       "  5351,\n",
       "  1144,\n",
       "  170,\n",
       "  1406,\n",
       "  9987,\n",
       "  1607,\n",
       "  102,\n",
       "  5351,\n",
       "  26360,\n",
       "  1954,\n",
       "  1329,\n",
       "  102],\n",
       " [101,\n",
       "  1218,\n",
       "  1105,\n",
       "  1108,\n",
       "  15207,\n",
       "  1106,\n",
       "  29370,\n",
       "  2755,\n",
       "  2704,\n",
       "  1105,\n",
       "  20562,\n",
       "  1107,\n",
       "  29371,\n",
       "  29372,\n",
       "  103,\n",
       "  29373,\n",
       "  102,\n",
       "  8682,\n",
       "  1105,\n",
       "  1117,\n",
       "  29374,\n",
       "  2799,\n",
       "  1140,\n",
       "  1106,\n",
       "  1129,\n",
       "  1120,\n",
       "  170,\n",
       "  2603,\n",
       "  1104,\n",
       "  2908,\n",
       "  1443,\n",
       "  102],\n",
       " [101,\n",
       "  29375,\n",
       "  27631,\n",
       "  1286,\n",
       "  29255,\n",
       "  29375,\n",
       "  29097,\n",
       "  29375,\n",
       "  17172,\n",
       "  29375,\n",
       "  29376,\n",
       "  29375,\n",
       "  103,\n",
       "  29375,\n",
       "  29377,\n",
       "  29375,\n",
       "  16530,\n",
       "  29378,\n",
       "  29375,\n",
       "  29379,\n",
       "  29380,\n",
       "  29375,\n",
       "  29381,\n",
       "  20557,\n",
       "  29382,\n",
       "  3839,\n",
       "  12503,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  29002,\n",
       "  102],\n",
       " [101,\n",
       "  1668,\n",
       "  1105,\n",
       "  26844,\n",
       "  1106,\n",
       "  1609,\n",
       "  1105,\n",
       "  11822,\n",
       "  29383,\n",
       "  5172,\n",
       "  1127,\n",
       "  9964,\n",
       "  1119,\n",
       "  1125,\n",
       "  170,\n",
       "  22538,\n",
       "  3661,\n",
       "  1133,\n",
       "  29384,\n",
       "  1108,\n",
       "  4303,\n",
       "  102,\n",
       "  1119,\n",
       "  1125,\n",
       "  1185,\n",
       "  29255,\n",
       "  29381,\n",
       "  1114,\n",
       "  1363,\n",
       "  20557,\n",
       "  103,\n",
       "  102],\n",
       " [101,\n",
       "  29243,\n",
       "  1185,\n",
       "  29034,\n",
       "  29375,\n",
       "  27631,\n",
       "  29385,\n",
       "  20346,\n",
       "  2527,\n",
       "  25158,\n",
       "  1406,\n",
       "  5962,\n",
       "  1108,\n",
       "  29185,\n",
       "  102,\n",
       "  1704,\n",
       "  1108,\n",
       "  170,\n",
       "  29028,\n",
       "  29029,\n",
       "  1299,\n",
       "  29030,\n",
       "  29031,\n",
       "  5731,\n",
       "  1659,\n",
       "  103,\n",
       "  9301,\n",
       "  5300,\n",
       "  1246,\n",
       "  1257,\n",
       "  4942,\n",
       "  102],\n",
       " [101,\n",
       "  29386,\n",
       "  1512,\n",
       "  29022,\n",
       "  29019,\n",
       "  29181,\n",
       "  29027,\n",
       "  1406,\n",
       "  29387,\n",
       "  29020,\n",
       "  29354,\n",
       "  103,\n",
       "  29038,\n",
       "  29315,\n",
       "  3055,\n",
       "  8779,\n",
       "  102,\n",
       "  29026,\n",
       "  1275,\n",
       "  29022,\n",
       "  29019,\n",
       "  29020,\n",
       "  2952,\n",
       "  8179,\n",
       "  1119,\n",
       "  1108,\n",
       "  170,\n",
       "  19455,\n",
       "  29388,\n",
       "  1299,\n",
       "  1107,\n",
       "  102],\n",
       " [101,\n",
       "  6406,\n",
       "  2229,\n",
       "  2489,\n",
       "  29034,\n",
       "  29146,\n",
       "  2489,\n",
       "  1137,\n",
       "  5625,\n",
       "  29389,\n",
       "  103,\n",
       "  1137,\n",
       "  5173,\n",
       "  2607,\n",
       "  1119,\n",
       "  3697,\n",
       "  29205,\n",
       "  1104,\n",
       "  102,\n",
       "  2952,\n",
       "  8179,\n",
       "  1892,\n",
       "  2997,\n",
       "  29390,\n",
       "  8561,\n",
       "  11150,\n",
       "  29130,\n",
       "  6795,\n",
       "  3307,\n",
       "  4652,\n",
       "  29391,\n",
       "  102],\n",
       " [101,\n",
       "  1276,\n",
       "  1106,\n",
       "  1129,\n",
       "  1107,\n",
       "  29104,\n",
       "  29105,\n",
       "  1114,\n",
       "  170,\n",
       "  2603,\n",
       "  1104,\n",
       "  103,\n",
       "  1106,\n",
       "  4214,\n",
       "  1105,\n",
       "  1114,\n",
       "  102,\n",
       "  1607,\n",
       "  1310,\n",
       "  1107,\n",
       "  29337,\n",
       "  1104,\n",
       "  2164,\n",
       "  1165,\n",
       "  1119,\n",
       "  1872,\n",
       "  29338,\n",
       "  29085,\n",
       "  3718,\n",
       "  1106,\n",
       "  29051,\n",
       "  102],\n",
       " [101,\n",
       "  100,\n",
       "  9174,\n",
       "  102,\n",
       "  1127,\n",
       "  1185,\n",
       "  13522,\n",
       "  1121,\n",
       "  1142,\n",
       "  7791,\n",
       "  1105,\n",
       "  1119,\n",
       "  1915,\n",
       "  1107,\n",
       "  2999,\n",
       "  29130,\n",
       "  6795,\n",
       "  1235,\n",
       "  1103,\n",
       "  1159,\n",
       "  1104,\n",
       "  12398,\n",
       "  1119,\n",
       "  1108,\n",
       "  29392,\n",
       "  1171,\n",
       "  1852,\n",
       "  103,\n",
       "  1105,\n",
       "  1680,\n",
       "  170,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  29063,\n",
       "  16278,\n",
       "  1694,\n",
       "  1120,\n",
       "  1142,\n",
       "  1159,\n",
       "  103,\n",
       "  1126,\n",
       "  29249,\n",
       "  29393,\n",
       "  19299,\n",
       "  102,\n",
       "  2704,\n",
       "  1486,\n",
       "  1140,\n",
       "  1105,\n",
       "  1464,\n",
       "  1115,\n",
       "  1114,\n",
       "  1117,\n",
       "  1763,\n",
       "  1607,\n",
       "  1104,\n",
       "  3850,\n",
       "  6704,\n",
       "  1115,\n",
       "  1119,\n",
       "  1108,\n",
       "  1136,\n",
       "  102],\n",
       " [101,\n",
       "  193,\n",
       "  1210,\n",
       "  1118,\n",
       "  29059,\n",
       "  29394,\n",
       "  1114,\n",
       "  1286,\n",
       "  4422,\n",
       "  29344,\n",
       "  18593,\n",
       "  29395,\n",
       "  16557,\n",
       "  16610,\n",
       "  29361,\n",
       "  13585,\n",
       "  29362,\n",
       "  1106,\n",
       "  1268,\n",
       "  29342,\n",
       "  18593,\n",
       "  29361,\n",
       "  13585,\n",
       "  29362,\n",
       "  1106,\n",
       "  102,\n",
       "  1934,\n",
       "  103,\n",
       "  2491,\n",
       "  1107,\n",
       "  29396,\n",
       "  102],\n",
       " [101,\n",
       "  2332,\n",
       "  1111,\n",
       "  29336,\n",
       "  1762,\n",
       "  103,\n",
       "  1104,\n",
       "  1126,\n",
       "  4138,\n",
       "  4844,\n",
       "  1107,\n",
       "  29397,\n",
       "  1105,\n",
       "  29398,\n",
       "  1105,\n",
       "  1173,\n",
       "  102,\n",
       "  1108,\n",
       "  1598,\n",
       "  1111,\n",
       "  1160,\n",
       "  1552,\n",
       "  1229,\n",
       "  29308,\n",
       "  1108,\n",
       "  1598,\n",
       "  1111,\n",
       "  1421,\n",
       "  1552,\n",
       "  1235,\n",
       "  29310,\n",
       "  102],\n",
       " [101,\n",
       "  5351,\n",
       "  1209,\n",
       "  1129,\n",
       "  15207,\n",
       "  1106,\n",
       "  1313,\n",
       "  1106,\n",
       "  3295,\n",
       "  7606,\n",
       "  1209,\n",
       "  1129,\n",
       "  1114,\n",
       "  29059,\n",
       "  4900,\n",
       "  1123,\n",
       "  2425,\n",
       "  1113,\n",
       "  29399,\n",
       "  12398,\n",
       "  102,\n",
       "  29400,\n",
       "  103,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  29021,\n",
       "  122,\n",
       "  17713,\n",
       "  29019,\n",
       "  29023,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  27146,\n",
       "  1104,\n",
       "  2999,\n",
       "  29401,\n",
       "  1373,\n",
       "  1114,\n",
       "  29402,\n",
       "  29403,\n",
       "  1126,\n",
       "  24716,\n",
       "  29404,\n",
       "  1108,\n",
       "  1982,\n",
       "  4000,\n",
       "  102,\n",
       "  1110,\n",
       "  2781,\n",
       "  2112,\n",
       "  20557,\n",
       "  103,\n",
       "  1107,\n",
       "  2761,\n",
       "  1105,\n",
       "  2459,\n",
       "  5165,\n",
       "  1114,\n",
       "  29283,\n",
       "  1105,\n",
       "  29055,\n",
       "  102],\n",
       " [101,\n",
       "  11344,\n",
       "  1132,\n",
       "  3426,\n",
       "  1104,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  1114,\n",
       "  2455,\n",
       "  2489,\n",
       "  1443,\n",
       "  103,\n",
       "  1133,\n",
       "  1114,\n",
       "  29153,\n",
       "  102,\n",
       "  1510,\n",
       "  2274,\n",
       "  29354,\n",
       "  29353,\n",
       "  1114,\n",
       "  3893,\n",
       "  1104,\n",
       "  1117,\n",
       "  2229,\n",
       "  2997,\n",
       "  1133,\n",
       "  1185,\n",
       "  1849,\n",
       "  1107,\n",
       "  102],\n",
       " [101,\n",
       "  1168,\n",
       "  12645,\n",
       "  29199,\n",
       "  29341,\n",
       "  102,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  1107,\n",
       "  1123,\n",
       "  4400,\n",
       "  1352,\n",
       "  103,\n",
       "  2332,\n",
       "  4004,\n",
       "  1103,\n",
       "  12645,\n",
       "  1104,\n",
       "  1268,\n",
       "  13093,\n",
       "  29367,\n",
       "  29289,\n",
       "  1165,\n",
       "  1131,\n",
       "  1310,\n",
       "  1106,\n",
       "  3689,\n",
       "  29121,\n",
       "  1105,\n",
       "  1113,\n",
       "  102],\n",
       " [101,\n",
       "  103,\n",
       "  1729,\n",
       "  1119,\n",
       "  1254,\n",
       "  1872,\n",
       "  29085,\n",
       "  1119,\n",
       "  1108,\n",
       "  4358,\n",
       "  5165,\n",
       "  1114,\n",
       "  29340,\n",
       "  1105,\n",
       "  1144,\n",
       "  1136,\n",
       "  102,\n",
       "  29324,\n",
       "  29325,\n",
       "  2226,\n",
       "  1285,\n",
       "  1131,\n",
       "  1108,\n",
       "  2382,\n",
       "  1106,\n",
       "  1129,\n",
       "  1107,\n",
       "  1105,\n",
       "  1149,\n",
       "  1104,\n",
       "  29104,\n",
       "  102],\n",
       " [101,\n",
       "  1106,\n",
       "  29405,\n",
       "  29406,\n",
       "  4029,\n",
       "  1118,\n",
       "  29407,\n",
       "  29408,\n",
       "  1111,\n",
       "  29059,\n",
       "  29409,\n",
       "  29410,\n",
       "  103,\n",
       "  9468,\n",
       "  29060,\n",
       "  29042,\n",
       "  29411,\n",
       "  29412,\n",
       "  29413,\n",
       "  29414,\n",
       "  29415,\n",
       "  29414,\n",
       "  29416,\n",
       "  102,\n",
       "  29417,\n",
       "  1108,\n",
       "  1145,\n",
       "  1982,\n",
       "  1120,\n",
       "  1115,\n",
       "  1159,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  29008,\n",
       "  13394,\n",
       "  1108,\n",
       "  1136,\n",
       "  10056,\n",
       "  102,\n",
       "  1117,\n",
       "  23897,\n",
       "  1127,\n",
       "  24423,\n",
       "  1166,\n",
       "  1103,\n",
       "  1736,\n",
       "  5157,\n",
       "  1103,\n",
       "  1378,\n",
       "  3615,\n",
       "  2005,\n",
       "  1118,\n",
       "  1378,\n",
       "  1103,\n",
       "  5806,\n",
       "  11934,\n",
       "  1103,\n",
       "  29418,\n",
       "  1108,\n",
       "  2886,\n",
       "  2856,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  29249,\n",
       "  29359,\n",
       "  1108,\n",
       "  2382,\n",
       "  1112,\n",
       "  1218,\n",
       "  1112,\n",
       "  170,\n",
       "  29419,\n",
       "  29420,\n",
       "  1286,\n",
       "  29066,\n",
       "  1105,\n",
       "  1126,\n",
       "  29008,\n",
       "  102,\n",
       "  29112,\n",
       "  4214,\n",
       "  103,\n",
       "  29019,\n",
       "  186,\n",
       "  29111,\n",
       "  29036,\n",
       "  1275,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  127,\n",
       "  29037,\n",
       "  29038,\n",
       "  102],\n",
       " [101,\n",
       "  29400,\n",
       "  2363,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  102,\n",
       "  17713,\n",
       "  29019,\n",
       "  29020,\n",
       "  1266,\n",
       "  1607,\n",
       "  1103,\n",
       "  5351,\n",
       "  3756,\n",
       "  1185,\n",
       "  1266,\n",
       "  1607,\n",
       "  1104,\n",
       "  4182,\n",
       "  1103,\n",
       "  1534,\n",
       "  1452,\n",
       "  1104,\n",
       "  15388,\n",
       "  1103,\n",
       "  1401,\n",
       "  1104,\n",
       "  1452,\n",
       "  1104,\n",
       "  29342,\n",
       "  102],\n",
       " [101,\n",
       "  29284,\n",
       "  29285,\n",
       "  29286,\n",
       "  29287,\n",
       "  29288,\n",
       "  1821,\n",
       "  29289,\n",
       "  1104,\n",
       "  1103,\n",
       "  29290,\n",
       "  102,\n",
       "  2884,\n",
       "  103,\n",
       "  3536,\n",
       "  9812,\n",
       "  1132,\n",
       "  4049,\n",
       "  12398,\n",
       "  14940,\n",
       "  29421,\n",
       "  29422,\n",
       "  12398,\n",
       "  14940,\n",
       "  1271,\n",
       "  29423,\n",
       "  29363,\n",
       "  170,\n",
       "  2587,\n",
       "  1295,\n",
       "  29364,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  5351,\n",
       "  1108,\n",
       "  1148,\n",
       "  11534,\n",
       "  1107,\n",
       "  29424,\n",
       "  1165,\n",
       "  1131,\n",
       "  2756,\n",
       "  1114,\n",
       "  1103,\n",
       "  8006,\n",
       "  1104,\n",
       "  103,\n",
       "  102,\n",
       "  1125,\n",
       "  8208,\n",
       "  11933,\n",
       "  29425,\n",
       "  3001,\n",
       "  1185,\n",
       "  29071,\n",
       "  10203,\n",
       "  1108,\n",
       "  6321,\n",
       "  29426,\n",
       "  29301,\n",
       "  1108,\n",
       "  1982,\n",
       "  102],\n",
       " [101,\n",
       "  11911,\n",
       "  2652,\n",
       "  29089,\n",
       "  24716,\n",
       "  2095,\n",
       "  1108,\n",
       "  29427,\n",
       "  1185,\n",
       "  10301,\n",
       "  15029,\n",
       "  29241,\n",
       "  1108,\n",
       "  29428,\n",
       "  4366,\n",
       "  102,\n",
       "  1131,\n",
       "  1144,\n",
       "  170,\n",
       "  1607,\n",
       "  1104,\n",
       "  29245,\n",
       "  1286,\n",
       "  1981,\n",
       "  1607,\n",
       "  1104,\n",
       "  29068,\n",
       "  1607,\n",
       "  1104,\n",
       "  1286,\n",
       "  2342,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  29253,\n",
       "  29254,\n",
       "  1104,\n",
       "  1117,\n",
       "  29117,\n",
       "  1268,\n",
       "  4422,\n",
       "  29255,\n",
       "  18593,\n",
       "  1107,\n",
       "  29335,\n",
       "  29429,\n",
       "  6059,\n",
       "  1107,\n",
       "  102,\n",
       "  5351,\n",
       "  1108,\n",
       "  1148,\n",
       "  11534,\n",
       "  1107,\n",
       "  29424,\n",
       "  1165,\n",
       "  1131,\n",
       "  2756,\n",
       "  1114,\n",
       "  1103,\n",
       "  8006,\n",
       "  16645,\n",
       "  29430,\n",
       "  102],\n",
       " [101,\n",
       "  29431,\n",
       "  4680,\n",
       "  29016,\n",
       "  23897,\n",
       "  103,\n",
       "  29112,\n",
       "  1620,\n",
       "  17713,\n",
       "  29019,\n",
       "  186,\n",
       "  29111,\n",
       "  102,\n",
       "  7413,\n",
       "  1108,\n",
       "  8886,\n",
       "  1107,\n",
       "  1103,\n",
       "  1268,\n",
       "  3105,\n",
       "  29088,\n",
       "  11911,\n",
       "  2652,\n",
       "  29089,\n",
       "  24716,\n",
       "  2095,\n",
       "  1108,\n",
       "  29427,\n",
       "  1185,\n",
       "  29432,\n",
       "  15029,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  2106,\n",
       "  1104,\n",
       "  10296,\n",
       "  1119,\n",
       "  1125,\n",
       "  1330,\n",
       "  2004,\n",
       "  1114,\n",
       "  103,\n",
       "  1104,\n",
       "  22882,\n",
       "  29153,\n",
       "  1105,\n",
       "  29433,\n",
       "  102,\n",
       "  1107,\n",
       "  29337,\n",
       "  1104,\n",
       "  2164,\n",
       "  1165,\n",
       "  1119,\n",
       "  1872,\n",
       "  29338,\n",
       "  29085,\n",
       "  3718,\n",
       "  1106,\n",
       "  29051,\n",
       "  3850,\n",
       "  6704,\n",
       "  102],\n",
       " [101,\n",
       "  1934,\n",
       "  1607,\n",
       "  1131,\n",
       "  8898,\n",
       "  103,\n",
       "  29434,\n",
       "  1551,\n",
       "  1679,\n",
       "  1989,\n",
       "  29435,\n",
       "  16777,\n",
       "  1679,\n",
       "  1285,\n",
       "  102,\n",
       "  23897,\n",
       "  29316,\n",
       "  2363,\n",
       "  29022,\n",
       "  29019,\n",
       "  29023,\n",
       "  1111,\n",
       "  1103,\n",
       "  1148,\n",
       "  1160,\n",
       "  2277,\n",
       "  18326,\n",
       "  1106,\n",
       "  2363,\n",
       "  29022,\n",
       "  29019,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  1103,\n",
       "  29278,\n",
       "  2332,\n",
       "  1113,\n",
       "  29279,\n",
       "  1105,\n",
       "  1973,\n",
       "  1113,\n",
       "  29246,\n",
       "  103,\n",
       "  29280,\n",
       "  102,\n",
       "  2274,\n",
       "  29354,\n",
       "  29353,\n",
       "  1114,\n",
       "  3893,\n",
       "  1104,\n",
       "  1117,\n",
       "  2229,\n",
       "  2997,\n",
       "  1133,\n",
       "  1185,\n",
       "  1849,\n",
       "  1107,\n",
       "  1168,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1145,\n",
       "  29436,\n",
       "  1104,\n",
       "  29121,\n",
       "  1113,\n",
       "  29122,\n",
       "  1105,\n",
       "  29437,\n",
       "  102,\n",
       "  1118,\n",
       "  29059,\n",
       "  6302,\n",
       "  1187,\n",
       "  1126,\n",
       "  29017,\n",
       "  3090,\n",
       "  1126,\n",
       "  29008,\n",
       "  13394,\n",
       "  1104,\n",
       "  29438,\n",
       "  29244,\n",
       "  11937,\n",
       "  2095,\n",
       "  29439,\n",
       "  123,\n",
       "  1106,\n",
       "  29440,\n",
       "  29441,\n",
       "  102],\n",
       " [101,\n",
       "  2229,\n",
       "  1108,\n",
       "  2330,\n",
       "  1106,\n",
       "  6316,\n",
       "  1105,\n",
       "  29442,\n",
       "  102,\n",
       "  24716,\n",
       "  12211,\n",
       "  3090,\n",
       "  13467,\n",
       "  15066,\n",
       "  1105,\n",
       "  23589,\n",
       "  29443,\n",
       "  29444,\n",
       "  11911,\n",
       "  1108,\n",
       "  1275,\n",
       "  29445,\n",
       "  103,\n",
       "  9346,\n",
       "  123,\n",
       "  1106,\n",
       "  124,\n",
       "  29445,\n",
       "  2071,\n",
       "  1103,\n",
       "  23298,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  1480,\n",
       "  1196,\n",
       "  10296,\n",
       "  1117,\n",
       "  1892,\n",
       "  2997,\n",
       "  1108,\n",
       "  29446,\n",
       "  1105,\n",
       "  8561,\n",
       "  1108,\n",
       "  5465,\n",
       "  102,\n",
       "  1261,\n",
       "  29354,\n",
       "  103,\n",
       "  1141,\n",
       "  1114,\n",
       "  1199,\n",
       "  3893,\n",
       "  1104,\n",
       "  1117,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  1133,\n",
       "  1185,\n",
       "  1849,\n",
       "  1107,\n",
       "  102],\n",
       " [101,\n",
       "  1103,\n",
       "  13585,\n",
       "  29447,\n",
       "  1127,\n",
       "  1737,\n",
       "  1106,\n",
       "  1129,\n",
       "  1304,\n",
       "  1601,\n",
       "  1105,\n",
       "  1104,\n",
       "  2869,\n",
       "  3068,\n",
       "  102,\n",
       "  1103,\n",
       "  29448,\n",
       "  1669,\n",
       "  1119,\n",
       "  1144,\n",
       "  1125,\n",
       "  29104,\n",
       "  29105,\n",
       "  1112,\n",
       "  1117,\n",
       "  29449,\n",
       "  6795,\n",
       "  1134,\n",
       "  1144,\n",
       "  1151,\n",
       "  5165,\n",
       "  102],\n",
       " [101,\n",
       "  29450,\n",
       "  170,\n",
       "  29199,\n",
       "  29200,\n",
       "  1108,\n",
       "  1982,\n",
       "  1105,\n",
       "  13592,\n",
       "  14402,\n",
       "  1104,\n",
       "  7201,\n",
       "  103,\n",
       "  1127,\n",
       "  2856,\n",
       "  29417,\n",
       "  1108,\n",
       "  1145,\n",
       "  1982,\n",
       "  1120,\n",
       "  102,\n",
       "  29284,\n",
       "  29285,\n",
       "  29286,\n",
       "  29287,\n",
       "  29288,\n",
       "  1821,\n",
       "  29289,\n",
       "  1104,\n",
       "  1103,\n",
       "  29290,\n",
       "  102],\n",
       " [101,\n",
       "  10846,\n",
       "  25550,\n",
       "  1194,\n",
       "  29350,\n",
       "  1127,\n",
       "  29351,\n",
       "  9964,\n",
       "  1105,\n",
       "  1127,\n",
       "  2999,\n",
       "  29451,\n",
       "  1127,\n",
       "  1750,\n",
       "  1190,\n",
       "  29452,\n",
       "  102,\n",
       "  8080,\n",
       "  1114,\n",
       "  170,\n",
       "  10338,\n",
       "  1133,\n",
       "  1185,\n",
       "  2747,\n",
       "  17102,\n",
       "  1108,\n",
       "  1276,\n",
       "  103,\n",
       "  1892,\n",
       "  8708,\n",
       "  1127,\n",
       "  102],\n",
       " [101,\n",
       "  29011,\n",
       "  29453,\n",
       "  1134,\n",
       "  4680,\n",
       "  22882,\n",
       "  1105,\n",
       "  29454,\n",
       "  102,\n",
       "  1106,\n",
       "  6246,\n",
       "  1112,\n",
       "  1119,\n",
       "  1245,\n",
       "  1167,\n",
       "  23024,\n",
       "  1105,\n",
       "  1750,\n",
       "  29227,\n",
       "  1105,\n",
       "  21,\n",
       "  1117,\n",
       "  29083,\n",
       "  29202,\n",
       "  11911,\n",
       "  4290,\n",
       "  12687,\n",
       "  1114,\n",
       "  1117,\n",
       "  29203,\n",
       "  29204,\n",
       "  102],\n",
       " [101,\n",
       "  1598,\n",
       "  1113,\n",
       "  29323,\n",
       "  1851,\n",
       "  29023,\n",
       "  1122,\n",
       "  1108,\n",
       "  6315,\n",
       "  103,\n",
       "  1123,\n",
       "  29323,\n",
       "  13753,\n",
       "  1129,\n",
       "  2569,\n",
       "  1106,\n",
       "  3453,\n",
       "  29023,\n",
       "  1123,\n",
       "  2229,\n",
       "  11182,\n",
       "  1127,\n",
       "  2023,\n",
       "  1107,\n",
       "  1496,\n",
       "  1106,\n",
       "  170,\n",
       "  1415,\n",
       "  102,\n",
       "  12398,\n",
       "  23897,\n",
       "  102],\n",
       " [101,\n",
       "  6531,\n",
       "  1105,\n",
       "  2846,\n",
       "  1106,\n",
       "  2767,\n",
       "  3718,\n",
       "  1106,\n",
       "  2603,\n",
       "  1126,\n",
       "  29455,\n",
       "  3325,\n",
       "  1108,\n",
       "  1136,\n",
       "  12503,\n",
       "  1175,\n",
       "  102,\n",
       "  1110,\n",
       "  2781,\n",
       "  3168,\n",
       "  20557,\n",
       "  29282,\n",
       "  1107,\n",
       "  2761,\n",
       "  1105,\n",
       "  2459,\n",
       "  5165,\n",
       "  1114,\n",
       "  29283,\n",
       "  1105,\n",
       "  29055,\n",
       "  102],\n",
       " [101,\n",
       "  1175,\n",
       "  1108,\n",
       "  1145,\n",
       "  1199,\n",
       "  103,\n",
       "  29456,\n",
       "  14928,\n",
       "  1131,\n",
       "  1460,\n",
       "  29220,\n",
       "  1105,\n",
       "  29311,\n",
       "  1114,\n",
       "  29457,\n",
       "  1105,\n",
       "  102,\n",
       "  170,\n",
       "  1246,\n",
       "  28999,\n",
       "  1108,\n",
       "  1145,\n",
       "  1982,\n",
       "  8387,\n",
       "  22172,\n",
       "  25163,\n",
       "  26052,\n",
       "  8080,\n",
       "  1114,\n",
       "  29219,\n",
       "  29458,\n",
       "  102],\n",
       " [101,\n",
       "  29059,\n",
       "  4900,\n",
       "  102,\n",
       "  29044,\n",
       "  29045,\n",
       "  29046,\n",
       "  103,\n",
       "  29459,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [101,\n",
       "  29460,\n",
       "  2539,\n",
       "  103,\n",
       "  29323,\n",
       "  1851,\n",
       "  29023,\n",
       "  29461,\n",
       "  4645,\n",
       "  29020,\n",
       "  10558,\n",
       "  1121,\n",
       "  29183,\n",
       "  29462,\n",
       "  29178,\n",
       "  29011,\n",
       "  1132,\n",
       "  29356,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  4358,\n",
       "  5165,\n",
       "  1114,\n",
       "  29340,\n",
       "  1105,\n",
       "  1144,\n",
       "  1136,\n",
       "  1215,\n",
       "  5557,\n",
       "  1290,\n",
       "  102],\n",
       " [101,\n",
       "  29097,\n",
       "  18593,\n",
       "  29463,\n",
       "  29464,\n",
       "  1108,\n",
       "  8080,\n",
       "  1114,\n",
       "  170,\n",
       "  10338,\n",
       "  1133,\n",
       "  1185,\n",
       "  2747,\n",
       "  17102,\n",
       "  1108,\n",
       "  1276,\n",
       "  102,\n",
       "  1105,\n",
       "  1120,\n",
       "  103,\n",
       "  1821,\n",
       "  1113,\n",
       "  29256,\n",
       "  1955,\n",
       "  1108,\n",
       "  8481,\n",
       "  2044,\n",
       "  29465,\n",
       "  29466,\n",
       "  29467,\n",
       "  29042,\n",
       "  102],\n",
       " [101,\n",
       "  12398,\n",
       "  2236,\n",
       "  29468,\n",
       "  1607,\n",
       "  1104,\n",
       "  1675,\n",
       "  6946,\n",
       "  1142,\n",
       "  1110,\n",
       "  1103,\n",
       "  1248,\n",
       "  29291,\n",
       "  2704,\n",
       "  10296,\n",
       "  1111,\n",
       "  1142,\n",
       "  1851,\n",
       "  1214,\n",
       "  1385,\n",
       "  1590,\n",
       "  102,\n",
       "  29207,\n",
       "  7035,\n",
       "  4463,\n",
       "  1105,\n",
       "  26844,\n",
       "  1106,\n",
       "  1609,\n",
       "  29190,\n",
       "  29191,\n",
       "  102],\n",
       " [101,\n",
       "  1106,\n",
       "  1103,\n",
       "  3389,\n",
       "  1395,\n",
       "  1113,\n",
       "  29469,\n",
       "  1105,\n",
       "  9315,\n",
       "  170,\n",
       "  29470,\n",
       "  29249,\n",
       "  11727,\n",
       "  5627,\n",
       "  1114,\n",
       "  170,\n",
       "  102,\n",
       "  1736,\n",
       "  1103,\n",
       "  103,\n",
       "  9315,\n",
       "  29342,\n",
       "  18593,\n",
       "  13981,\n",
       "  29343,\n",
       "  193,\n",
       "  123,\n",
       "  1114,\n",
       "  20557,\n",
       "  29344,\n",
       "  29345,\n",
       "  102],\n",
       " [101,\n",
       "  170,\n",
       "  29063,\n",
       "  29017,\n",
       "  1113,\n",
       "  29281,\n",
       "  2799,\n",
       "  1936,\n",
       "  10338,\n",
       "  1113,\n",
       "  1103,\n",
       "  11727,\n",
       "  1105,\n",
       "  170,\n",
       "  10496,\n",
       "  29393,\n",
       "  102,\n",
       "  1108,\n",
       "  3175,\n",
       "  1106,\n",
       "  170,\n",
       "  29263,\n",
       "  11138,\n",
       "  1313,\n",
       "  1106,\n",
       "  2335,\n",
       "  1117,\n",
       "  1565,\n",
       "  103,\n",
       "  1736,\n",
       "  1104,\n",
       "  102],\n",
       " [101,\n",
       "  2628,\n",
       "  12645,\n",
       "  29146,\n",
       "  14441,\n",
       "  8974,\n",
       "  1105,\n",
       "  103,\n",
       "  1104,\n",
       "  1103,\n",
       "  11911,\n",
       "  102,\n",
       "  1443,\n",
       "  13035,\n",
       "  13522,\n",
       "  1131,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29229,\n",
       "  1111,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  1131,\n",
       "  1108,\n",
       "  29230,\n",
       "  1113,\n",
       "  1285,\n",
       "  122,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  29207,\n",
       "  7035,\n",
       "  4463,\n",
       "  1668,\n",
       "  1105,\n",
       "  26844,\n",
       "  1106,\n",
       "  103,\n",
       "  2999,\n",
       "  29471,\n",
       "  2455,\n",
       "  29003,\n",
       "  102,\n",
       "  1106,\n",
       "  29442,\n",
       "  29385,\n",
       "  1185,\n",
       "  29004,\n",
       "  29472,\n",
       "  2366,\n",
       "  2603,\n",
       "  1105,\n",
       "  6795,\n",
       "  1185,\n",
       "  26792,\n",
       "  14701,\n",
       "  3112,\n",
       "  29206,\n",
       "  3807,\n",
       "  2991,\n",
       "  102],\n",
       " [101,\n",
       "  29219,\n",
       "  29220,\n",
       "  103,\n",
       "  1982,\n",
       "  1113,\n",
       "  29473,\n",
       "  1126,\n",
       "  29017,\n",
       "  3090,\n",
       "  170,\n",
       "  29199,\n",
       "  29341,\n",
       "  1103,\n",
       "  5351,\n",
       "  1125,\n",
       "  102,\n",
       "  1106,\n",
       "  29188,\n",
       "  6059,\n",
       "  1114,\n",
       "  8006,\n",
       "  1104,\n",
       "  29121,\n",
       "  1105,\n",
       "  170,\n",
       "  29199,\n",
       "  29341,\n",
       "  3090,\n",
       "  1113,\n",
       "  29017,\n",
       "  102],\n",
       " [101,\n",
       "  1106,\n",
       "  15187,\n",
       "  1114,\n",
       "  170,\n",
       "  23589,\n",
       "  29474,\n",
       "  1485,\n",
       "  1103,\n",
       "  3811,\n",
       "  1120,\n",
       "  2532,\n",
       "  4842,\n",
       "  17688,\n",
       "  12211,\n",
       "  3090,\n",
       "  102,\n",
       "  1103,\n",
       "  29475,\n",
       "  1108,\n",
       "  29476,\n",
       "  13577,\n",
       "  1107,\n",
       "  1103,\n",
       "  2286,\n",
       "  29477,\n",
       "  1413,\n",
       "  1114,\n",
       "  1126,\n",
       "  16557,\n",
       "  29478,\n",
       "  102],\n",
       " [101,\n",
       "  1119,\n",
       "  1108,\n",
       "  2752,\n",
       "  1171,\n",
       "  1106,\n",
       "  29059,\n",
       "  29060,\n",
       "  1111,\n",
       "  29470,\n",
       "  29249,\n",
       "  11727,\n",
       "  5627,\n",
       "  102,\n",
       "  29002,\n",
       "  2455,\n",
       "  1108,\n",
       "  29003,\n",
       "  8682,\n",
       "  1127,\n",
       "  2330,\n",
       "  2589,\n",
       "  1111,\n",
       "  1199,\n",
       "  103,\n",
       "  10496,\n",
       "  29004,\n",
       "  17688,\n",
       "  29005,\n",
       "  1114,\n",
       "  170,\n",
       "  102],\n",
       " [101,\n",
       "  29356,\n",
       "  1105,\n",
       "  1353,\n",
       "  7919,\n",
       "  1104,\n",
       "  29479,\n",
       "  1105,\n",
       "  1120,\n",
       "  125,\n",
       "  1821,\n",
       "  1113,\n",
       "  29256,\n",
       "  1955,\n",
       "  1108,\n",
       "  8481,\n",
       "  2044,\n",
       "  29465,\n",
       "  29466,\n",
       "  29467,\n",
       "  29042,\n",
       "  29411,\n",
       "  29480,\n",
       "  29413,\n",
       "  29481,\n",
       "  29415,\n",
       "  29482,\n",
       "  102,\n",
       "  103,\n",
       "  4680,\n",
       "  29016,\n",
       "  102],\n",
       " [101,\n",
       "  1551,\n",
       "  1679,\n",
       "  1480,\n",
       "  1133,\n",
       "  1185,\n",
       "  20085,\n",
       "  1105,\n",
       "  1185,\n",
       "  2841,\n",
       "  4361,\n",
       "  1780,\n",
       "  1119,\n",
       "  1144,\n",
       "  1125,\n",
       "  2773,\n",
       "  102,\n",
       "  1107,\n",
       "  2115,\n",
       "  2799,\n",
       "  29292,\n",
       "  29293,\n",
       "  1104,\n",
       "  1103,\n",
       "  29172,\n",
       "  1114,\n",
       "  170,\n",
       "  1344,\n",
       "  3654,\n",
       "  19122,\n",
       "  29294,\n",
       "  102],\n",
       " [101,\n",
       "  29074,\n",
       "  5538,\n",
       "  1110,\n",
       "  170,\n",
       "  1653,\n",
       "  2581,\n",
       "  2781,\n",
       "  2112,\n",
       "  29259,\n",
       "  29260,\n",
       "  193,\n",
       "  1160,\n",
       "  2781,\n",
       "  2112,\n",
       "  29342,\n",
       "  18593,\n",
       "  13981,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  1549,\n",
       "  170,\n",
       "  27146,\n",
       "  1104,\n",
       "  2999,\n",
       "  29401,\n",
       "  1373,\n",
       "  1114,\n",
       "  103,\n",
       "  29403,\n",
       "  102],\n",
       " [101,\n",
       "  10296,\n",
       "  29103,\n",
       "  2999,\n",
       "  29130,\n",
       "  6795,\n",
       "  1120,\n",
       "  4573,\n",
       "  1268,\n",
       "  15119,\n",
       "  3392,\n",
       "  29483,\n",
       "  29484,\n",
       "  14078,\n",
       "  20085,\n",
       "  29485,\n",
       "  102,\n",
       "  29205,\n",
       "  1104,\n",
       "  2184,\n",
       "  1114,\n",
       "  2455,\n",
       "  2489,\n",
       "  1443,\n",
       "  8432,\n",
       "  1133,\n",
       "  1114,\n",
       "  29153,\n",
       "  29433,\n",
       "  1105,\n",
       "  1199,\n",
       "  102],\n",
       " [101,\n",
       "  1125,\n",
       "  170,\n",
       "  29486,\n",
       "  103,\n",
       "  2129,\n",
       "  29418,\n",
       "  1413,\n",
       "  1973,\n",
       "  1113,\n",
       "  29318,\n",
       "  1285,\n",
       "  126,\n",
       "  1111,\n",
       "  29487,\n",
       "  29488,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  29002,\n",
       "  2455,\n",
       "  1108,\n",
       "  29003,\n",
       "  8682,\n",
       "  1127,\n",
       "  2330,\n",
       "  2589,\n",
       "  1111,\n",
       "  1199,\n",
       "  7648,\n",
       "  10496,\n",
       "  102]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['681',\n",
       " '641',\n",
       " '757',\n",
       " '704',\n",
       " '704',\n",
       " '641',\n",
       " '643',\n",
       " '681',\n",
       " '681',\n",
       " '757',\n",
       " '681',\n",
       " '704',\n",
       " '643',\n",
       " '681',\n",
       " '704',\n",
       " '643',\n",
       " '704',\n",
       " '681',\n",
       " '641',\n",
       " '757',\n",
       " '704',\n",
       " '641',\n",
       " '681',\n",
       " '681',\n",
       " '757',\n",
       " '681',\n",
       " '681',\n",
       " '704',\n",
       " '643',\n",
       " '681',\n",
       " '643',\n",
       " '641',\n",
       " '681',\n",
       " '757',\n",
       " '704',\n",
       " '641',\n",
       " '643',\n",
       " '681',\n",
       " '643',\n",
       " '643',\n",
       " '704',\n",
       " '641',\n",
       " '643',\n",
       " '704',\n",
       " '681',\n",
       " '681',\n",
       " '681',\n",
       " '704',\n",
       " '641',\n",
       " '641',\n",
       " '704',\n",
       " '681',\n",
       " '704',\n",
       " '757',\n",
       " '641',\n",
       " '757',\n",
       " '641',\n",
       " '643',\n",
       " '643',\n",
       " '641',\n",
       " '643',\n",
       " '681',\n",
       " '704',\n",
       " '681',\n",
       " '704',\n",
       " '757',\n",
       " '757',\n",
       " '704',\n",
       " '681',\n",
       " '704',\n",
       " '704',\n",
       " '681',\n",
       " '641',\n",
       " '704',\n",
       " '643',\n",
       " '757',\n",
       " '643',\n",
       " '704',\n",
       " '704',\n",
       " '704',\n",
       " '704',\n",
       " '643',\n",
       " '643',\n",
       " '681',\n",
       " '681',\n",
       " '704',\n",
       " '643',\n",
       " '704',\n",
       " '681',\n",
       " '704',\n",
       " '704',\n",
       " '681',\n",
       " '704',\n",
       " '704',\n",
       " '643',\n",
       " '641',\n",
       " '704',\n",
       " '643',\n",
       " '681',\n",
       " '681',\n",
       " '704',\n",
       " '681',\n",
       " '643',\n",
       " '641',\n",
       " '643',\n",
       " '641',\n",
       " '704',\n",
       " '643',\n",
       " '704',\n",
       " '757',\n",
       " '681',\n",
       " '704',\n",
       " '704',\n",
       " '704',\n",
       " '704',\n",
       " '643',\n",
       " '704',\n",
       " '704',\n",
       " '757',\n",
       " '704',\n",
       " '643',\n",
       " '643',\n",
       " '757',\n",
       " '681',\n",
       " '757',\n",
       " '681',\n",
       " '681',\n",
       " '641',\n",
       " '643',\n",
       " '643',\n",
       " '704',\n",
       " '681',\n",
       " '641',\n",
       " '704',\n",
       " '704',\n",
       " '681',\n",
       " '681']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['train'].dataset['note_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 28996,  1142,  7791,  1218,  1105,  1108, 28997,  1106,  1103,\n",
       "         17688,  6059,  2585,  1205,  1111,  1748,   102,  1113, 28998,  1744,\n",
       "           170,  2229, 28999,  3090,  2967, 20557, 29000,  1105,   103, 29001,\n",
       "           170,   102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'next_sentence_label': tensor([1]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 20557,  -100,\n",
       "          -100,  -100])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt['train'].__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([2, 32]),\n",
       " 'attention_mask': torch.Size([2, 32]),\n",
       " 'token_type_ids': torch.Size([2, 32]),\n",
       " 'masked_lm_positions': torch.Size([2, 1]),\n",
       " 'masked_lm_ids': torch.Size([2, 1]),\n",
       " 'masked_lm_weights': torch.Size([2, 1]),\n",
       " 'next_sentence_label': torch.Size([2, 1]),\n",
       " 'labels': torch.Size([2, 32])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "train_dataloader = DataLoader(dt, batch_size=2)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForNextSentencePrediction, AutoModel, BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "# checkpoint = 'bert-base-uncased'\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "# lm_model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "# nsp_model = AutoModelForNextSentencePrediction.from_pretrained(checkpoint)\n",
    "model = BertForPreTraining.from_pretrained(checkpoint)\n",
    "\n",
    "# model\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example['sentence'])\n",
    "\n",
    "# tkn_dt = mydata.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29067"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(29067, 768)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], token_type_ids=batch['token_type_ids'],\n",
    "           next_sentence_label=batch['next_sentence_label'], labels=batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTrainingOutput(loss=tensor(6.4335, grad_fn=<AddBackward0>), prediction_logits=tensor([[[ -5.5433,  -5.7473,  -5.2080,  ...,  -0.2150,   0.6874,   0.4766],\n",
       "         [ -3.1968,  -2.7779,  -3.8491,  ...,   1.2901,  -0.4080,   0.7658],\n",
       "         [-10.6632, -12.3199, -10.2557,  ...,   0.5584,  -0.7573,   0.0931],\n",
       "         ...,\n",
       "         [ -7.9459,  -8.5186,  -7.6065,  ...,   0.7876,  -0.2686,   0.6793],\n",
       "         [ -4.4783,  -4.7499,  -5.7213,  ...,   1.5266,   0.6093,   1.1534],\n",
       "         [ -4.1019,  -4.7829,  -5.0443,  ...,  -0.3381,  -0.3011,  -0.8171]],\n",
       "\n",
       "        [[ -4.9559,  -5.8952,  -4.3157,  ...,   0.9165,   0.1068,   0.3251],\n",
       "         [ -5.6556,  -7.7267,  -5.6868,  ...,   3.3330,   0.2633,  -0.9636],\n",
       "         [ -2.5063,  -5.4581,  -4.3001,  ...,   1.7406,  -0.1276,  -0.1048],\n",
       "         ...,\n",
       "         [-10.6349, -11.4123, -11.5160,  ...,   1.5977,  -0.0950,   0.3320],\n",
       "         [ -3.4796,  -6.6407,  -5.6742,  ...,   0.5350,   1.6468,  -0.1994],\n",
       "         [ -3.5459,  -4.2667,  -2.0309,  ...,   0.7970,   0.1199,  -0.1982]]],\n",
       "       grad_fn=<AddBackward0>), seq_relationship_logits=tensor([[-3.4879,  3.7552],\n",
       "        [ 4.3912, -4.4233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logits = out.prediction_logits[1][np.where(batch['labels'][1] != -100)]\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lad']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'subtotal',\n",
       " 'occlusion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rca',\n",
       " 'with',\n",
       " 'a',\n",
       " 'high',\n",
       " 'grade',\n",
       " 'lad',\n",
       " 'lesion',\n",
       " 'the',\n",
       " 'patient',\n",
       " 'has',\n",
       " 'had',\n",
       " '[SEP]',\n",
       " 'referred',\n",
       " 'for',\n",
       " 'laser',\n",
       " 'angiography',\n",
       " 'in',\n",
       " '11/89',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'not',\n",
       " 'done',\n",
       " 'secondasry',\n",
       " 'to',\n",
       " 'extent',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(batch['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "        19122,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lad']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([batch['labels'][1][10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2293, 14255, 18684,  2099,  1012,   102,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  1045,  2123,  1005,  1056,  2729,  2005, 14255, 18684,  2099,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.input_ids = [dt[\"input_ids\"] for dt in dataset.values()]\n",
    "        # self.input_mask = dataset[\"input_mask\"]\n",
    "        # self.segment_ids = dataset[\"segment_ids\"]\n",
    "        # self.masked_lm_positions = dataset[\"masked_lm_positions\"]\n",
    "        # self.masked_lm_ids = dataset[\"masked_lm_ids\"]\n",
    "        # self.masked_lm_weights = dataset[\"masked_lm_weights\"]\n",
    "        # self.next_sentence_labels = dataset[\"next_sentence_labels\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in dataset[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "train_dataset = MyDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,   103,  1607,  1104,  1675,  6946,  1142,  1110,  1103,  1248,\n",
       "         28996,  2704, 10296,  1111,  1142,  1851,  1214,  1385,  1590,  1114,\n",
       "           170,  1607,  1104, 28997, 13468,  9318, 20557,   102,  1119,  1108,\n",
       "         28998,   102]),\n",
       " 'input_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1]),\n",
       " 'masked_lm_positions': tensor([1]),\n",
       " 'masked_lm_ids': tensor([100]),\n",
       " 'masked_lm_weights': tensor([1.]),\n",
       " 'next_sentence_labels': tensor([1])}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  103,\n",
       "  1607,\n",
       "  1104,\n",
       "  1675,\n",
       "  6946,\n",
       "  1142,\n",
       "  1110,\n",
       "  1103,\n",
       "  1248,\n",
       "  28996,\n",
       "  2704,\n",
       "  10296,\n",
       "  1111,\n",
       "  1142,\n",
       "  1851,\n",
       "  1214,\n",
       "  1385,\n",
       "  1590,\n",
       "  1114,\n",
       "  170,\n",
       "  1607,\n",
       "  1104,\n",
       "  28997,\n",
       "  13468,\n",
       "  9318,\n",
       "  20557,\n",
       "  102,\n",
       "  1119,\n",
       "  1108,\n",
       "  28998,\n",
       "  102],\n",
       " [101,\n",
       "  28999,\n",
       "  29000,\n",
       "  1104,\n",
       "  1103,\n",
       "  29001,\n",
       "  1114,\n",
       "  170,\n",
       "  1344,\n",
       "  3654,\n",
       "  19122,\n",
       "  29002,\n",
       "  1103,\n",
       "  5351,\n",
       "  1144,\n",
       "  1125,\n",
       "  102,\n",
       "  2752,\n",
       "  1111,\n",
       "  10221,\n",
       "  29003,\n",
       "  1107,\n",
       "  29004,\n",
       "  1133,\n",
       "  1122,\n",
       "  1108,\n",
       "  1136,\n",
       "  1694,\n",
       "  29005,\n",
       "  1106,\n",
       "  6102,\n",
       "  102],\n",
       " [101,\n",
       "  29006,\n",
       "  29007,\n",
       "  19122,\n",
       "  29008,\n",
       "  29009,\n",
       "  29008,\n",
       "  29001,\n",
       "  15543,\n",
       "  16530,\n",
       "  17811,\n",
       "  15543,\n",
       "  29010,\n",
       "  29011,\n",
       "  29012,\n",
       "  29013,\n",
       "  102,\n",
       "  1763,\n",
       "  2657,\n",
       "  1607,\n",
       "  103,\n",
       "  1111,\n",
       "  1112,\n",
       "  1807,\n",
       "  28997,\n",
       "  13468,\n",
       "  29014,\n",
       "  29015,\n",
       "  29016,\n",
       "  29017,\n",
       "  1105,\n",
       "  102],\n",
       " [101,\n",
       "  103,\n",
       "  1110,\n",
       "  2781,\n",
       "  2112,\n",
       "  20557,\n",
       "  29018,\n",
       "  1107,\n",
       "  2761,\n",
       "  1105,\n",
       "  2459,\n",
       "  5165,\n",
       "  1114,\n",
       "  29019,\n",
       "  1105,\n",
       "  29020,\n",
       "  102,\n",
       "  170,\n",
       "  1607,\n",
       "  1104,\n",
       "  29021,\n",
       "  1286,\n",
       "  1981,\n",
       "  1607,\n",
       "  1104,\n",
       "  29022,\n",
       "  1607,\n",
       "  1104,\n",
       "  1286,\n",
       "  2342,\n",
       "  29023,\n",
       "  102],\n",
       " [101,\n",
       "  1131,\n",
       "  1144,\n",
       "  1309,\n",
       "  1151,\n",
       "  1113,\n",
       "  29024,\n",
       "  1763,\n",
       "  13467,\n",
       "  1607,\n",
       "  2781,\n",
       "  2112,\n",
       "  29025,\n",
       "  29026,\n",
       "  29027,\n",
       "  102,\n",
       "  29028,\n",
       "  1744,\n",
       "  170,\n",
       "  2229,\n",
       "  29029,\n",
       "  3090,\n",
       "  2967,\n",
       "  20557,\n",
       "  29030,\n",
       "  1105,\n",
       "  20557,\n",
       "  29031,\n",
       "  170,\n",
       "  1268,\n",
       "  3105,\n",
       "  102],\n",
       " [101,\n",
       "  23897,\n",
       "  29032,\n",
       "  2539,\n",
       "  29033,\n",
       "  29034,\n",
       "  1851,\n",
       "  29033,\n",
       "  29035,\n",
       "  4645,\n",
       "  29036,\n",
       "  10558,\n",
       "  1121,\n",
       "  29037,\n",
       "  29038,\n",
       "  29039,\n",
       "  102,\n",
       "  4940,\n",
       "  1106,\n",
       "  1103,\n",
       "  2704,\n",
       "  1118,\n",
       "  1117,\n",
       "  1488,\n",
       "  1272,\n",
       "  1104,\n",
       "  2103,\n",
       "  29040,\n",
       "  1105,\n",
       "  1704,\n",
       "  1920,\n",
       "  102],\n",
       " [101,\n",
       "  29041,\n",
       "  1132,\n",
       "  29042,\n",
       "  17573,\n",
       "  29043,\n",
       "  102,\n",
       "  8898,\n",
       "  6272,\n",
       "  29044,\n",
       "  1551,\n",
       "  1679,\n",
       "  1989,\n",
       "  29045,\n",
       "  16777,\n",
       "  1679,\n",
       "  1285,\n",
       "  2704,\n",
       "  1736,\n",
       "  1103,\n",
       "  5351,\n",
       "  9315,\n",
       "  29046,\n",
       "  18593,\n",
       "  13981,\n",
       "  29047,\n",
       "  193,\n",
       "  123,\n",
       "  1114,\n",
       "  20557,\n",
       "  29048,\n",
       "  102],\n",
       " [101,\n",
       "  29049,\n",
       "  1106,\n",
       "  29001,\n",
       "  29049,\n",
       "  1106,\n",
       "  19122,\n",
       "  103,\n",
       "  5351,\n",
       "  1108,\n",
       "  1443,\n",
       "  13035,\n",
       "  13522,\n",
       "  102,\n",
       "  1117,\n",
       "  1607,\n",
       "  1310,\n",
       "  1107,\n",
       "  29050,\n",
       "  1104,\n",
       "  2164,\n",
       "  1165,\n",
       "  1119,\n",
       "  1872,\n",
       "  29051,\n",
       "  29052,\n",
       "  3718,\n",
       "  1106,\n",
       "  29053,\n",
       "  3850,\n",
       "  6704,\n",
       "  102],\n",
       " [101,\n",
       "  1131,\n",
       "  1108,\n",
       "  4120,\n",
       "  1106,\n",
       "  29054,\n",
       "  1111,\n",
       "  1892,\n",
       "  2997,\n",
       "  1654,\n",
       "  102,\n",
       "  29055,\n",
       "  25643,\n",
       "  1285,\n",
       "  122,\n",
       "  1105,\n",
       "  29056,\n",
       "  1108,\n",
       "  7087,\n",
       "  1131,\n",
       "  1598,\n",
       "  1106,\n",
       "  1202,\n",
       "  1218,\n",
       "  1131,\n",
       "  1108,\n",
       "  1678,\n",
       "  1228,\n",
       "  29057,\n",
       "  1105,\n",
       "  1598,\n",
       "  102],\n",
       " [101,\n",
       "  1113,\n",
       "  29058,\n",
       "  29059,\n",
       "  2226,\n",
       "  1285,\n",
       "  1131,\n",
       "  1108,\n",
       "  2382,\n",
       "  1106,\n",
       "  1129,\n",
       "  1107,\n",
       "  1105,\n",
       "  1149,\n",
       "  1104,\n",
       "  29060,\n",
       "  102,\n",
       "  1108,\n",
       "  29061,\n",
       "  1141,\n",
       "  2587,\n",
       "  1104,\n",
       "  8733,\n",
       "  1894,\n",
       "  1892,\n",
       "  3652,\n",
       "  1111,\n",
       "  170,\n",
       "  29062,\n",
       "  1104,\n",
       "  1695,\n",
       "  102],\n",
       " [101,\n",
       "  1851,\n",
       "  29033,\n",
       "  1122,\n",
       "  1108,\n",
       "  6315,\n",
       "  1115,\n",
       "  1123,\n",
       "  103,\n",
       "  13753,\n",
       "  1129,\n",
       "  2569,\n",
       "  1106,\n",
       "  3453,\n",
       "  29033,\n",
       "  1123,\n",
       "  2229,\n",
       "  11182,\n",
       "  1127,\n",
       "  2023,\n",
       "  1107,\n",
       "  102,\n",
       "  1119,\n",
       "  1225,\n",
       "  1218,\n",
       "  1133,\n",
       "  1598,\n",
       "  1106,\n",
       "  1329,\n",
       "  29053,\n",
       "  5557,\n",
       "  102],\n",
       " [101,\n",
       "  2229,\n",
       "  2799,\n",
       "  20557,\n",
       "  29063,\n",
       "  103,\n",
       "  1353,\n",
       "  29010,\n",
       "  29064,\n",
       "  1317,\n",
       "  1552,\n",
       "  102,\n",
       "  1103,\n",
       "  2229,\n",
       "  11182,\n",
       "  1127,\n",
       "  4441,\n",
       "  1235,\n",
       "  29065,\n",
       "  1120,\n",
       "  1134,\n",
       "  1159,\n",
       "  1152,\n",
       "  1127,\n",
       "  1865,\n",
       "  29066,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq):\n",
    "    \"\"\"Create input data from `TrainingInstance`s.\"\"\"\n",
    "\n",
    "    # writers = []\n",
    "    # for output_file in output_files:\n",
    "    #     writers.append(tf.compat.v1.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "    writer_index = 0\n",
    "\n",
    "    total_written = 0\n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        print(instance.tokens)\n",
    "        tokenizer.add_tokens(instance.tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(\n",
    "            instance.tokens)  # update vocab with new words given that the tokenization has been done with the raw split(' ') [to modify]\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        masked_lm_positions = list(instance.masked_lm_positions)\n",
    "        masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "        while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "            masked_lm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "            masked_lm_weights.append(0.0)\n",
    "\n",
    "        next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "        features = dict()\n",
    "        features[\"input_ids\"] = input_ids\n",
    "        features[\"input_mask\"] = input_mask\n",
    "        features[\"segment_ids\"] = segment_ids\n",
    "        features[\"masked_lm_positions\"] = masked_lm_positions\n",
    "        features[\"masked_lm_ids\"] = masked_lm_ids\n",
    "        features[\"masked_lm_weights\"] = masked_lm_weights\n",
    "        features[\"next_sentence_labels\"] = [next_sentence_label]\n",
    "        # print(features)\n",
    "        dataset = Dataset.from_dict(features)\n",
    "        return dataset\n",
    "    #\n",
    "    #     tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    #\n",
    "    #     writers[writer_index].write(tf_example.SerializeToString())\n",
    "    #     writer_index = (writer_index + 1) % len(writers)\n",
    "    #\n",
    "    #     total_written += 1\n",
    "    #\n",
    "    #     if inst_index < 20:\n",
    "    #         tf.compat.v1.logging.info(\"*** Example ***\")\n",
    "    #         tf.compat.v1.logging.info(\"tokens: %s\" % \" \".join(\n",
    "    #             [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "    #\n",
    "    #         for feature_name in features.keys():\n",
    "    #             feature = features[feature_name]\n",
    "    #             values = []\n",
    "    #             if feature.int64_list.value:\n",
    "    #                 values = feature.int64_list.value\n",
    "    #             elif feature.float_list.value:\n",
    "    #                 values = feature.float_list.value\n",
    "    #             tf.compat.v1.logging.info(\n",
    "    #                 \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "    #\n",
    "    # for writer in writers:\n",
    "    #     writer.close()\n",
    "    #\n",
    "    # tf.compat.v1.logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n",
    "\n",
    "# model\n",
    "if __name__ == '__main__':\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydata['train'][0]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(mydata['train']['document']))\n",
    "len(np.unique(mydata['train']['challenge']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'document', 'challenge'],\n",
       "    num_rows: 297\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata['test'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(AutoTokenizer):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint):\n",
    "        super().__init__()\n",
    "        super(TrainingInstance, self).from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "AutoTokenizer is designed to be instantiated using the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_70153/1629419162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"emilyalsentzer/Bio_ClinicalBERT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mTrainingInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_70153/439008511.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingInstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;34m\"AutoTokenizer is designed to be instantiated \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;34m\"using the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)` method.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: AutoTokenizer is designed to be instantiated using the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)` method."
     ]
    }
   ],
   "source": [
    "checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "TrainingInstance(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_69726/4110746982.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmydata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "files = [mydata[split]['sentence'] for split in ['train', 'test']]\n",
    "bert_tokenizer.train(files, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'document', 'challenge'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT masked model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c803b3e8d30e41f497c88233e513dda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecef814bdf7c4ebe88651884b4caa6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /Users/landii03/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77b6488f1eb4c9f8e3f462263a0b03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98e2fbb447447df83e743e9f2c0ff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f0784a9a4443c88ce7f1c74b4a5004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93ad0d9c7c1413eb22cf191f38c6ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb1a0566e8544ad9b3ac0f196733b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3e80de281042838b8887763d51ad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85b2aece7314ad69386a20c18833be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /Users/landii03/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3300eb3d48b6473b9d3d0383e5a9acf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForNextSentencePrediction, AutoModel, BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "# checkpoint = 'bert-base-uncased'\n",
    "data = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "# lm_model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "# nsp_model = AutoModelForNextSentencePrediction.from_pretrained(checkpoint)\n",
    "model = BertForPreTraining.from_pretrained(checkpoint)\n",
    "\n",
    "# model\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example['sentence'])\n",
    "\n",
    "# tkn_dt = mydata.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tokenizer(data['train']['sentence1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357117b31df24ef698786fc5cc405d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ef533ad014fc8bafb84215161918c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c601c49464a4214ba6d1d884987a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dt, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['token_type_ids'] = batch['segment_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['next_sentence_label'] = batch['next_sentence_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,  1607,  1104,  1675,  6946,  1142,   103,  1103,   103,\n",
       "         28996,  2704,   103,  1111,  1142,  1851,  1214,  1385,  1590,   103,\n",
       "           170,  1607,  1104, 28997, 13468,  9318, 20557,   102, 20140,  1108,\n",
       "         28998,   102],\n",
       "        [  101, 28999, 29000,   103,  1103, 29001,  1114,   170,  1344,  3654,\n",
       "         19122, 29002,  1103,  5351,  1144,  1125,   102,  2752,  1111,   103,\n",
       "         29003,  1107, 29004,  1133,  1122,  1108,  1136,  1694, 29005,  1106,\n",
       "          6102,   102]]), 'input_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'masked_lm_positions': tensor([[ 1],\n",
       "        [10]]), 'masked_lm_ids': tensor([[  100],\n",
       "        [19122]]), 'masked_lm_weights': tensor([[1.],\n",
       "        [1.]]), 'next_sentence_labels': tensor([[1],\n",
       "        [0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  1110,  1103,  1248,\n",
       "          -100,  -100, 10296,  -100,  -100,  -100,  -100,  -100,  -100,  1114,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1119,  -100,\n",
       "          -100,  -100],\n",
       "        [ -100,  -100,  -100,  1104,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10221,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'next_sentence_label': tensor([[1],\n",
       "        [0]])}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch = {'input_ids': batch['input_ids'],\n",
    "            'attention_mask': batch['attention_mask'],\n",
    "            'token_type_ids': batch['token_type_ids'],\n",
    "            'labels': batch['labels'],\n",
    "            'next_sentence_label': batch['next_sentence_label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_tokens() missing 1 required positional argument: 'new_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_75608/1019939423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtkn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnew_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: add_tokens() missing 1 required positional argument: 'new_tokens'"
     ]
    }
   ],
   "source": [
    "new_ids = []\n",
    "for tkn in dt.input_ids:\n",
    "    new_ids.extend(tkn)\n",
    "tokenizer.add_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'humorous': 15705,\n",
       " 'civilian': 6688,\n",
       " '1930': 3630,\n",
       " 'grasped': 16375,\n",
       " '##ك': 28494,\n",
       " 'chronicles': 22329,\n",
       " '##smith': 11195,\n",
       " 'fluids': 24024,\n",
       " '21st': 6880,\n",
       " 'slapping': 25006,\n",
       " 'momentarily': 17429,\n",
       " 'gut': 9691,\n",
       " 'teens': 15050,\n",
       " '##chtenstein': 23561,\n",
       " 'caught': 2347,\n",
       " '##ʁ': 28287,\n",
       " '##tom': 18778,\n",
       " 'standings': 12813,\n",
       " 'describing': 7645,\n",
       " 'Operational': 24531,\n",
       " 'Congo': 8695,\n",
       " 'arches': 14738,\n",
       " 'Minsk': 24137,\n",
       " 'matter': 2187,\n",
       " '##lman': 12151,\n",
       " 'enzymes': 17664,\n",
       " 'attack': 2035,\n",
       " '##nat': 24226,\n",
       " 'boat': 3499,\n",
       " 'Qaeda': 20205,\n",
       " '##に': 28807,\n",
       " 'Smile': 21278,\n",
       " 'classic': 5263,\n",
       " 'vertex': 22132,\n",
       " '##lass': 17223,\n",
       " 'щ': 500,\n",
       " '##forcing': 20586,\n",
       " '##oxide': 22040,\n",
       " '##chet': 17374,\n",
       " 'vampires': 6039,\n",
       " 'tiger': 13778,\n",
       " 'buildings': 2275,\n",
       " '₤': 835,\n",
       " 'Mike': 2639,\n",
       " 'Mussolini': 25086,\n",
       " '##rocodile': 24198,\n",
       " 'Maid': 27527,\n",
       " 'martyr': 26899,\n",
       " 'tuned': 17169,\n",
       " 'Yes': 2160,\n",
       " 'landowners': 22671,\n",
       " '##stituted': 26742,\n",
       " 'ladder': 11413,\n",
       " 'limited': 2609,\n",
       " 'carnival': 21446,\n",
       " '##bon': 8868,\n",
       " '##U': 2591,\n",
       " 'premiership': 18262,\n",
       " '4': 125,\n",
       " 'lived': 2077,\n",
       " 'rain': 4458,\n",
       " 'Trains': 20223,\n",
       " '##av': 23140,\n",
       " 'journal': 4897,\n",
       " 'SP': 16625,\n",
       " 'Bird': 8258,\n",
       " '##！': 28989,\n",
       " 'Derek': 6749,\n",
       " 'verge': 18691,\n",
       " 'monastery': 7197,\n",
       " '##chanted': 21874,\n",
       " 'goaltender': 27521,\n",
       " 'loyalty': 10075,\n",
       " '##rsk': 25223,\n",
       " '##spiration': 21240,\n",
       " 'weakly': 19057,\n",
       " 'artist': 2360,\n",
       " '##boro': 10090,\n",
       " 'oral': 9619,\n",
       " 'attain': 20386,\n",
       " 'prehistoric': 16969,\n",
       " 'Bryce': 21528,\n",
       " 'Μ': 404,\n",
       " 'frigate': 17201,\n",
       " 'floods': 15501,\n",
       " '##̍': 28312,\n",
       " 'low': 1822,\n",
       " 'ʁ': 366,\n",
       " 'disposition': 25622,\n",
       " '##iddle': 25826,\n",
       " '##fair': 19803,\n",
       " '\\\\': 165,\n",
       " 'climax': 17396,\n",
       " 'Kansas': 4312,\n",
       " 'sits': 7250,\n",
       " '##retta': 19781,\n",
       " 'discretion': 21435,\n",
       " 'Tori': 28017,\n",
       " 'broader': 12594,\n",
       " 'coaching': 7600,\n",
       " '##bar': 6824,\n",
       " 'fearing': 19424,\n",
       " 'Vol': 5713,\n",
       " '##brecht': 22716,\n",
       " 'asphalt': 19164,\n",
       " '##corn': 13433,\n",
       " 'believers': 26584,\n",
       " 'Bowman': 21502,\n",
       " 'Sheldon': 21084,\n",
       " 'brownish': 21233,\n",
       " '₈': 819,\n",
       " 'counselor': 23550,\n",
       " 'investigate': 8242,\n",
       " 'Maureen': 20895,\n",
       " 'trains': 3918,\n",
       " 'urged': 9497,\n",
       " '##SL': 13726,\n",
       " 'Sumatra': 19679,\n",
       " 'violently': 14835,\n",
       " 'detectives': 23641,\n",
       " 'circuits': 15329,\n",
       " 'Galileo': 23926,\n",
       " 'separation': 8865,\n",
       " 'pathetic': 18970,\n",
       " 'Featuring': 27397,\n",
       " '##mpest': 25214,\n",
       " 'stretch': 7461,\n",
       " 'walls': 2928,\n",
       " 'Viscount': 12897,\n",
       " '##kling': 18278,\n",
       " 'prosecuted': 24004,\n",
       " 'climbing': 8259,\n",
       " 'profession': 9545,\n",
       " 'competing': 6259,\n",
       " 'Patricia': 10711,\n",
       " 'Jana': 26415,\n",
       " 'Plus': 8696,\n",
       " '†': 792,\n",
       " 'Shan': 20642,\n",
       " 'arts': 3959,\n",
       " 'Amos': 14341,\n",
       " 'medley': 19362,\n",
       " '##rup': 20910,\n",
       " 'installing': 27432,\n",
       " 'breeding': 8103,\n",
       " '2000s': 8509,\n",
       " '国': 1004,\n",
       " 'bolt': 11538,\n",
       " 'Joshua': 8777,\n",
       " 'Tu': 17037,\n",
       " 'akin': 21584,\n",
       " 'spur': 16650,\n",
       " 'viewpoint': 25216,\n",
       " 'ʌ': 371,\n",
       " 'deposited': 14735,\n",
       " 'grandparents': 15313,\n",
       " '##ג': 28447,\n",
       " '##·': 28172,\n",
       " 'solemn': 22752,\n",
       " 'dazed': 24528,\n",
       " 'Automobile': 27336,\n",
       " 'cycles': 13874,\n",
       " 'bar': 2927,\n",
       " 'focusing': 7781,\n",
       " 'Schuster': 27243,\n",
       " 'efforts': 3268,\n",
       " 'grip': 5688,\n",
       " '##umour': 27226,\n",
       " 'threw': 3885,\n",
       " 'க': 671,\n",
       " 'pathways': 19530,\n",
       " 'bomber': 11895,\n",
       " '##imus': 26552,\n",
       " 'necessary': 3238,\n",
       " 'Parliament': 2901,\n",
       " 'Speed': 10856,\n",
       " 'ğ': 293,\n",
       " 'contain': 4651,\n",
       " 'failure': 4290,\n",
       " 'random': 7091,\n",
       " 'Xavier': 9867,\n",
       " '##cky': 23143,\n",
       " 'carriers': 11837,\n",
       " 'terribly': 18049,\n",
       " '##rted': 26048,\n",
       " 'Liang': 18842,\n",
       " 'contention': 21374,\n",
       " 'arrested': 3950,\n",
       " 'trailing': 13161,\n",
       " 'lab': 8074,\n",
       " '##18': 15292,\n",
       " 'Mosque': 16815,\n",
       " 'ি': 664,\n",
       " '##TP': 17433,\n",
       " 'Deal': 15361,\n",
       " 'E': 142,\n",
       " '##Ε': 28319,\n",
       " 'Streets': 11433,\n",
       " 'Bill': 2617,\n",
       " 'LSU': 24184,\n",
       " 'tested': 7289,\n",
       " '##rals': 16179,\n",
       " 'exhausted': 8984,\n",
       " 'Investment': 13623,\n",
       " '##aud': 16631,\n",
       " 'Archibald': 16849,\n",
       " 'sixteen': 7423,\n",
       " 'petals': 17270,\n",
       " 'Davenport': 17352,\n",
       " '##acon': 24825,\n",
       " 'officials': 3878,\n",
       " 'adaptations': 18830,\n",
       " 'bound': 4930,\n",
       " 'transcription': 15416,\n",
       " 'waiting': 2613,\n",
       " 'dictated': 26754,\n",
       " '##MC': 10044,\n",
       " 'administrators': 18083,\n",
       " 'sensed': 10494,\n",
       " 'Choir': 12511,\n",
       " '##gins': 19579,\n",
       " '##chuk': 25271,\n",
       " 'Shu': 23274,\n",
       " 'Turning': 12848,\n",
       " '##apa': 26519,\n",
       " 'touching': 6893,\n",
       " 'Food': 6702,\n",
       " 'Lyons': 17728,\n",
       " 'runway': 9822,\n",
       " 'Problems': 23855,\n",
       " 'motorway': 15316,\n",
       " 'X': 161,\n",
       " '##asus': 27384,\n",
       " '##moor': 19216,\n",
       " 'Petroleum': 20396,\n",
       " 'pass': 2789,\n",
       " '##ooby': 27404,\n",
       " 'how': 1293,\n",
       " 'analogue': 26116,\n",
       " 'noteworthy': 22076,\n",
       " '##Ş': 28245,\n",
       " '##teen': 10381,\n",
       " 'Bombay': 11888,\n",
       " 'Forever': 11694,\n",
       " '##ema': 14494,\n",
       " 'Not': 1753,\n",
       " 'forms': 2769,\n",
       " '04': 5129,\n",
       " 'Clair': 19699,\n",
       " 'Bangladesh': 6735,\n",
       " 'Changes': 21395,\n",
       " 'heat': 3208,\n",
       " '##mitage': 27811,\n",
       " 'policing': 27841,\n",
       " 'tiny': 4296,\n",
       " '##erial': 19860,\n",
       " 'tall': 3543,\n",
       " 'highly': 3023,\n",
       " 'accuracy': 10893,\n",
       " '##্': 28572,\n",
       " 'exercised': 19065,\n",
       " 'Wilcox': 26813,\n",
       " 'Richards': 9450,\n",
       " '##hul': 24287,\n",
       " 'extensions': 16003,\n",
       " 'nor': 4040,\n",
       " '##ouse': 17237,\n",
       " 'businesses': 5028,\n",
       " 'Poor': 11767,\n",
       " 'Lucas': 5675,\n",
       " 'failures': 16676,\n",
       " 'enthusiasm': 12430,\n",
       " 'edition': 2596,\n",
       " 'portray': 18483,\n",
       " 'plotting': 23214,\n",
       " '1688': 27041,\n",
       " '##pose': 14811,\n",
       " 'Molly': 9564,\n",
       " '##nitz': 21505,\n",
       " 'ò': 264,\n",
       " 'analysts': 22018,\n",
       " 'everyone': 2490,\n",
       " 'sibling': 26563,\n",
       " '##uary': 18487,\n",
       " 'Goldman': 19085,\n",
       " 'tuning': 19689,\n",
       " 'letting': 5074,\n",
       " '##ndo': 13645,\n",
       " '##world': 13070,\n",
       " 'Birds': 13873,\n",
       " '39': 3614,\n",
       " 'Bahamas': 18242,\n",
       " 'absorbed': 8761,\n",
       " 'prayers': 13865,\n",
       " '##ams': 18450,\n",
       " 'blacks': 14892,\n",
       " 'semifinals': 11499,\n",
       " 'acronym': 22478,\n",
       " 'Malaysia': 5355,\n",
       " 'Detroit': 4908,\n",
       " 'rotated': 23050,\n",
       " 'photographs': 6810,\n",
       " 'fertility': 20060,\n",
       " 'hallway': 6219,\n",
       " 'page': 3674,\n",
       " 'glance': 5410,\n",
       " 'instinctively': 20749,\n",
       " '##hoe': 10061,\n",
       " 'î': 260,\n",
       " 'attendees': 23150,\n",
       " 'hard': 1662,\n",
       " 'politician': 2931,\n",
       " 'lease': 10549,\n",
       " 'Resistance': 15598,\n",
       " '##istice': 20788,\n",
       " 'numbered': 8324,\n",
       " '##た': 28801,\n",
       " 'bricks': 15453,\n",
       " 'glances': 13363,\n",
       " 'it': 1122,\n",
       " 'Math': 15112,\n",
       " '##ldon': 20822,\n",
       " 'Honorable': 26421,\n",
       " '##pa': 4163,\n",
       " 'economy': 4190,\n",
       " 'Devon': 7122,\n",
       " 'ř': 317,\n",
       " '##kov': 7498,\n",
       " '##ogist': 25976,\n",
       " '##ো': 28571,\n",
       " '##guard': 12188,\n",
       " 'yes': 4208,\n",
       " 'translating': 26894,\n",
       " 'Marie': 4238,\n",
       " '##cedural': 27433,\n",
       " 'chooses': 16826,\n",
       " 'Given': 10470,\n",
       " '##rage': 20240,\n",
       " 'Du': 12786,\n",
       " 'inability': 14267,\n",
       " 'electromagnetic': 19805,\n",
       " '##berman': 24419,\n",
       " 'rounds': 5720,\n",
       " '##sume': 22369,\n",
       " 'Online': 10523,\n",
       " 'finalist': 12039,\n",
       " 'hoping': 4717,\n",
       " 'confusion': 6406,\n",
       " 'trend': 10209,\n",
       " '##roft': 27421,\n",
       " 'correlated': 27053,\n",
       " '##rin': 4854,\n",
       " '##hly': 25469,\n",
       " '##allo': 20797,\n",
       " 'ད': 687,\n",
       " 'feat': 8809,\n",
       " 'unpopular': 21665,\n",
       " '##ch': 1732,\n",
       " 'Thomson': 11141,\n",
       " 'accelerated': 16112,\n",
       " 'scene': 2741,\n",
       " 'Caucasus': 17570,\n",
       " 'underwear': 15082,\n",
       " 'ὐ': 771,\n",
       " 'classes': 3553,\n",
       " 'begun': 4972,\n",
       " 'sofa': 10907,\n",
       " 'Highland': 12103,\n",
       " 'domains': 13770,\n",
       " '##š': 10222,\n",
       " 'Southwestern': 22557,\n",
       " 'Bertrand': 22554,\n",
       " '##lberg': 20974,\n",
       " '##ing': 1158,\n",
       " 'য': 656,\n",
       " '##ix': 7231,\n",
       " 'jumping': 8987,\n",
       " 'stuck': 5342,\n",
       " 'shirt': 2969,\n",
       " '102': 9081,\n",
       " 'piercing': 15659,\n",
       " 'cheek': 4310,\n",
       " 'remnant': 22499,\n",
       " '12th': 5247,\n",
       " 'somehow': 5006,\n",
       " '##sect': 26338,\n",
       " 'noted': 2382,\n",
       " 'urge': 8869,\n",
       " 'hoarse': 23523,\n",
       " 'lines': 2442,\n",
       " 'goal': 2273,\n",
       " 'In': 1130,\n",
       " 'close': 1601,\n",
       " 'generates': 21241,\n",
       " 'seminar': 28067,\n",
       " '##smus': 23283,\n",
       " 'Vickers': 20067,\n",
       " 'trusts': 25802,\n",
       " '##oft': 18874,\n",
       " '##icles': 26407,\n",
       " 'talent': 5939,\n",
       " 'scattered': 7648,\n",
       " 'Monte': 10046,\n",
       " 'Elijah': 14581,\n",
       " 'Halloween': 15101,\n",
       " '##ński': 15713,\n",
       " '##xes': 21210,\n",
       " '[unused53]': 53,\n",
       " 'Luce': 22340,\n",
       " 'unfolded': 27118,\n",
       " '##א': 28445,\n",
       " 'finding': 4006,\n",
       " 'wherever': 12826,\n",
       " 'effect': 2629,\n",
       " '##physics': 18256,\n",
       " '1925': 4053,\n",
       " 'glacier': 19121,\n",
       " 'versions': 3827,\n",
       " 'Homeland': 21725,\n",
       " 'concentrate': 10191,\n",
       " 'nobody': 8582,\n",
       " '##elli': 12164,\n",
       " 'Butcher': 25991,\n",
       " 'cast': 2641,\n",
       " 'Till': 22430,\n",
       " '##ium': 3656,\n",
       " '##formation': 24152,\n",
       " '##lump': 15363,\n",
       " 'series': 1326,\n",
       " 'Townsend': 17131,\n",
       " '##rassed': 27857,\n",
       " 'motto': 13658,\n",
       " 'Tyrone': 20314,\n",
       " 'ф': 495,\n",
       " '##：': 28995,\n",
       " 'retailers': 18399,\n",
       " 'twelve': 4030,\n",
       " '##cture': 22355,\n",
       " 'detected': 11168,\n",
       " '##fica': 24685,\n",
       " 'Leisure': 26090,\n",
       " 'castle': 3804,\n",
       " 'priest': 4924,\n",
       " 'pace': 6418,\n",
       " 'Miami': 4916,\n",
       " '##charya': 24396,\n",
       " 'Wilhelm': 8725,\n",
       " 'capacity': 3211,\n",
       " 'THE': 7462,\n",
       " 'aristocratic': 21809,\n",
       " 'happiness': 9266,\n",
       " 'furrowed': 22538,\n",
       " 'Adriatic': 26122,\n",
       " 'aforementioned': 18501,\n",
       " 'connector': 25609,\n",
       " 'hiking': 14249,\n",
       " 'Merrill': 17247,\n",
       " 'ι': 426,\n",
       " 'cartoonist': 23095,\n",
       " 'Census': 4496,\n",
       " 'Mt': 13086,\n",
       " 'Collegiate': 14329,\n",
       " '1767': 23714,\n",
       " 'initiation': 21252,\n",
       " '230': 11866,\n",
       " 'Mohamed': 15083,\n",
       " 'Eddy': 18998,\n",
       " '##bow': 14251,\n",
       " 'player': 1591,\n",
       " 'spacecraft': 12568,\n",
       " 'Best': 1798,\n",
       " 'opposed': 4151,\n",
       " 'rebel': 10474,\n",
       " '##द': 28516,\n",
       " 'imports': 20171,\n",
       " '##lub': 21794,\n",
       " 'floating': 8379,\n",
       " 'burn': 6790,\n",
       " 'secretly': 10173,\n",
       " 'Sleep': 15153,\n",
       " 'Leonardo': 15358,\n",
       " 'dizzy': 21441,\n",
       " 'peers': 14097,\n",
       " '##ilian': 27308,\n",
       " 'sail': 10339,\n",
       " 'section': 2237,\n",
       " 'melt': 16399,\n",
       " '##や': 28816,\n",
       " 'Peacock': 26102,\n",
       " 'pencil': 16372,\n",
       " 'Corps': 3158,\n",
       " 'Working': 9612,\n",
       " 'emotional': 6438,\n",
       " 'Savage': 12160,\n",
       " 'el': 8468,\n",
       " 'superficial': 26558,\n",
       " 'Sri': 4471,\n",
       " 'acceptance': 10030,\n",
       " 'Advance': 20706,\n",
       " '##amed': 25282,\n",
       " 'smile': 2003,\n",
       " 'perhaps': 3229,\n",
       " '##iate': 15045,\n",
       " 'Commodore': 14053,\n",
       " 'account': 3300,\n",
       " '##rang': 22081,\n",
       " 'comes': 2502,\n",
       " 'neutrality': 25449,\n",
       " 'urgency': 21573,\n",
       " 'ye': 6798,\n",
       " 'requests': 11458,\n",
       " '##bara': 18228,\n",
       " 'fifth': 3049,\n",
       " 'of': 1104,\n",
       " 'velocity': 10537,\n",
       " 'Swiss': 4614,\n",
       " '##ile': 4759,\n",
       " '⟩': 883,\n",
       " 'chorus': 8220,\n",
       " 'seeds': 8365,\n",
       " 'trust': 3496,\n",
       " 'grumbled': 18163,\n",
       " 'interred': 14432,\n",
       " 'drilling': 18218,\n",
       " '##lacing': 24001,\n",
       " 'scalp': 23658,\n",
       " 'Glen': 8820,\n",
       " 'transgender': 20141,\n",
       " 'inquired': 28110,\n",
       " 'explores': 16001,\n",
       " '##iness': 8405,\n",
       " '##έ': 28336,\n",
       " 'bracelet': 21982,\n",
       " 'Snake': 16335,\n",
       " 'Severn': 27908,\n",
       " '##pelling': 24542,\n",
       " '##nation': 9199,\n",
       " 'Reef': 24302,\n",
       " 'Yoga': 25035,\n",
       " 'proof': 6777,\n",
       " 'Petersen': 25245,\n",
       " 'Closing': 23408,\n",
       " 'complementary': 24671,\n",
       " '##and': 5709,\n",
       " 'Broken': 12703,\n",
       " '##oa': 12985,\n",
       " '##ŭ': 28251,\n",
       " '⇌': 847,\n",
       " '##week': 21394,\n",
       " '220': 10423,\n",
       " 'comparable': 12763,\n",
       " 'Wire': 24781,\n",
       " 'synthetic': 13922,\n",
       " 'Varsity': 21360,\n",
       " 'populace': 25087,\n",
       " 'Rhodesia': 22228,\n",
       " 'suspended': 6232,\n",
       " '[unused56]': 56,\n",
       " '##ents': 9857,\n",
       " 'judging': 16839,\n",
       " 'Glad': 27652,\n",
       " 'Sarawak': 23794,\n",
       " '117': 12737,\n",
       " '##lev': 23403,\n",
       " 'Louie': 18676,\n",
       " '##raga': 24036,\n",
       " 'Germanic': 15886,\n",
       " 'clan': 7019,\n",
       " 'Sofia': 8713,\n",
       " '##mic': 7257,\n",
       " 'undercover': 20486,\n",
       " '##rmal': 25635,\n",
       " 'Soap': 26019,\n",
       " 'Close': 11145,\n",
       " 'Tribal': 24252,\n",
       " 'poet': 4225,\n",
       " '##star': 10058,\n",
       " '##hout': 24284,\n",
       " '##キ': 28832,\n",
       " 'Macintosh': 26162,\n",
       " 'gateway': 19154,\n",
       " 'ւ': 526,\n",
       " 'line': 1413,\n",
       " 'tropical': 5065,\n",
       " '##ucci': 15072,\n",
       " 'Date': 14265,\n",
       " '##gau': 20130,\n",
       " 'chain': 4129,\n",
       " 'Bus': 8947,\n",
       " '##太': 28904,\n",
       " '1941': 3018,\n",
       " 'attached': 4309,\n",
       " 'associations': 9815,\n",
       " 'Mother': 4872,\n",
       " 'Twice': 24715,\n",
       " 'heavily': 3777,\n",
       " '##mott': 27374,\n",
       " '##dira': 27987,\n",
       " 'â': 248,\n",
       " 'Bryan': 8129,\n",
       " 'partnerships': 15605,\n",
       " '##pling': 11624,\n",
       " 'Diesel': 21651,\n",
       " '）': 1098,\n",
       " 'Greenwood': 17999,\n",
       " 'steamed': 24410,\n",
       " 'telescope': 16737,\n",
       " '##itis': 10721,\n",
       " 'Universities': 14482,\n",
       " '##ifice': 26718,\n",
       " 'generate': 9509,\n",
       " 'pornography': 22912,\n",
       " 'Print': 27833,\n",
       " 'transmit': 21994,\n",
       " 'Austro': 18128,\n",
       " 'Ghana': 9446,\n",
       " 'shades': 16327,\n",
       " 'Help': 12056,\n",
       " 'defeat': 3326,\n",
       " '##tone': 4793,\n",
       " 'Apollo': 9053,\n",
       " '⁄': 799,\n",
       " 'happily': 11786,\n",
       " '##rini': 19764,\n",
       " 'apron': 22859,\n",
       " 'О': 461,\n",
       " 'condition': 3879,\n",
       " 'titles': 3727,\n",
       " '221': 21319,\n",
       " 'Navajo': 24954,\n",
       " '##sla': 26597,\n",
       " '1801': 12561,\n",
       " 'meeting': 2309,\n",
       " 'Leinster': 16619,\n",
       " '178': 20977,\n",
       " 'groom': 23799,\n",
       " 'farther': 8791,\n",
       " '##lein': 18929,\n",
       " '2002': 1617,\n",
       " 'reminding': 17235,\n",
       " 'Live': 3374,\n",
       " 'Emeritus': 18212,\n",
       " 'Fisher': 8476,\n",
       " 'brighter': 18044,\n",
       " '##ction': 5796,\n",
       " 'molecules': 10799,\n",
       " '##侍': 28882,\n",
       " 'Brewers': 24591,\n",
       " 'brand': 4097,\n",
       " 'cousin': 5009,\n",
       " 'dark': 1843,\n",
       " 'ė': 290,\n",
       " 'Mint': 21192,\n",
       " 'plaintiff': 23940,\n",
       " 'Nearly': 16992,\n",
       " 'McLaughlin': 26721,\n",
       " 'surveys': 13634,\n",
       " 'milestone': 24697,\n",
       " '##éra': 25570,\n",
       " '部': 1077,\n",
       " 'greatly': 5958,\n",
       " 'ground': 1747,\n",
       " 'Prentice': 26323,\n",
       " 'Christ': 4028,\n",
       " 'reflected': 7226,\n",
       " 'posting': 15537,\n",
       " '##ć': 7128,\n",
       " '##tista': 25645,\n",
       " 'Emerald': 24464,\n",
       " 'metallic': 13256,\n",
       " 'bilingual': 20959,\n",
       " '##urous': 25576,\n",
       " 'shoving': 18164,\n",
       " 'tolerance': 15745,\n",
       " 'Ng': 26057,\n",
       " 'occupying': 14854,\n",
       " 'Ronan': 20320,\n",
       " 'Bert': 15035,\n",
       " 'better': 1618,\n",
       " 'ditch': 15916,\n",
       " 'Companion': 17631,\n",
       " 'Emily': 5590,\n",
       " 'noble': 8604,\n",
       " 'foothills': 21864,\n",
       " '234': 24354,\n",
       " 'streaks': 24177,\n",
       " 'January': 1356,\n",
       " '##rive': 17389,\n",
       " '##itting': 26927,\n",
       " '社': 1063,\n",
       " 'Becky': 15009,\n",
       " 'compass': 23962,\n",
       " 'sect': 21354,\n",
       " 'elemental': 24789,\n",
       " 'fronts': 25036,\n",
       " '1763': 19877,\n",
       " 'simultaneously': 7344,\n",
       " '##22': 20581,\n",
       " '##ool': 21778,\n",
       " 'World': 1291,\n",
       " 'Wichita': 20124,\n",
       " 'NRHP': 24894,\n",
       " '##）': 28991,\n",
       " 'expenditure': 24106,\n",
       " 'outsider': 26948,\n",
       " '##chanized': 26040,\n",
       " 'wholesale': 20767,\n",
       " '##nut': 12251,\n",
       " 'MV': 23061,\n",
       " 'makeup': 5150,\n",
       " '##mett': 23355,\n",
       " 'funny': 6276,\n",
       " 'concerning': 6995,\n",
       " 'websites': 12045,\n",
       " '##unting': 14597,\n",
       " 'imp': 24034,\n",
       " '##ий': 21911,\n",
       " '##ania': 11262,\n",
       " 'Toro': 27470,\n",
       " '##vious': 18854,\n",
       " 'Court': 2031,\n",
       " 'Gao': 20756,\n",
       " 'membership': 5467,\n",
       " 'nobility': 12276,\n",
       " 'trips': 9185,\n",
       " 'Fishing': 23738,\n",
       " 'Carpenter': 13190,\n",
       " 'shipment': 25464,\n",
       " 'lengthy': 12628,\n",
       " '##8': 1604,\n",
       " 'bow': 7125,\n",
       " 'ས': 693,\n",
       " 'lying': 4009,\n",
       " '1874': 7079,\n",
       " 'clues': 17645,\n",
       " 'bell': 7315,\n",
       " 'Gene': 9066,\n",
       " 'nun': 22108,\n",
       " 'stop': 1831,\n",
       " 'hearted': 21898,\n",
       " 'drawing': 4619,\n",
       " '##wash': 24745,\n",
       " 'temples': 8433,\n",
       " 'crown': 6371,\n",
       " 'Used': 17627,\n",
       " 'choreography': 22543,\n",
       " '##jer': 21569,\n",
       " 'tattoo': 13271,\n",
       " '##ław': 26196,\n",
       " 'abbot': 19966,\n",
       " 'company': 1419,\n",
       " 'mark': 4551,\n",
       " 'spines': 22503,\n",
       " 'Rifles': 19275,\n",
       " 'Michaels': 19108,\n",
       " 'Tokugawa': 24424,\n",
       " '##leo': 26918,\n",
       " '##sts': 10047,\n",
       " '##tin': 6105,\n",
       " 'excelled': 25749,\n",
       " '##lum': 7776,\n",
       " 'variety': 2783,\n",
       " 'Shrewsbury': 19561,\n",
       " '##ider': 18494,\n",
       " '##え': 28789,\n",
       " 'fighters': 7705,\n",
       " '##lint': 22761,\n",
       " 'scored': 2297,\n",
       " 'Commons': 7554,\n",
       " 'renowned': 7985,\n",
       " '43': 3887,\n",
       " '##め': 28814,\n",
       " 'rewards': 22278,\n",
       " '##nob': 22360,\n",
       " 'Γ': 396,\n",
       " 'pre': 3073,\n",
       " 'freaked': 25818,\n",
       " '##aire': 9674,\n",
       " '##ond': 16838,\n",
       " 'nude': 17899,\n",
       " '##uter': 18614,\n",
       " '##em': 5521,\n",
       " 'unions': 10230,\n",
       " '##par': 17482,\n",
       " 'Commonwealth': 5044,\n",
       " '##verance': 24374,\n",
       " 'scraping': 27088,\n",
       " '##sace': 28072,\n",
       " 'funeral': 6594,\n",
       " 'Alfa': 27099,\n",
       " 'Si': 14159,\n",
       " 'Troy': 9454,\n",
       " '##tude': 17684,\n",
       " 'Hot': 4126,\n",
       " '##ness': 1757,\n",
       " 'noticing': 16927,\n",
       " '##nov': 15847,\n",
       " 'aluminum': 14349,\n",
       " 'relics': 19052,\n",
       " 'subjective': 23481,\n",
       " 'analogous': 21960,\n",
       " 'Lac': 23929,\n",
       " 'wreck': 13573,\n",
       " 'Hundreds': 25793,\n",
       " 'θ': 425,\n",
       " '68': 5599,\n",
       " 'Wales': 2717,\n",
       " '1829': 11527,\n",
       " 'penis': 21504,\n",
       " 'shrieked': 24863,\n",
       " 'appointment': 5516,\n",
       " 'worthy': 11649,\n",
       " 'Rooney': 27238,\n",
       " 'sinks': 27004,\n",
       " '##ctus': 18045,\n",
       " 'slightest': 16960,\n",
       " '##mio': 18248,\n",
       " 'correlation': 18741,\n",
       " '##ample': 26671,\n",
       " 'were': 1127,\n",
       " 'Atlas': 12974,\n",
       " 'notable': 3385,\n",
       " 'ghost': 7483,\n",
       " 'varied': 9177,\n",
       " 'publisher': 6654,\n",
       " 'unfortunate': 16702,\n",
       " '##6th': 25104,\n",
       " 'synthesizer': 15258,\n",
       " 'Powerplant': 25390,\n",
       " 'mascot': 14623,\n",
       " 'ę': 291,\n",
       " 'own': 1319,\n",
       " 'crossed': 3809,\n",
       " 'before': 1196,\n",
       " 'says': 1867,\n",
       " 'Sean': 5499,\n",
       " 'livery': 21193,\n",
       " '1943': 2976,\n",
       " 'Quality': 14801,\n",
       " 'ừ': 762,\n",
       " 'b': 171,\n",
       " '##rophe': 24658,\n",
       " 'danced': 11103,\n",
       " '##ized': 2200,\n",
       " '##people': 21159,\n",
       " 'Chambers': 12990,\n",
       " 'Geographic': 15472,\n",
       " 'Meyer': 11545,\n",
       " 'harness': 20211,\n",
       " '##rick': 8053,\n",
       " 'stabbing': 24728,\n",
       " 'activity': 3246,\n",
       " 'Sheffield': 8139,\n",
       " 'Hell': 5479,\n",
       " 'tilting': 26954,\n",
       " 'elaborate': 9427,\n",
       " 'observatory': 23014,\n",
       " 'Rising': 11948,\n",
       " 'Larson': 23360,\n",
       " 'measures': 5252,\n",
       " '##mat': 21943,\n",
       " 'transmission': 6580,\n",
       " 'chewing': 19791,\n",
       " 'Ljubljana': 23835,\n",
       " 'heels': 8260,\n",
       " 'generous': 12839,\n",
       " '##MB': 20660,\n",
       " '1882': 6543,\n",
       " '##cription': 27530,\n",
       " 'penetrate': 20890,\n",
       " 'Parker': 5597,\n",
       " 'summon': 21147,\n",
       " 'scientist': 7482,\n",
       " 'Baton': 22178,\n",
       " 'Institutions': 26343,\n",
       " 'ǎ': 344,\n",
       " '##iology': 17288,\n",
       " 'commenced': 8042,\n",
       " 'Galen': 23295,\n",
       " '##ز': 28483,\n",
       " 'protein': 4592,\n",
       " 'apple': 12075,\n",
       " 'seal': 9438,\n",
       " 'tomb': 8880,\n",
       " '##ma': 1918,\n",
       " 'diocese': 9856,\n",
       " 'Sparhawk': 14128,\n",
       " 'emerging': 8999,\n",
       " '##beat': 14262,\n",
       " 'patience': 14355,\n",
       " 'headache': 16320,\n",
       " '##rrence': 21629,\n",
       " '##tha': 7702,\n",
       " 'Disc': 14856,\n",
       " 'ambiguous': 22405,\n",
       " '##ney': 4695,\n",
       " 'Mercer': 13576,\n",
       " 'shoved': 6958,\n",
       " '[unused38]': 38,\n",
       " 'classified': 5667,\n",
       " '##ants': 7418,\n",
       " 'shrub': 16965,\n",
       " 'Authority': 5987,\n",
       " 'Kemp': 22443,\n",
       " 'Tacoma': 25588,\n",
       " 'reservoirs': 26536,\n",
       " 'Rex': 10896,\n",
       " 'evidence': 2554,\n",
       " 'flower': 7366,\n",
       " '##ivity': 6366,\n",
       " 'certification': 12146,\n",
       " 'cough': 21810,\n",
       " 'Katy': 19513,\n",
       " 'Kirby': 15899,\n",
       " 'Silence': 12939,\n",
       " 'transports': 21951,\n",
       " 'Philippe': 11162,\n",
       " 'Vehicle': 19357,\n",
       " 'Plain': 13823,\n",
       " 'parting': 24402,\n",
       " 'Greene': 10983,\n",
       " 'taken': 1678,\n",
       " 'Stacy': 20072,\n",
       " 'racing': 3915,\n",
       " 'bit': 2113,\n",
       " 'Andrew': 3217,\n",
       " 'CEO': 5058,\n",
       " 'Draft': 7060,\n",
       " 'Carole': 27422,\n",
       " 'Gibbs': 16556,\n",
       " 'ņ': 308,\n",
       " 'Wight': 23721,\n",
       " 'conflicts': 9802,\n",
       " 'curious': 8193,\n",
       " 'players': 2139,\n",
       " 'Izzy': 17779,\n",
       " '1813': 12282,\n",
       " '##DM': 20002,\n",
       " '##ɛ': 28279,\n",
       " 'Miranda': 9120,\n",
       " 'unarmed': 26809,\n",
       " '上': 978,\n",
       " 'GT': 14965,\n",
       " '##rton': 11530,\n",
       " '##oth': 12858,\n",
       " 'Daniels': 13549,\n",
       " '##thest': 22722,\n",
       " 'Daly': 20058,\n",
       " '##nso': 27035,\n",
       " 'glued': 25615,\n",
       " '40': 1969,\n",
       " '##vanna': 19434,\n",
       " 'differential': 12630,\n",
       " 'Osborne': 17802,\n",
       " 'Green': 2565,\n",
       " 'Off': 8060,\n",
       " '##them': 25962,\n",
       " '##な': 28806,\n",
       " 'brutal': 12800,\n",
       " '##lines': 10443,\n",
       " 'vassal': 28080,\n",
       " 'student': 2377,\n",
       " 'café': 20583,\n",
       " 'bidding': 19520,\n",
       " 'supper': 20644,\n",
       " 'postal': 14889,\n",
       " 'organizational': 15610,\n",
       " '##lean': 21649,\n",
       " '##Ú': 28195,\n",
       " '##hil': 20473,\n",
       " 'tactic': 21800,\n",
       " '##ods': 16476,\n",
       " '##aid': 19954,\n",
       " '##ych': 21155,\n",
       " 'ads': 17641,\n",
       " '##folia': 26749,\n",
       " 'aerospace': 27689,\n",
       " '##я': 14800,\n",
       " 'altar': 9625,\n",
       " 'argues': 8935,\n",
       " '##qi': 17276,\n",
       " 'entirely': 3665,\n",
       " '##hra': 20955,\n",
       " '##fra': 27476,\n",
       " 'Records': 2151,\n",
       " 'federal': 2877,\n",
       " 'topping': 27001,\n",
       " 'survival': 8115,\n",
       " 'symbolic': 13516,\n",
       " 'Rocket': 17100,\n",
       " 'Josh': 5868,\n",
       " 'ordinance': 26585,\n",
       " 'supporting': 4374,\n",
       " 'regardless': 8334,\n",
       " '##gaard': 19572,\n",
       " 'shed': 8478,\n",
       " '##tem': 18408,\n",
       " '##ker': 4188,\n",
       " 'Harvest': 23119,\n",
       " 'convenient': 14785,\n",
       " ...}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_75608/2557593435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, next_sentence_label, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/redundancy/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "out = model(**new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "ciao: int64\n",
       "ciaociao: int64\n",
       "----\n",
       "ciao: [[1,2,3]]\n",
       "ciaociao: [[4,5,6]]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['contempt', 'supercalifragilistichespiralidoso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['contempt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['supercalifragilistichespiralidoso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "mydt = datasets.DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydt['train'] = datasets.Dataset.from_dict({'attention_mask': [1]*10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "with open('./out/train_sentences.txt', 'r') as f:\n",
    "    note_list = f.readlines()\n",
    "    note = []\n",
    "    for sen in note_list:\n",
    "        if sen.strip('\\n') == '':\n",
    "            notes.append(note)\n",
    "            note = []\n",
    "        else:\n",
    "            note.append(sen.strip('\\n'))\n",
    "\n",
    "mynote = notes[0]\n",
    "\n",
    "sentence1 = 'associated diagnosis urinary tract infection and cirrhosis of the liver'\n",
    "sentence2 = 'history of present illness the patient is an male who had a history of colon cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2628, 12645, 190, 9324, 1616, 14441, 8974, 1105, 172, 3161, 1197, 15342, 1548, 1104, 1103, 11911, 102, 1607, 1104, 1675, 6946, 1103, 5351, 1110, 1126, 2581, 1150, 1125, 170, 1607, 1104, 1884, 4934, 4182, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn = tokenizer(sentence1, sentence2)\n",
    "tkn\n",
    "# tokenizer.convert_ids_to_tokens(tkn['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_tensors = torch.tensor([tkn['token_type_ids']])\n",
    "token_tensors = torch.tensor([tkn['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -5.0978,  -5.3727,  -5.3744,  ...,  -5.6708,  -6.7929,  -6.1098],\n",
       "         [ -7.3259,  -7.2716,  -7.7855,  ...,  -6.6673,  -8.9316,  -7.5792],\n",
       "         [ -5.8150,  -6.0850,  -4.8353,  ...,  -6.9794,  -7.2884,  -6.1053],\n",
       "         ...,\n",
       "         [ -7.5736,  -6.1877,  -6.6858,  ...,  -7.5300,  -5.6632,  -7.9293],\n",
       "         [ -9.7014,  -9.9979, -10.0304,  ...,  -8.5529,  -7.2215,  -8.4779],\n",
       "         [ -3.5424,  -5.0459,  -4.3914,  ...,  -4.3864,  -6.3197,  -5.1185]]],\n",
       "       grad_fn=<AddBackward0>), seq_relationship_logits=tensor([[-0.7762,  1.4471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(token_tensors, token_type_ids=segment_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/landii03/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "\n",
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Who',\n",
       " 'was',\n",
       " 'Jim',\n",
       " 'He',\n",
       " '##nson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'Jim',\n",
       " 'He',\n",
       " '##nson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/landii03/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbe9387db1542edbe5e91716f224441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0\n",
      "Who 0\n",
      "was 0\n",
      "Jim 0\n",
      "He 0\n",
      "##nson 0\n",
      "? 0\n",
      "[SEP] 0\n",
      "Jim 1\n",
      "He 1\n",
      "##nson 1\n",
      "was 1\n",
      "a 1\n",
      "puppet 1\n",
      "##eer 1\n",
      "[SEP] 1\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(tokenizer.convert_ids_to_tokens(indexed_tokens), segments_ids):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/landii03/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "indexed_tokens[masked_index] = tokenizer.mask_token_id\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "masked_lm_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForMaskedLM', 'bert-base-cased')\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = masked_lm_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "\n",
    "# Get the predicted token\n",
    "predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'Jim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jim'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(torch.argmax(predictions[0][0][masked_index]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2627, 1108, 3104, 1124, 15703, 136, 102, 3104, 1124, 15703, 1108, 170, 16797, 8284, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text_1, text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused1]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[unused1]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.get_special_tokens_mask(tokenizer(sentence1)['input_ids'])\n",
    "tokenizer.convert_ids_to_tokens(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2628, 12645,   190,  9324,  1616, 14441,  8974,  1105,   172,\n",
       "          3161,  1197, 15342,  1548,  1104,  1103, 11911,   102,  1607,  1104,\n",
       "          1675,  6946,  1103,  5351,  1110,  1126,  2581,  1150,  1125,   170,\n",
       "          1607,  1104,  1884,  4934,  4182,   102]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_sent1 = 'associated diagnosis urinary tract infection and cirrhosis of the liver'\n",
    "labels = tokenizer(true_sent1, sentence2, return_tensors='pt')[\"input_ids\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] associated [MASK] u ##rina ##ry tract infection and c ##ir ##r ##hos ##is of the liver [SEP] history of present illness the patient is an male who had a history of co ##lon cancer [SEP]\n",
      "[CLS] associated diagnosis u ##rina ##ry tract infection and c ##ir ##r ##hos ##is of the liver [SEP] history of present illness the patient is an male who had a history of co ##lon cancer [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(labels[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] associated [MASK] u ##rina ##ry tract infection and c ##ir ##r ##hos ##is of the liver [SEP] history of present illness the patient is an male who had a history of co ##lon cancer [SEP]\n",
      "[CLS] associated diagnosis u ##rina ##ry tract infection and c ##ir ##r ##hos ##is of the liver [SEP] history of present illness the patient is an male who had a history of co ##lon cancer [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_tkns = [tkn for tkn in tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])]\n",
    "print(' '.join(input_tkns))\n",
    "output_tkns = [tkn for tkn in tokenizer.convert_ids_to_tokens(labels[0])]\n",
    "print(' '.join(output_tkns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=labels, next_sentence_label=torch.LongTensor([0]))\n",
    "loss = outputs.loss\n",
    "lm_logits = outputs.prediction_logits\n",
    "nsp_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 28996])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_mask = torch.nn.functional.softmax(lm_logits[0], dim=1)\n",
    "lm_label = torch.argmax(softmax_mask, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1105,  2628,  1114,   190,  9324,  1616, 14441,  8974,  1105,   172,\n",
       "         3161,  1197, 15342,  1548,  1104,  1103, 11911,   119,  1607,  1104,\n",
       "         1675,  6946,  1103,  5351,  1110,   170,  2581,  1150,  1125,   170,\n",
       "         1607,  1104,  1884,  4934,  4182,   119])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_label.shape\n",
    "lm_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and associated with u ##rina ##ry tract infection and c ##ir ##r ##hos ##is of the liver . history of present illness the patient is a male who had a history of co ##lon cancer .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens(lm_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.nn.functional.softmax(nsp_logits, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sentence1 = 'associated diagnosis urinary tract infection and cirrhosis of the liver'\n",
    "sentence2 = 'history of present illness the patient is an male who had a history of colon cancer.'\n",
    "encoding = tokenizer(sentence1, sentence2, return_tensors='pt')\n",
    "outputs = nsp_model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSentencePredictorOutput(loss=tensor(0.0792, grad_fn=<NllLossBackward0>), logits=tensor([[-0.9300,  1.5663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0,0] < logits[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.nn.functional.softmax(outputs.logits[0], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/48jnd6px0rn3ll7myxfmb9sm0000gn/T/ipykernel_43482/65648085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./clinicalBERT-master/lm_pretraining/out/n2c2_train.tfrecord'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "filename = './clinicalBERT-master/lm_pretraining/out/n2c2_train.tfrecord'\n",
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 178, 102, 103, 102])\n",
    "tokenizer.convert_ids_to_tokens([1117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from datasets import ClassLabel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, TrainingArguments, DataCollatorForLanguageModeling, Trainer\n",
    "\n",
    "datasets = ['This is my first sentence.', 'This is the second sentence.', 'What about the third?'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "train_dl = DataLoader(inputs, batch_size=3, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "model_checkpoint = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(tokenized_datasets['train']['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets['train'][10]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'][10]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets['train'][10]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dt = DatasetDict({'train': lm_datasets['train'][:2], 'validation': lm_datasets['validation'][:2], 'test': lm_datasets['test'][:2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dt[\"train\"],\n",
    "    eval_dataset=lm_dt[\"validation\"],\n",
    "    data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, AutoTokenizer, AutoModel, AutoModelForMaskedLM, BertForMaskedLM, BertTokenizer, BertModel, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", output_attentions=True)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = ['This is my first sentence.', 'This is my second sentence.', 'This is my third sentence.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(raw_datasets, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "train_dl = DataLoader(out, batch_size=3, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    break\n",
    "print({k: v for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(out[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(**train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(out[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(torch.argmax(res[0][0], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundancy Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from IPython.display import display, Markdown\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from spacy import Language\n",
    "nlp = spacy.load('en_core_sci_md', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = \"\"\"Admission Date:  [**2174-4-18**]              Discharge Date:   [**2174-5-17**]\n",
    "\n",
    "Date of Birth:  [**2135-11-15**]             Sex:   F\n",
    "\n",
    "Service: MEDICINE\n",
    "\n",
    "Allergies:\n",
    "Prochlorperazine / Heparin Agents\n",
    "\n",
    "Attending:[**First Name3 (LF) 3918**]\n",
    "Chief Complaint:\n",
    "Abdominal Pain\"\"\"\n",
    "\n",
    "for t in re.finditer('\\[\\*\\*.+?\\*\\*\\]', re.sub('  +', ' ', prova.replace('\\n', ' '))):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('  +', ' ', prova.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note = \"\"\"\n",
    "470971328 | AECH | 09071283 | | 6159055 | 5/26/2006 12:00:00 AM | PNUEMONIA | Signed | DIS | Admission Date: 4/22/2006 Report Status: Signed\n",
    "\n",
    "Discharge Date: 7/27/2006\n",
    "ATTENDING: CARINE , WALTER MD\n",
    "SERVICE:\n",
    "Medicine Service.\n",
    "ADMISSION INFORMATION AND CHIEF COMPLAINT:\n",
    "Hypoxemic respiratory failure.\n",
    "HISTORY OF PRESENT ILLNESS:\n",
    "The patient is a 57-year-old woman with a past medical history of\n",
    "OSA , asthma , CAD status post CABG. On 8/19/06 , she underwent a\n",
    "right total knee replacement at Dola Elan Hospital .  On\n",
    "8/9/06 , she was discharged to rehabilitation. There , she\n",
    "experienced fever , cough and dyspnea. She was started on\n",
    "vancomycin , ceftazidime , and Flagyl for presumed pneumonia. In\n",
    "the L ED , the patient was afebrile with a temperature of 97.6 ,\n",
    "pulse of 88 , blood pressure 117/70 , oxygen saturation 97% on 6\n",
    "liters nasal cannula. Her exam was notable for crackles in the\n",
    "left base and 1+ lower extremity edema.\n",
    "ADMISSION LABS:\n",
    "Notable for white blood cell count of 20 , hematocrit 3of 5 ,\n",
    "platelets of 442 , 000 , creatinine of 0.6 , and INR of 1.2. Her\n",
    "admission EKG revealed sinus tachycardia of 119 beats per minute ,\n",
    "normal axis , QRS 104 milliseconds , QTC 461 milliseconds , no\n",
    "evidence of atrial enlargement or ventricular hypertrophy , poor\n",
    "R-wave progression , 2 mm ST depressions and T-wave inversions in\n",
    "leads 1 , aVL , V5 , V6 , 1 mm J-point elevation in V3 ( prior EKG\n",
    "showed T-wave inversions in 1 , and aVL with no ST depressions ).\n",
    "Her admission chest x-ray revealed bilateral diffuse patchy\n",
    "opacities.\n",
    "The patient was presumed to have pneumonia versus CHF. She was\n",
    "treated with vancomycin , cefotaxime , levofloxacin , and\n",
    "azithromycin , and was admitted to the Medicine Service for\n",
    "further evaluation and management.\n",
    "PAST MEDICAL HISTORY:\n",
    "1. Left carotid artery stenosis status post CEA.\n",
    "2. Right carotid artery stenosis , status post angioplasty.\n",
    "3. OSA.\n",
    "4. Asthma.\n",
    "5. CAD status post three-vessel CABG in 2004 and subsequent PCI\n",
    "to the ramus in 2005.\n",
    "6. 70-80% RCA stenosis not bypassed during CABG.\n",
    "7. Hypertension.\n",
    "8. CHF , ejection fraction 45-50%.\n",
    "9. AS status post aortic valve replacement.\n",
    "10. Pericarditis removal.\n",
    "11. Diabetes.\n",
    "12. Peripheral vascular disease.\n",
    "MEDICATIONS AT REHAB:\n",
    "1. Vancomycin 1 gram IV q. 12h. , ( first dose 27 of March ).\n",
    "2. Ceftazidime 1 g IV q. 8h. , ( first dose 7/17/06 )\n",
    "3. Flagyl 500 mg IV q. 8h. , ( first dose 7/17/06 .\n",
    "4. Advair 100/50 inhaled b.i.d.\n",
    "5. Aspirin 325 mg p.o. daily.\n",
    "6. Lipitor 80 mg p.o. at bedtime.\n",
    "7. Zetia 10 mg p.o. daily.\n",
    "8. Lopressor 75 mg p.o. q. 6h.\n",
    "9. Lasix 1 tablet p.o. daily.\n",
    "10. Colace 100 mg p.o. b.i.d.\n",
    "11. Multivitamin 1 tab p.o. daily.\n",
    "12. CaCO3 500 mg p.o. daily.\n",
    "13. Cholecalciferol 400 units p.o. daily.\n",
    "14. Ferrous sulfate 300 mg p.o. t.i.d.\n",
    "15. Folic acid 1 mg p.o. daily.\n",
    "16. Avapro 225 mg p.o. daily.\n",
    "17. Lantus 100 units subq daily.\n",
    "18. Lispro sliding scale.\n",
    "19. Coumadin.\n",
    "20. P.r.n. oxycodone , Tylenol , Benadryl , and Metamucil.\n",
    "ALLERGIES:\n",
    "Lisinopril leads to cough and metformin leads to GI distress.\n",
    "SOCIAL HISTORY:\n",
    "The patient was formerly employed as a cashier. She has two\n",
    "children. She is a former cigarette smoker. She does not use\n",
    "alcohol.\n",
    "FAMILY HISTORY:\n",
    "The patient has a positive family history of coronary disease ,\n",
    "hypertension and diabetes.\n",
    "HOSPITAL COURSE BY SYSTEM/ PROBLEM:\n",
    "Persistent pulmonary\n",
    "1. Hypoxemic respiratory failure. On 1/7/06 , shortly after\n",
    "her admission to the medical floor , the patient was noted to be\n",
    "in respiratory distress with tachypnea , accessory muscle use and\n",
    "oxygen saturation of 68% on 6 liters nasal cannula. She was\n",
    "placed on a nonrebreather. Her oxygen saturation increased to\n",
    "93%; however , she continued to be in respiratory distress with\n",
    "tachypnea and accessory muscle use. She was intubated and\n",
    "transferred to the Medical Intensive Care Unit for further\n",
    "evaluation and management. Her respiratory failure was thought\n",
    "to be secondary to pneumonia with a component of superimposed\n",
    "volume overload. She was treated with a 10-day course of\n",
    "vancomycin , levofloxacin and ceftazidime as well as with IV\n",
    "Lasix. She underwent a code green on 7/30/06 during an ETT tube\n",
    "change , wherein a patent airway was transiently loss. The\n",
    "patient was slow to wean from the ventilator. Her chest imaging\n",
    "revealed persistent bilateral opacifications. It was thought\n",
    "that after an initial infectious insult , the patient developed\n",
    "ARDS. On 9/14/06 , the patient underwent bronchoscopy and BAL\n",
    "revealing MRSA and HSV. The patient was treated with a 10-day\n",
    "course of acyclovir for presumed HSV tracheobronchitis. Given\n",
    "her inability to be weaned from the vent , the patient underwent a\n",
    "tracheostomy on 4/17/06 .  Post-tracheostomy , the patient\n",
    "alternated between pressure support ventilation with low driving\n",
    "pressure and PEEP with a trach collar..\n",
    "Infectious disease:\n",
    "1. Fevers. From 1/7/06 to 7/28/06 , the patient was treated\n",
    "with vancomycin , levofloxacin and ceftazidime for hospital\n",
    "acquired pneumonia. After discontinuation of her antibiotics ,\n",
    "the patient continued to spike fevers and evidence of\n",
    "leukocytosis. In verification into source of her fevers included\n",
    "serial blood cultures , urine cultures and C-dif. Positive data\n",
    "included: 7/28/06 , urine culture with yeast , 7/28/06 , blood\n",
    "culture with coag-negative staph , 9/14/06 BAL washings with HSV ,\n",
    "9/14/06 , blood culture with coag-negative staph , 7/11/06 , blood\n",
    "culture with coag-negative staph , 2/29/06 , urine culture with\n",
    "yeast , 2/29/06 BAL washings with MRSA , 10/28/06 urine with yeast\n",
    "urine with yeast , 10/28/06 , 6/23/06 , 4/17/06 , and 10/11/06\n",
    "sputum with MRSA. 7/28/06 , chest CT with bilateral\n",
    "opacification in the lung parenchyma. 2/29/06 facial CT with\n",
    "left sphenoid maxillary thickening. 2/29/06 chest CT with\n",
    "bilateral opacification in the lung parenchyma. Of note , a\n",
    "2/29/06 abdominal CT showed no evidence of abdominal infection ,\n",
    "2/29/06 TTE showed no obvious vegetations , and 10/11/06 tap of\n",
    "the right knee grew no organisms. In light of the data above ,\n",
    "the patient&apos;s indwelling catheters were changed. She underwent\n",
    "treatment with linezolid x7 days for MRSA line infection. She\n",
    "also underwent treatment with acyclovir x10 days for HSV\n",
    "tracheobronchitis. The aforementioned antibiosis was mostly\n",
    "prophylactic. It was thought that the patient&apos;s intermittent\n",
    "fevers were not infectious , but rather reflected a drug allergy ,\n",
    "most likely to vancomycin. This hypothesis was supported by a\n",
    "robust eosinophilia coinciding with vancomycin administration.\n",
    "Shortly after vancomycin discontinuation , the patient&apos;s fevers\n",
    "resolved. The patient was afebrile for greater than 48 hours off\n",
    "all antibiotics prior to transfer to rehabilitation.\n",
    "Cardiovascular:\n",
    "1. Volume status. The patient&apos;s admission weight was 106.2 kg.\n",
    "It is unclear what her dry weight was. Given that pulmonary\n",
    "edema was thought to be contributing to the patient&apos;s slow\n",
    "ventilator wean , she was diuresed with a combination of Lasix and\n",
    "Diuril followed by combinations of torsemide and Diuril. Her\n",
    "discharge weight was 100.7 kilograms. Her diuretic regimen on\n",
    "discharge with torsemide 100 mg IV t.i.d. and Diuril 500 mg IV\n",
    "t.i.d. The patient&apos;s diuretic regimen will need be adjusted as\n",
    "her intake is adjusted. Her creatinine will need to be monitored\n",
    "very closely. Her weight will need to be checked daily.\n",
    "2. Pump: The patient underwent echocardiogram on 11/19/06 ,\n",
    "7/17/06 , and 2/29/06 .  On the whole , these studies revealed an\n",
    "ejection fraction of 45-50% , concentric LVH , global hypokinesis\n",
    "with regional variation , mild left atrial enlargement , mild\n",
    "tricuspid regurgitation , a question of mild atrial stenosis , and\n",
    "pulmonary artery pressures in the 40s. For her heart failure ,\n",
    "the patient was treated with Lopressor and diuretics as above.\n",
    "She was not started on an ACE inhibitor given her allergy\n",
    "( cough ). She was started on low-dose ARB.\n",
    "3. Ischemia: The patient has a history of coronary artery\n",
    "disease status post three-vessel CABG and subsequent\n",
    "single-vessel PPI. She has an RCA stenosis , 70-80% that has not\n",
    "intervened upon. On admission , in the setting of respiratory\n",
    "distress , the patient was in sinus tachycardia with rate related\n",
    "to lateral ST depressions. Her cardiac biomarkers were positive\n",
    "consistent with NSTEMI. It was thought that the patient\n",
    "experienced demand ischemia rather than an acute plaque rupture.\n",
    "On 10/8/06 , her troponin peaked at 8.53 , her CK at 275 , and her\n",
    "MB of 16.3. The patient was treated with aspirin , Lopressor , and\n",
    "Zocor. She was not started on an ACE inhibitor as detailed\n",
    "above. She will likely warrant Cardiology followup with possible\n",
    "RCA revascularization.\n",
    "Neuro:\n",
    "1. Sedation: While intubated , the patient was treated with IV\n",
    "Versed and fentanyl titrated to light sedation. After her\n",
    "tracheostomy , the patient&apos;s Versed and fentanyl drips were\n",
    "discontinued. She was treated with Seroquel at bedtime to\n",
    "preclude nighttime agitation. Her QTC should be monitored while\n",
    "on Seroquel.\n",
    "GI:\n",
    "1. FEN: The patient initially received tube feeds via feeding\n",
    "tube. She underwent a PEG placement on 4/17/06 .  She continued\n",
    "on tube feeds. She also passed speech and Swallow and was thus\n",
    "started on p.o. feeds with aspiration precautions. She also\n",
    "received supplemental multivitamins , calcium carbonate , and\n",
    "cholecalciferol. The patient will require speech and swallow\n",
    "evaluation at rehabilitation. Now that she is awake , she may be\n",
    "able to tolerate oral feeds with aspiration precautions.\n",
    "2. Bowel regimen. The patient was treated with Colace , senna ,\n",
    "and Dulcolax.\n",
    "Heme:\n",
    "1. Anemia: The patient has known iron deficiency anemia. She\n",
    "was continued on iron and folate. She may benefit from an\n",
    "outpatient colonoscopy if she has not had one recently.\n",
    "2. Bleeding from tracheostomy site: On 4/24/06 , the patient\n",
    "was noted to have bleeding from her tracheostomy site. She had\n",
    "no hematocrit drop. She remained hemodynamically stable. She\n",
    "underwent a bronchoscopy , which showed no active bleeding. Her\n",
    "mild bleeding was thought to relate to suction trauma. Her\n",
    "prophylactic heparin was held x1 day. It was thought that the\n",
    "patient should be discharged on prophylactic heparin because her\n",
    "DVT risk is so high. Should the patient have intense bleeding\n",
    "from the tracheostomy site , a hematocrit drop or hemodynamic\n",
    "changes. Her heparin subcu should be discontinued , her\n",
    "hematocrit should be monitored closely , and she should be\n",
    "transfused as needed. She should also at that point probably\n",
    "undergo reevaluation by Pulmonary or Thoracics.\n",
    "Endocrine:\n",
    "1. Diabetes: The patient was treated with Lantus plus regular\n",
    "insulin q. 6h. plus sliding scale insulin while she was on tube\n",
    "feeds. Her insulin was changed on the night prior to discharge.\n",
    "She was started on Lantus 100 subq b.i.d. She got her first dose\n",
    "of 100 units subq on the evening prior to discharge , her morning\n",
    "sugars were in the mid 100s. Her blood sugars should be followed\n",
    "closely on the first one or two days at rehab. The blood sugars\n",
    "should be monitored every 2-3 hours and her insulin should be\n",
    "adjusted accordingly. Her insulin dose should be adjusted if her\n",
    "tube feeds are cycled rather than given continuously or if she is\n",
    "NPO.\n",
    "Ortho:\n",
    "1. Total knee replacement: The patient is status post right\n",
    "total knee replacement on 8/19/06 .  Her right knee has a\n",
    "well-healed incision and is not erythematous or tender. The\n",
    "patient was initially on low-dose anticoagulation with Coumadin.\n",
    "Her Coumadin was discontinued given her acute illness and her\n",
    "need for procedures. She was treated with heparin subq\n",
    "prophylactic doses 5000 units t.i.d. as described above. She\n",
    "will need rehabilitation for her knee.\n",
    "2. Prophylaxis: The patient was treated with subcutaneous\n",
    "heparin and Nexium.\n",
    "3. Access: The patient has peripheral IVs.\n",
    "DISCHARGE STATUS:\n",
    "On the day of discharge 3/8/06 , the patient was afebrile.\n",
    "Heart rate was in the 80s , blood pressure was in the\n",
    "100s-130s/60s-80s. Discharge weight was 100.7 kilograms. The\n",
    "patient was drowsy , but arouseable. She was breathing on a trach\n",
    "collar. She had decreased breath sounds at the bases. She had\n",
    "an S1 and S2 with a 2/6 systolic murmur at the lower sternal\n",
    "border. Her abdomen was soft and nontender with positive bowel\n",
    "sounds. Her trach and PEG site were intact without any\n",
    "surrounding erythema. Her extremities were warm without edema.\n",
    "She was on trach collar with 50% FiO2 and oxygen saturation of\n",
    "97%. Her discharge labs included a white blood cell count of 21 ,\n",
    "stable , hematocrit of 28 , and platelets 436 , 000. Her chem-7\n",
    "included a sodium of 131 , potassium 3 , chloride 87 , CO2 32 , BUN\n",
    "67 , creatinine 0.8 , and glucose 154. Her LFTs were normal. Her\n",
    "INR was 1.1. Her most recent microdata showed sputum with few\n",
    "polys and a few gram-positive cocci in clusters. Her chest x-ray\n",
    "showed a trach in place and bilateral hazy infiltrate consistent\n",
    "with resolving ARDS.\n",
    "An addendum will be given with discharge medications and doses.\n",
    "At rehab , the patient&apos;s weight should be monitored daily. Her\n",
    "blood sugars initially be monitored every 2-3 hours. After that ,\n",
    "her blood sugar monitoring can be spaced out to t.i.d. or\n",
    "whenever deemed appropriate by the physician at the\n",
    "rehabilitation facility. Her hematocrit should be checked\n",
    "particularly if she has any bleeding from her tracheostomy site;\n",
    "otherwise , the hematocrit can be checked every day. The white\n",
    "blood cell count should be monitored if the patient has any\n",
    "fever. The creatinine should be monitored daily in light of the\n",
    "patient&apos;s changing diuretic regimen. The patient in&apos;s and out&apos;s\n",
    "should be monitored closely.\n",
    "CONTACTS AT THE HOSPITAL:\n",
    "Liri Hospital Of regarding the patient&apos;s inpatient course or\n",
    "Dr. Julio Golding . Contacts regarding follow up issues , the\n",
    "patient&apos;s primary care physician , Dr. Adrian Mckinley Rickenbaker at Mawahutche Rehabilitation Hospital Of 080-524-3286. The patient&apos;s healthcare proxy is her\n",
    "sister , Kyle Lusk who is a registered nurse , her telephone\n",
    "number 507-122-9398 at home and 581-981-9887 cell phone.\n",
    "eScription document: 9-3627813 EMSSten Tel\n",
    "CC: Geraldo Lanny Bogda MD\n",
    "Los Di Wi\n",
    "CC: Raleigh Semmens MD\n",
    "Dictated By: DELIBERTIS , BRADLY\n",
    "Attending: BEAGLEY , ORLANDO\n",
    "Dictation ID 4434301\n",
    "D: 3/8/06\n",
    "T: 3/8/06\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note = \"\"\"\n",
    "Admission Date:  [**2174-4-18**]              Discharge Date:   [**2174-5-17**]\n",
    "\n",
    "Date of Birth:  [**2135-11-15**]             Sex:   F\n",
    "\n",
    "Service: MEDICINE\n",
    "\n",
    "Allergies:\n",
    "Prochlorperazine / Heparin Agents\n",
    "\n",
    "Attending:[**First Name3 (LF) 3918**]\n",
    "Chief Complaint:\n",
    "Abdominal Pain\n",
    "\n",
    "Major Surgical or Invasive Procedure:\n",
    "Upper GI series with small bowel follow through\n",
    "Right heart catheterization\n",
    "IR guided paracentesis\n",
    "\n",
    "\n",
    "History of Present Illness:\n",
    "38 yo F w/ h/o ALL in remission s/p cord transplant in [**1-13**],\n",
    "anthracycline-induced cardiomyopathy (EF 15-20% [**1-14**]) and\n",
    "recurrent nausea and vomiting who presents with abdominal pain,\n",
    "N/V x1 week\n",
    "\n",
    "Of note, the pt was admitted here from [**Date range (1) **] with nausea and\n",
    "vomitting of unclear etiology. When discharged, she was\n",
    "tolerating good PO and had planned f/u with neuro for ?\n",
    "abdominal migraine and GI for possible other contributing\n",
    "factors including food sensitivities and gastroparesis.\n",
    "\n",
    "In the ED, VS: 98.8 94 138/100 16 100% and [**10-15**] pain. CT A/P\n",
    "showed a small umbilical hernia; interval increase in size and\n",
    "mild fat stranding and interval increase in ascites compared to\n",
    "recent prior imaging.  WBC 12.4 with no left shift, bili 2.1 up\n",
    "from 1.1, Cr 2.7 up from 2.3.  Surgery was consulted give CT\n",
    "finding and did not feel there was an indication for surgery.\n",
    "She received iv zofran and morphine 4mg iv and 1L IVF.\n",
    "\n",
    "On arrival to the floor, patient reports [**11-14**] total body pain\n",
    "and nausea.  She has had ice chips today but threw them up in\n",
    "the ED.\n",
    "\n",
    "Review of Systems:\n",
    "(+) Per HPI\n",
    "(-) Review of Systems: Denies fevers, chest pain, SOB, diarrhea,\n",
    "constipation, dysuria, HA, change in vision or dizziness.\n",
    "\n",
    "\n",
    "Past Medical History:\n",
    "ONCOLOGIC HISTORY:\n",
    "ALL:\n",
    "- initially presented in [**2172-8-5**] right chest and right upper\n",
    "extremity pain and paresthesias and visual blurriness. WBC\n",
    "149,000; received leukapheresis, started on hydroxyurea. Dx'ed\n",
    "with precursor B-cell ALL.\n",
    "- underwent phase I induction with daunorubicin, vincristine,\n",
    "dexamethasone, L-asparaginase, MTX; phase II with\n",
    "cyclophosphamide, cytarabine, mercaptopurine, MTX\n",
    "- Bone Marrow Aspirate/Biopsy on [**2172-10-26**] showed no morphologic\n",
    "\n",
    "evidence of residual leukemia\n",
    "- underwent allo double cord blood SCT [**2173-1-11**], course\n",
    "complicated by neutropenic fever and acute skin GVHD\n",
    "\n",
    "OTHER MEDICAL HISTORY:\n",
    "- Embolic stroke in [**3-/2174**] on coumadin\n",
    "- Cardiomyopathy due to early anthracycline-related\n",
    "cardiotoxicity [**10/2172**]\n",
    "- Chronic kidney disease stage III/IV, baseline creatinine\n",
    "~2.0-2.2\n",
    "- Asthma\n",
    "- HTN\n",
    "- Cervical Intraepithelial neoplasia\n",
    "- C-section in [**2165**]\n",
    "\n",
    "\n",
    "Social History:\n",
    "Smoke: never\n",
    "EtOH: Occasional in past, none currently\n",
    "Drugs: Never\n",
    "Lives/works: Single, has two children (ages 7 and 18). Lives in\n",
    "[**Location 686**]. Was previously employed at [**Company 59330**], hasn't been\n",
    "working since being diagnosed with ALL in [**2172-8-5**].\n",
    "\n",
    "\n",
    "Family History:\n",
    "Mother with gastric cancer, passed at the age of 40\n",
    "Father with HTN.\n",
    "\n",
    "\n",
    "Physical Exam:\n",
    "VS: 98 145/76 87 15 100% RA\n",
    "GEN: well appearing F in NAD\n",
    "HEENT: slight dry MM, sclera anicteric, PERRL\n",
    "Cards: RR S1/S2 normal. prominent S3\n",
    "Pulm: CTAB\n",
    "Abd: Hyperactive BS.  Initially soft when palpating with\n",
    "stethoscope over all 4 quadrants then suddenly exquisitely\n",
    "tender on right.  No guarding initially.  Unable to assess for\n",
    "HSM.\n",
    "Extremities: wwp, no edema. PTs 2+.\n",
    "Neuro: CNs II-XII grossly intact. normal gait\n",
    "Psych:  overly dramatic affect\n",
    "\n",
    "\n",
    "Pertinent Results:\n",
    "On admission:\n",
    "[**2174-4-18**] 02:00PM BLOOD WBC-12.4* RBC-3.78* Hgb-11.4* Hct-36.3\n",
    "MCV-96 MCH-30.2 MCHC-31.4 RDW-16.5* Plt Ct-212\n",
    "[**2174-4-18**] 02:00PM BLOOD Neuts-67.3 Lymphs-23.8 Monos-7.7 Eos-0.5\n",
    "Baso-0.7\n",
    "[**2174-4-18**] 04:30PM BLOOD PT-30.1* PTT-29.4 INR(PT)-3.0*\n",
    "[**2174-4-18**] 02:00PM BLOOD UreaN-30* Creat-2.7* Na-142 K-4.8 Cl-99\n",
    "HCO3-31 AnGap-17\n",
    "[**2174-4-18**] 02:00PM BLOOD ALT-15 AST-18 AlkPhos-127* TotBili-2.1*\n",
    "[**2174-4-18**] 02:00PM BLOOD Lipase-63*\n",
    "[**2174-4-18**] 02:00PM BLOOD cTropnT-<0.01\n",
    "[**2174-4-18**] 02:00PM BLOOD Albumin-3.8 Calcium-9.3 Phos-4.8* Mg-2.0\n",
    "\n",
    "On discharge:\n",
    "[**2174-5-17**] 12:00AM BLOOD WBC-19.1* RBC-3.86* Hgb-11.3* Hct-37.7\n",
    "MCV-98 MCH-29.3 MCHC-30.0* RDW-17.8* Plt Ct-419\n",
    "[**2174-5-17**] 12:00AM BLOOD Neuts-81.3* Lymphs-11.4* Monos-6.9\n",
    "Eos-0.1 Baso-0.3\n",
    "[**2174-5-17**] 12:00AM BLOOD PT-31.2* PTT-28.6 INR(PT)-3.1*\n",
    "[**2174-5-17**] 12:00AM BLOOD Fibrino-162\n",
    "[**2174-5-17**] 12:00AM BLOOD Glucose-152* UreaN-78* Creat-2.9* Na-137\n",
    "K-4.7 Cl-95* HCO3-31 AnGap-16\n",
    "[**2174-5-17**] 12:00AM BLOOD ALT-51* AST-41* LD(LDH)-327* AlkPhos-107*\n",
    "TotBili-0.7\n",
    "[**2174-5-13**] 12:11PM BLOOD cTropnT-<0.01\n",
    "[**2174-5-17**] 12:00AM BLOOD Albumin-3.8 Calcium-8.7 Phos-2.1* Mg-2.7*\n",
    "UricAcd-8.7*\n",
    "[**2174-4-27**] 02:51AM BLOOD calTIBC-246* Ferritn-107 TRF-189*\n",
    "[**2174-5-2**] 05:55AM BLOOD [**Doctor First Name **]-NEGATIVE dsDNA-NEGATIVE\n",
    "[**2174-4-28**]  HHV-8 DNA, QL PCR       Not Detected\n",
    "[**2174-4-27**] QUANTIFERON(R)-TB GOLD        NEGATIVE\n",
    "NEGATIVE\n",
    "[**2174-4-29**] ACE, SERUM                    30                  [**10/2130**]\n",
    "U/L\n",
    "\n",
    "Micro:\n",
    "[**2174-4-25**] 1:07 pm PERITONEAL FLUID\n",
    "   GRAM STAIN (Final [**2174-4-25**]):\n",
    "      NO POLYMORPHONUCLEAR LEUKOCYTES SEEN.\n",
    "      NO MICROORGANISMS SEEN.\n",
    "   FLUID CULTURE (Final [**2174-4-28**]):    NO GROWTH.\n",
    "   ANAEROBIC CULTURE (Final [**2174-5-1**]):    NO GROWTH.\n",
    "   ACID FAST CULTURE (Preliminary):    NO MYCOBACTERIA ISOLATED.\n",
    "\n",
    "   ACID FAST SMEAR (Final [**2174-4-30**]):\n",
    "      NO ACID FAST BACILLI SEEN ON DIRECT SMEAR.\n",
    "   FUNGAL CULTURE (Final [**2174-5-13**]):    NO FUNGUS ISOLATED.\n",
    "\n",
    "[**2174-4-29**] 10:15 pm BLOOD CULTURE ( MYCO/F LYTIC BOTTLE)\n",
    "   BLOOD/FUNGAL CULTURE (Preliminary):    NO FUNGUS ISOLATED.\n",
    "   BLOOD/AFB CULTURE (Preliminary):    NO MYCOBACTERIA ISOLATED.\n",
    "\n",
    "\n",
    "CMV Viral Load (Final [**2174-5-6**]): CMV DNA not detected.\n",
    "\n",
    "ECG [**2174-4-18**]:\n",
    "Sinus rhythm. Possible left atrial abnormality. Lateral ST-T\n",
    "wave\n",
    "abnormality. Cannot rule out myocardial ischemia. Poor R wave\n",
    "progression.  Cannot rule out anterior wall myocardial\n",
    "infarction of indeterminate age. Compared to the previous\n",
    "tracing of [**2174-4-2**] multiple described abnormalities persist.\n",
    "\n",
    "CT abdomen/pelvis without contrast [**2174-4-18**]:\n",
    "FINDINGS: There is a small-to-moderate right pleural effusion,\n",
    "smaller in\n",
    "size compared to last CT torso. There is a small pericardial\n",
    "effusion. Study is suboptimal for evaluation of solid organs due\n",
    "to lack of IV contrast. With this limitation in mind, there is\n",
    "no extra- or intra-hepatic biliary duct dilatation. Previously\n",
    "described presumably focal nodular hyperplasia in segment VI of\n",
    "the liver is not clearly visualized on a non-contrast CT. There\n",
    "is a presumably gallbladder wall edema from third spacing with\n",
    "moderate amount of ascites. There is likely gallbladder sludge.\n",
    "Pancreas and bilateral adrenal glands are within normal limits\n",
    "considering the limitation of no contrast administration. There\n",
    "is interval increase in size of a fat-containing umbilical\n",
    "hernia measuring 2 cm in transverse dimension with mild fat\n",
    "stranding(2:50), correlate with point tenderness/physical exam.\n",
    "The appendix is not dilated (2:49), contains air and there is a\n",
    "likely small appendicolith (2:53). There is no bowel\n",
    "obstruction. There is no evidence of colonic wall thickening,\n",
    "although evaluation is suboptimal given lack of IV or PO\n",
    "contrast and adjacent ascites.. The kidneys are normal in size.\n",
    "There is no evidence of hydronephrosis. Due to lack of oral\n",
    "contrast, evaluation for mesenteric lymph nodes is suboptimal.\n",
    "There are scattered lymph nodes in the retroperitoneum, however,\n",
    "do not meet the CT criteria for pathologic enlargement.\n",
    "\n",
    "CT PELVIS: There is free fluid in the pelvis - ascites. The\n",
    "uterus and urinary bladder appear normal. The rectum and sigmoid\n",
    "have scattered diverticula; however, no evidence of\n",
    "diverticulitis.\n",
    "\n",
    "OSSEOUS STRUCTURES: No suspicious lytic or sclerotic lesion.\n",
    "There is soft tissue stranding suggesting anasarca.\n",
    "\n",
    "IMPRESSION:\n",
    "1. Mild-to-moderate right pleural effusion; however, interval\n",
    "decrease in size compared to prior.\n",
    "2. Moderate ascites with interval increase.\n",
    "3. No drainable fluid collection, however, evaluation is\n",
    "suboptimal due to lack of IV and oral contrast.\n",
    "4. Diverticulosis.\n",
    "5. Interval increase in size of a small fat-containing umbilical\n",
    "hernia with mild fat stranding, correlate with point tenderness.\n",
    "\n",
    "6. No bowel obstruction. No definite bowel wall thickening,\n",
    "although the examination is suboptimal for such.\n",
    "7. Pericardial effusion, similar to prior.\n",
    "\n",
    "RUQ ultrasound [**2174-4-18**]:\n",
    "FINDINGS: The liver is of normal echogenicity. Previously\n",
    "described presumably focal nodular hyperplasia in segment VI of\n",
    "the liver is not clearly visualized. There is no intra- or\n",
    "extra-hepatic biliary duct dilatation. The common bile duct\n",
    "measures 2 mm. There is ascites. There is gallbladder wall\n",
    "edema/thickening presumably from third spacing; the gallbadder\n",
    "is not distended. No convincing evidence of sludge on\n",
    "ultrasound. The main portal vein is patent. Pancreas is\n",
    "suboptimally evaluated due to overlapping bowel gas. There is a\n",
    "small-to-moderate right pleural effusion as seen on recent CT.\n",
    "\n",
    "IMPRESSION:\n",
    "1. Ascites.\n",
    "2. Gallbladder wall edema presumably from third spacing.\n",
    "3. Small-to-moderate right pleural effusion.\n",
    "4. No biliary duct dilatation.\n",
    "5. Previously described presummed focal nodular hyperplasia in\n",
    "segment VI of the liver is not clearly visualized.\n",
    "\n",
    "Small bowel follow through [**2174-4-20**]:\n",
    "IMPRESSION:\n",
    "1. Small, anterior cervical web that does not hinder the passage\n",
    "of a 13mm\n",
    "barium tablet.\n",
    "2. Filling defect in the mid esophagus just below the carina\n",
    "appears to be either extrinsic compression versus a submucosal\n",
    "lesion. In correlation with the comparison CT torso, mediastinal\n",
    "lesion is less likely. Submucosal esophageal lesion remains\n",
    "within the differential, and direct visualization with EGD is\n",
    "recommended. Other possibility includes an aberrant vessel in\n",
    "this vicinity.\n",
    "3. Mobile cecum which does not appear to be obstructive in any\n",
    "manner on today's examination.\n",
    "\n",
    "Renal ultrasound [**2174-4-20**]:\n",
    "FINDINGS: The right kidney measures 10.5 cm. The left kidney\n",
    "measures 9.7\n",
    "cm. There is no evidence of hydronephrosis, stone or mass\n",
    "bilaterally. The\n",
    "bladder is unremarkable. Moderate amount of ascites is\n",
    "incidentally noted.\n",
    "\n",
    "IMPRESSION: No hydronephrosis, stone or mass within the kidneys.\n",
    "\n",
    "\n",
    "Peritoneal Fluid [**2174-4-25**]:\n",
    "  ATYPICAL.\n",
    "  Scattered atypical lymphoid cells in a background of\n",
    "  reactive mesothelial cells\n",
    "\n",
    "IR guided paracentesis [**2174-4-25**]:\n",
    "IMPRESSION:\n",
    "Ultrasound-guided diagnostic paracentesis, with a total of 200\n",
    "mL of ascites removed.\n",
    "\n",
    "TTE [**2174-5-2**]:\n",
    "The left atrium is mildly elongated. Left ventricular wall\n",
    "thicknesses and cavity size are normal. There is severe global\n",
    "left ventricular hypokinesis (LVEF = 20 %). Systolic function of\n",
    "apical segments is relatively preserved. No masses or thrombi\n",
    "are seen in the left ventricle. Right ventricular chamber size\n",
    "is mildly increased with moderate global free wall hypokinesis.\n",
    "[Intrinsic right ventricular systolic function is likely more\n",
    "depressed given the severity of tricuspid regurgitation.] The\n",
    "aortic valve leaflets (3) appear structurally normal with good\n",
    "leaflet excursion and no aortic stenosis or aortic\n",
    "regurgitation. The mitral valve appears structurally normal with\n",
    "trivial mitral regurgitation. There is no mitral valve prolapse.\n",
    "Severe [4+] tricuspid regurgitation is seen. There is mild\n",
    "pulmonary artery systolic hypertension [In the setting of at\n",
    "least moderate to severe tricuspid regurgitation, the estimated\n",
    "pulmonary artery systolic pressure may be underestimated due to\n",
    "a very high right atrial pressure.] There is a small\n",
    "circumferential pericardial effusion without echocardiographic\n",
    "signs of tamponade.\n",
    "\n",
    "IMPRESSION: Severe biventricular global hypokinesis. Severe\n",
    "tricuspid regurgitation. Pulmonary artery systolic hypertension.\n",
    "Small circumferential pericardial effusion without evidence of\n",
    "tamponade physiology.\n",
    "Compared with the prior study (images reviewed) of [**2174-4-1**],\n",
    "the findings are similar.\n",
    "\n",
    "TTE [**2174-5-10**]:\n",
    "The left atrium is dilated. A left-to-right shunt across the\n",
    "interatrial septum is seen at rest consistent with a stretched\n",
    "patent foramen ovale (or small atrial septal defect). There is\n",
    "mild symmetric left ventricular hypertrophy. The left\n",
    "ventricular cavity size is normal with mildly impaired global\n",
    "left ventricular systolic function. The aortic valve leaflets\n",
    "(3) appear structurally normal with good leaflet excursion and\n",
    "no aortic stenosis or aortic regurgitation. The mitral valve\n",
    "appears structurally normal with trivial mitral regurgitation.\n",
    "The tricuspid valve leaflets are mildly thickened. There is\n",
    "moderate (2+) tricuspid regurgitation. There is mild pulmonary\n",
    "artery systolic hypertension. There is a small pericardial\n",
    "effusion. There are no echocardiographic signs of tamponade.\n",
    "Echocardiographic signs of tamponade may be absent in the\n",
    "presence of elevated right sided pressures.\n",
    "\n",
    "Compared with the prior study (images reviewed) of [**2174-5-6**],\n",
    "ther pericardial effusion is now smaller. Biventricular\n",
    "sysotolic function appears slightly less vigorous compared to\n",
    "the prior study (on a lower dose of milrinone now than during\n",
    "the prior study).\n",
    "\n",
    "Cardiac cath [**2174-5-5**]:\n",
    "COMMENTS:\n",
    "1. Hemodynamics measurements in this patient demonstrate low\n",
    "cardiac output. Following administration of milrinone, cardiac\n",
    "index increased to the low-normal range with 2.5 L/min/m2.\n",
    "2. Moderate pulmonary hypertension with right atrial v-waves\n",
    "consistent with severe TR noted. Pulmonary vascular resistance\n",
    "is elevated at 280 dyne-cm-sec5.\n",
    "\n",
    "FINAL DIAGNOSIS:\n",
    "1. Severe systolic ventricular dysfunction.\n",
    "2. Moderate diastolic ventricular dysfunction.\n",
    "3. Pulmonary hypertension\n",
    "\n",
    "LE ultrasound [**2174-5-13**]:\n",
    "IMPRESSION:\n",
    "1. No evidence for deep venous thrombosis in either lower\n",
    "extremity.\n",
    "2. 3.6 cm [**Hospital Ward Name 4675**] cyst in the right popliteal fossa as previous.\n",
    "Superficial soft tissue edema in the right mid thigh, may be\n",
    "related to partial rupture of [**Hospital Ward Name 4675**] cyst.\n",
    "\n",
    "TTE [**2174-5-16**]:\n",
    "The left atrium is dilated. Left ventricular wall thicknesses\n",
    "and cavity size are normal. The diameters of aorta at the sinus,\n",
    "ascending and arch levels are normal. The aortic valve leaflets\n",
    "(3) appear structurally normal with good leaflet excursion and\n",
    "no aortic stenosis or aortic regurgitation. The mitral valve\n",
    "leaflets are structurally normal. There is no mitral valve\n",
    "prolapse. Trivial mitral regurgitation is seen. Moderate [2+]\n",
    "tricuspid regurgitation is seen. There is moderate pulmonary\n",
    "artery systolic hypertension. There is a small pericardial\n",
    "effusion. The effusion appears circumferential. There are no\n",
    "echocardiographic signs of tamponade. Echocardiographic signs of\n",
    "tamponade may be absent in the presence of elevated right sided\n",
    "pressures.\n",
    "\n",
    "Compared with the prior study (images reviewed) of [**2174-5-10**],\n",
    "biventricular systolic function is slightly worse. The size of\n",
    "the pericardial effusion is slightly smaller.\n",
    "\n",
    "Brief Hospital Course:\n",
    "38 yo F w/ h/o ALL in remission s/p cord transplant in [**1-13**],\n",
    "anthracycline-induced cardiomyopathy (EF 15-20% [**1-14**]) and\n",
    "recurrent nausea and vomiting who presents with 1 week abd pain,\n",
    "acute on chronic renal failure and new hyperbilirubinemia.\n",
    "Unclear unifying diagnosis.\n",
    "\n",
    "# Acute on Chronic Abdominal Pain:  Pt noted to have significant\n",
    "abdominal pain as well as increased [**Month/Year (2) 4394**] on admission.  Of\n",
    "note, she had an extensive work up of her chronic abdominal pain\n",
    "in the past with no clear cause.  Abdominal CT was unrevealing\n",
    "for any obvious source of her pain. GI was consulted who\n",
    "recommended a SBFT which did not reveal any significant\n",
    "pathology.  GI recommended bentyl for antispasmodic effect.  She\n",
    "was also continued on her home MS contin and IV morphine for\n",
    "breakthrough.  Her pain persisted as did her [**Last Name (LF) 4394**], [**First Name3 (LF) **] the\n",
    "decision was made to perform a diagnositc paracentesis under\n",
    "ultrasound guidance.  200ml peritoneal fluid was removed.  This\n",
    "revealed 775 WBCs, but a lymphocytic/monocytic predominance with\n",
    "only 1% polys making SBP unlikely.  Fluid was sent for culture\n",
    "which showed no growth and flow cytometry which showed no\n",
    "evidence of ALL recurrence.  Despite lack of evidence for SBP,\n",
    "she was started on zosyn empirically which was stopped on [**5-2**].\n",
    "She continued to have mild-moderate abdominal pain but was able\n",
    "to eat full meals and had BMs. She was continued on her home\n",
    "mscontin and morphine IR.\n",
    ".\n",
    "# Anthracycline-induced/ GVHD cardiomyopathy:  EF <20% on echo\n",
    "from 2/[**2174**].  Pt was maintained on diuresis as above, which was\n",
    "subsequently held in the setting of rising creatinine with\n",
    "improvement in creatinine. Torsemide was slowly reintroduced and\n",
    "uptitrated to 40mg [**Hospital1 **] which caused another bump in creatinine\n",
    "to 3.0, so renal and cardiology were consulted. Renal ultrasound\n",
    "was unrevealing.  She was then taken to the Cath lab and placed\n",
    "on a milrinone/lasix gtt and transfered to the CCU. Her volume\n",
    "overload slowly improved and her peripheral edema/ascites slowly\n",
    "improved as well. A repeat echo showed improved EF to 40-45% on\n",
    "the milrinone gtt. She was then started on solumedrol 30mg IV\n",
    "due to a concern for GVHD directed towards myocardium.  After\n",
    "further discussion between cardiology and her oncology team she\n",
    "was also started on cellcept for further management of her GVHD.\n",
    " She did well on milrinone and lasix drip, but the drip was\n",
    "stopped when her creatinine bumped to 3.0 and it was felt her\n",
    "volume status was near maximization.  Her milrinone was then\n",
    "discontiued and she was then transferred back to [**Hospital1 3242**] for further\n",
    "management of her abdominal pain and GVHD.  She was continued on\n",
    "torsemide for diuresis with close follow-up with her outpatient\n",
    "cardiologist.  Of note, she had frequent alarms on telemetry for\n",
    "tachycardia that cardiologist felt was mostly due to artifact;\n",
    "her beta blocker was uptitrated.  Repeat TTE prior to discharge\n",
    "showed an EF of 35-40%.  She was discharged home on cellcept and\n",
    "prednisone for possible GVHD.\n",
    "\n",
    "# Acute Renal Failure:  On admission Cr was 2.7 (recent baseline\n",
    "was 2), but at last discharge Cr was 2.3.  Renal saw the patient\n",
    "who thought this was likely from overdiuresis (home torsemide\n",
    "regimen of 20mg [**Hospital1 **]) in conjunction with her [**Last Name (LF) **], [**First Name3 (LF) **] recommended\n",
    "holding diuresis. Her Cr subsequently improved, but in the\n",
    "setting of her worsening [**First Name3 (LF) 4394**] and her cardiomyopathy,\n",
    "decision was made to slowly add back diuresis, and eventually\n",
    "she was up titrated to toresemide 40mg [**Hospital1 **] and her [**Last Name (un) **] was\n",
    "restarted.  With this, however, her Cr began to climb again to\n",
    "3.0.  Given the delicate balance between her renal failure\n",
    "cardiomyopathy, cardiology/renal were consulted. Given her\n",
    "depressed EF, her rising Cr was thought to be [**3-9**] volume\n",
    "overload. She was sent to the cath lab and started on a\n",
    "milrinone/lasix gtt and transfered to the CCU with a goal\n",
    "diuresis of 1L per day. She was actively diuresed on her\n",
    "milrinone and lasix drip with a total net negative of close to\n",
    "9L.  Her Cr then returned to baseline by time of discharge and\n",
    "she was discharged home on torsemide.\n",
    "\n",
    "# Hyperbilirubinemia:  Unclear cause, could have been related to\n",
    "a viral infection but no transaminitis to support this.  RUQ u/s\n",
    "without cause for pain. This trended down to normal values and\n",
    "remained stable by time of discharge\n",
    "\n",
    "# Leukocytosis: patient had uptrending WBC in setting of\n",
    "starting solumedrol, clutures were sent which revealed no\n",
    "growth.\n",
    ".\n",
    "# H/O Embolic Stroke:  Has new opening of PFO based on most\n",
    "recent echo which likely contributed to her recent stroke.  She\n",
    "was maintained on coumadin 4mg daily, but anticoagulation was\n",
    "held on day of paracentesis and remained subtherapeutic for\n",
    "several days, so she was maintained on a heparin drip to bridge\n",
    "her to a therapeutic INR [**3-10**]. She was maintained on a decreased\n",
    "dose of coumadin throughout hospital admission with INR within\n",
    "goal between 2 and 3. She was arranged with follow-up at\n",
    "outpatient [**Hospital3 **].\n",
    "\n",
    "Medications on Admission:\n",
    "Carvedilol 25 mg [**Hospital1 **]\n",
    "Fluticasone-salmeterol [**Hospital1 **]\n",
    "Morphine 15 mg q6h prn pain\n",
    "Valsartan 40 mg qd\n",
    "Torsemide 20 mg [**Hospital1 **]\n",
    "Multivitamin qd\n",
    "Albuterol prn\n",
    "Lorazepam 0.5 mg q6h prn nausea\n",
    "Warfarin 4 mg qd\n",
    "Ondansetron 8 mg tid prn\n",
    "Pentamidine 300 mg inhalation qmonth\n",
    "Colace 100 mg qd prn\n",
    "\n",
    "\n",
    "Discharge Medications:\n",
    "1. fluticasone-salmeterol 250-50 mcg/dose Disk with Device Sig:\n",
    "One (1) Disk with Device Inhalation [**Hospital1 **] (2 times a day).\n",
    "2. docusate sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2\n",
    "times a day).\n",
    "3. lorazepam 0.5 mg Tablet Sig: One (1) Tablet PO every six (6)\n",
    "hours as needed for nausea.\n",
    "Disp:*60 Tablet(s)* Refills:*0*\n",
    "4. albuterol sulfate 90 mcg/Actuation HFA Aerosol Inhaler Sig:\n",
    "Two (2) Puff Inhalation Q4H (every 4 hours) as needed for sob or\n",
    "wheeze.\n",
    "5. Zofran 8 mg Tablet Sig: One (1) Tablet PO every eight (8)\n",
    "hours as needed for nausea.\n",
    "6. multivitamin     Tablet Sig: One (1) Tablet PO once a day.\n",
    "7. metoprolol succinate 100 mg Tablet Extended Release 24 hr\n",
    "Sig: One (1) Tablet Extended Release 24 hr PO DAILY (Daily).\n",
    "Disp:*30 Tablet Extended Release 24 hr(s)* Refills:*0*\n",
    "8. morphine 15 mg Tablet Extended Release Sig: One (1) Tablet\n",
    "Extended Release PO Q12H (every 12 hours).\n",
    "Disp:*60 Tablet Extended Release(s)* Refills:*0*\n",
    "9. dicyclomine 20 mg Tablet Sig: One (1) Tablet PO four times a\n",
    "day.\n",
    "Disp:*120 Tablet(s)* Refills:*0*\n",
    "10. allopurinol 100 mg Tablet Sig: One (1) Tablet PO DAILY\n",
    "(Daily).\n",
    "Disp:*30 Tablet(s)* Refills:*0*\n",
    "11. sulfamethoxazole-trimethoprim 400-80 mg Tablet Sig: One (1)\n",
    "Tablet PO DAILY (Daily).\n",
    "Disp:*30 Tablet(s)* Refills:*0*\n",
    "12. acyclovir 400 mg Tablet Sig: One (1) Tablet PO Q12H (every\n",
    "12 hours).\n",
    "Disp:*60 Tablet(s)* Refills:*0*\n",
    "13. torsemide 20 mg Tablet Sig: Two (2) Tablet PO DAILY (Daily).\n",
    "Disp:*60 Tablet(s)* Refills:*0*\n",
    "14. simethicone 80 mg Tablet, Chewable Sig: One (1) Tablet,\n",
    "Chewable PO QID (4 times a day) as needed for abdominal pain or\n",
    "gas.\n",
    "Disp:*120 Tablet, Chewable(s)* Refills:*0*\n",
    "15. mycophenolate mofetil 500 mg Tablet Sig: Two (2) Tablet PO\n",
    "BID (2 times a day).\n",
    "Disp:*120 Tablet(s)* Refills:*0*\n",
    "16. prednisone 20 mg Tablet Sig: Three (3) Tablet PO DAILY\n",
    "(Daily).\n",
    "Disp:*90 Tablet(s)* Refills:*0*\n",
    "17. magnesium hydroxide 400 mg/5 mL Suspension Sig: Thirty (30)\n",
    "ML PO Q6H (every 6 hours) as needed for constipation.\n",
    "Disp:*500 ML(s)* Refills:*0*\n",
    "18. morphine 15 mg Tablet Sig: One (1) Tablet PO every twelve\n",
    "(12) hours as needed for pain.\n",
    "19. warfarin 2 mg Tablet Sig: One (1) Tablet PO once a day.\n",
    "Disp:*30 Tablet(s)* Refills:*0*\n",
    "\n",
    "\n",
    "Discharge Disposition:\n",
    "Home\n",
    "\n",
    "Discharge Diagnosis:\n",
    "Primary:\n",
    "-Abdominal Pain\n",
    "-Acute on chronic renal failure\n",
    "-Systolic Heart failure\n",
    "\n",
    "Secondary:\n",
    "-ALL\n",
    "-History of embolic stroke\n",
    "\n",
    "\n",
    "Discharge Condition:\n",
    "Mental Status: Clear and coherent.\n",
    "Level of Consciousness: Alert and interactive.\n",
    "Activity Status: Ambulatory - Independent.\n",
    "\n",
    "\n",
    "Discharge Instructions:\n",
    "Ms. [**Known lastname **],\n",
    "\n",
    "You were admitted to the hospital for abdominal pain.  Your pain\n",
    "was treated with pain medications, and a new medication called\n",
    "Bentyl.  You were also switched to a longer acting form of your\n",
    "morphine.  We did a test to look at your small bowel which was\n",
    "negative.  At this point we are not sure what is causing your\n",
    "pain, but you had increased swelling of your abdomen which\n",
    "likely contributed to your pain.\n",
    "\n",
    "You underwent a right heart catheterization and [**Known lastname 461**]\n",
    "to assess your heart function because worsening heart failure\n",
    "can cause fluid in your belly and worsening kidney disease.  You\n",
    "were at the cardiac intensive care unit and placed on a\n",
    "medication that improved your heart function.  A repeat\n",
    "[**Known lastname 461**] prior to your discharge showed that your heart\n",
    "function has improved somewhat and is stable.  You will follow\n",
    "up closely with your cardiologist as several of your heart\n",
    "medications have changed.  You were started on steroids and\n",
    "mycophenolate mofetil because it was felt that you heart\n",
    "problems may be due to your leukemia.\n",
    "\n",
    "You also had some worsening of your renal failure.  You were\n",
    "followed by our kidney consult team while you were in the\n",
    "hospital.  Your kidney function was stable prior to discharge.\n",
    "\n",
    "We made the following changes to your medications:\n",
    "-Mycophenolate Mofetil 1000mg twice a day was started\n",
    "-Prednisone 60mg daily was started\n",
    "-Coumadin was decreased to 2mg daily\n",
    "-Torsemide was increased to 40mg daily\n",
    "-Please hold your valsartan until you see your cardiologist\n",
    "-Metoprolol succinate 100mg daily was started; please stop\n",
    "carvedilol\n",
    "-Bentyl (dicyclomine) was started for your abdominal pain\n",
    "-Simethicone was started for abdominal discomfort/gas\n",
    "-Your morphine was switched to long-acting Morphine 15mg twice a\n",
    "day\n",
    "-Bactrim single strength, 1 tablet daily, was started to help\n",
    "prevent infection\n",
    "-Acyclovir 400mg twice a day was started to help prevent\n",
    "infection\n",
    "-Allopurinol 100mg daily was started because your uric acid\n",
    "levels were high\n",
    "\n",
    "Weigh yourself every morning, [**Name8 (MD) 138**] MD if weight goes up more\n",
    "than 3 lbs.\n",
    "\n",
    "Followup Instructions:\n",
    "You have the following appointments [**Name8 (MD) 1988**] for you.  You will\n",
    "need to follow up at [**Hospital3 **] on Thursday,\n",
    "[**2174-5-19**], for an INR (coumadin level) check.  Please come to\n",
    "the [**Hospital Ward Name 23**] Center [**Location (un) 895**] for this lab test between 9am and\n",
    "5pm.\n",
    "\n",
    "Department: HEMATOLOGY/ONCOLOGY\n",
    "When: FRIDAY [**2174-5-20**] at 3:30 PM\n",
    "With: [**Name6 (MD) **] [**Name8 (MD) **], MD [**Telephone/Fax (1) 22**]\n",
    "Building: [**Hospital6 29**] [**Location (un) **]\n",
    "Campus: EAST     Best Parking: [**Hospital Ward Name 23**] Garage\n",
    "\n",
    "Department: HEMATOLOGY/ONCOLOGY\n",
    "When: FRIDAY [**2174-5-20**] at 3:30 PM\n",
    "With: [**First Name11 (Name Pattern1) **] [**Last Name (NamePattern1) 10565**], NP [**Telephone/Fax (1) 22**]\n",
    "Building: SC [**Hospital Ward Name 23**] Clinical Ctr [**Location (un) 24**]\n",
    "Campus: EAST     Best Parking: [**Hospital Ward Name 23**] Garage\n",
    "\n",
    "[**First Name8 (NamePattern2) 449**] [**Last Name (NamePattern1) 437**] MD, Cardiology\n",
    "[**Last Name (LF) 766**], [**2174-5-30**] at 11:00AM\n",
    "SC [**Hospital Ward Name **] CLINICAL CTR, [**Location (un) **]\n",
    "\n",
    "Department: WEST [**Hospital 2002**] CLINIC\n",
    "When: THURSDAY [**2174-6-9**] at 10:00 AM\n",
    "With: [**First Name11 (Name Pattern1) 177**] [**Last Name (NamePattern4) 720**], M.D. [**Telephone/Fax (1) 721**]\n",
    "Building: De [**Hospital1 **] Building ([**Hospital Ward Name 121**] Complex) [**Location (un) **]\n",
    "Campus: WEST     Best Parking: [**Street Address(1) 592**] Garage\n",
    "\n",
    "\n",
    "                             [**Name6 (MD) **] [**Name8 (MD) **] MD [**MD Number(2) 3922**]\n",
    "\n",
    "Completed by:[**2174-5-26**]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting sentence boundaries\n",
    "@Language.component('sentbound')\n",
    "def custom_sentencizer(doc):\n",
    "    for i, token in enumerate(doc[:-1]):\n",
    "#         if token.text == r' ?\\.' and (doc[i+1].is_title or doc[i+1].is_upper or doc[i+1].like_num):\n",
    "        if re.match(r'^ ?\\.', token.text) and (doc[i+1].is_upper or doc[i+1].is_title):\n",
    "            doc[i+1].is_sent_start = True\n",
    "            token.is_sent_start = False\n",
    "        elif re.match(r'[0-9]{1,2}\\.$', token.text):\n",
    "            if not doc[i-1].is_stop:\n",
    "                token.is_sent_start = True\n",
    "                doc[i+1].is_sent_start = False\n",
    "            else:\n",
    "                token.is_sent_start = False\n",
    "                doc[i+1].is_sent_start = True\n",
    "#             print(token.text, token.is_sent_start, doc[i+1], doc[i+1].is_sent_start)\n",
    "        elif token.text == '-' and doc[i+1].text != '-':\n",
    "            token.is_sent_start = True\n",
    "            doc[i+1].is_sent_start = False\n",
    "        else:\n",
    "            doc[i+1].is_sent_start = False\n",
    "#         if re.match(r'[0-9]{1,2}\\.$', token.text) :\n",
    "#             print(token.text, token.is_sent_start, doc[i-1], doc[i-1].is_stop)\n",
    "#         print(token, token.is_sent_start)\n",
    "#         if re.match(\"VENBONLEA HEALTH\", token.text):\n",
    "#             print(doc[i-1], doc[i-1].is_sent_start, token, token.is_sent_start, doc[i+1], doc[i+1].is_sent_start)\n",
    "    return doc\n",
    "\n",
    "#convert de-identification text, dates and lab results into one token\n",
    "@Language.component('tkndef')\n",
    "def def_tokens(doc):\n",
    "    patterns = [r'\\[\\*\\*.+?\\*\\*\\]', ##de-identification\n",
    "                r'[0-9]{1,4}[/\\-][0-9]{1,2}[/\\-][0-9]{1,4}', ##date\n",
    "                r'[0-9]+\\-?[0-9]+%?', ##lab/test result\n",
    "                r'[0-9]+/[0-9]+', ##lab/test result\n",
    "                r'([0-9]{1,3} ?, ?[0-9]{3})+', ##number >= 10^3\n",
    "                r'[0-9]{1,2}\\+', ##lab/test result\n",
    "                r'[A-Za-z]{1,3}\\.', ##abbrv, e.g., pt.\n",
    "                r'[A-Za-z]\\.([a-z]\\.){1,2}', ##abbrv, e.g., p.o., b.i.d.\n",
    "                r'[0-9]{1,2}h\\.', ##time, e.g., 12h\n",
    "                r'(\\+[0-9] )?\\(?[0-9]{3}\\)?[\\- ][0-9]{3}[\\- ][0-9]{4}', ##phone number\n",
    "                r'[0-9]{1,2}\\.' ##Numbered lists\n",
    "               ]\n",
    "    for expression in patterns:\n",
    "        for match in re.finditer(expression, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            # This is a Span object or None if match doesn't map to valid token sequence\n",
    "            if span is not None:\n",
    "                print(\"Found match:\", span.text)\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(span)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('tkndef', before='parser')\n",
    "nlp.add_pipe('sentbound', before='parser')\n",
    "\n",
    "doc = nlp(re.sub('  +', ' ', note.replace('\\n', ' ')))\n",
    "# new_doc = fix_deid_tokens(patterns, doc)\n",
    "# expression = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\"\n",
    "# for match in re.finditer(expression, doc.text):\n",
    "#     start, end = match.span()\n",
    "#     span = doc.char_span(start, end)\n",
    "#     # This is a Span object or None if match doesn't map to valid token sequence\n",
    "#     if span is not None:\n",
    "#         print(\"Found match:\", span.text)\n",
    "#     with doc.retokenize() as retokenizer:\n",
    "#         retokenizer.merge(span)\n",
    "        \n",
    "# @Language.component('mycomp2')\n",
    "# def lower_all(doc):\n",
    "#     for i, tkn in enumerate(doc):\n",
    "#         doc[i].text = doc[i].text.lower()\n",
    "#     return doc.lower()\n",
    "\n",
    "\n",
    "# nlp.add_pipe(\"merge_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(doc.sents):\n",
    "    print(s)\n",
    "    for t in s:\n",
    "        print(f\"{t}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tk in enumerate(doc):\n",
    "    print(tk, doc[i].is_sent_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(note)\n",
    "patterns = [r\"\\[\\*\\*|\\*\\*\\]\", ##de-identification\n",
    "                r'[0-9]{1,4}[/\\-][0-9]{1,2}[/\\-][0-9]{1,4}', ##date\n",
    "            r'(\\+[0-9] )?\\(?[0-9]{3}\\)?[\\- ][0-9]{3}[\\- ][0-9]{4}', ##phone number\n",
    "                r'[0-9]+\\-?[0-9]+%?', ##lab/test result\n",
    "                r'[0-9]+/[0-9]+', ##lab/test result\n",
    "                r'([0-9]{1,3} ?, ?[0-9]{3})+', ##number >= 10^3\n",
    "                r'[0-9]{1,2}\\+', ##lab/test result\n",
    "                r'[A-Za-z]{1,3}\\.', ##abbrv, e.g., pt.\n",
    "                r'[A-Za-z]\\.([a-z]\\.){1,2}', ##abbrv, e.g., p.o., b.i.d.\n",
    "                r'[0-9]{1,2}h\\.', ##time, e.g., 12h\n",
    "               ]\n",
    "for expression in patterns:\n",
    "    print(expression)\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        print(match)\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        print(span)\n",
    "        # This is a Span object or None if match doesn't map to valid token sequence\n",
    "        if span is not None:\n",
    "            print(\"Found match:\", span.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.char_span(13872, 13884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(s)\n",
    "    for t in s:\n",
    "        print(t)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    if t.like_num:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fix_deid_tokens('[**123**] blah blah blah', nlp('[**123**] blah blah blah'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pkl.load(open('./data/train_n2c2_datasets_preprocessed.pkl', 'rb'))\n",
    "n_sents = pkl.load(open('./data/train_n2c2_datasets_sentences_preprocessed.pkl', 'rb'))\n",
    "n_words = pkl.load(open('./data/train_n2c2_datasets_tokenized_preprocessed.pkl', 'rb'))\n",
    "n_words = {str(el[0]): el[1] for el in n_words}\n",
    "n_sents = {el[0]: el[1] for el in n_sents}\n",
    "with open('./data/train_newk_to_oldk.csv', 'r') as f:\n",
    "    rd = csv.reader(f)\n",
    "    next(rd)\n",
    "    new_to_old = {row[0]: (row[1], row[2]) for row in rd} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ch = {}\n",
    "for val in new_to_old.values():\n",
    "    if val[1] in count_ch:\n",
    "        count_ch[val[1]] += 1\n",
    "    else:\n",
    "        count_ch[val[1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-note redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_redundancy = namedtuple('wn_redundancy', ['note_id', 'nr_score', 'counts', 'challenge'])\n",
    "wn_list = pkl.load(open('./data/train_wn_redundancy.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_dic = {'note_id': [], 'nr_score': [], \n",
    "          'counts': [], 'challenge': []}\n",
    "for el in wn_list:\n",
    "    if el.note_id not in [\"75\", \"653\", \"851447\", \"330501671\", \"103377\", \"197869\", \"102937\", \"328::02\", \"903_1\", \"333::02\", \"963\", \"343\", \"354_1\", \"915_1\", \"1080\"]:\n",
    "        wn_dic['note_id'].append(el.note_id)\n",
    "        wn_dic['nr_score'].append(el.nr_score)\n",
    "        wn_dic['counts'].append(el.counts)\n",
    "        wn_dic['challenge'].append(el.challenge)\n",
    "    \n",
    "wn_dt = pd.DataFrame(wn_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sen = []\n",
    "c_rep = []\n",
    "ch_counts = {}\n",
    "ch_values = {}\n",
    "for _, row in wn_dt.iterrows():\n",
    "    c_sen.append(len(row.counts))\n",
    "    c_rep.extend([val for val in row.counts.values()])\n",
    "    if row.challenge in ch_counts:\n",
    "        ch_counts[row.challenge].append(len(row.counts))\n",
    "    else:\n",
    "        ch_counts[row.challenge] = [len(row.counts)]\n",
    "    if row.challenge in ch_values:\n",
    "        ch_values[row.challenge].extend([val for val in row.counts.values()])\n",
    "    else:\n",
    "        ch_values[row.challenge] = list(row.counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"<u>Number of notes with repeated sequences by challenge:\"))\n",
    "# Number of notes (with percentages) that include repeated sentences (>=2) by challenge group\n",
    "for ch, n in zip(wn_dt.challenge.value_counts().index, wn_dt.challenge.value_counts()):\n",
    "    display(Markdown(f\"**{ch}**: {int(n)} ({round(int(n)/count_ch[ch]*100, 2)}%)\"))\n",
    "print('\\n')\n",
    "\n",
    "# Overall number of repeated sentences (median [min; max])\n",
    "display(Markdown(f\"Overall median ([min; max]) number of repeated sentences: \"\n",
    "                 f\"{np.median(c_sen)} [{min(c_sen)}; {max(c_sen)}]\"))\n",
    "# Overall number of repetitions per sentence (median [min; max])\n",
    "display(Markdown(f\"Overall median ([min; max]) number of repetitions per sentence: \"\n",
    "                 f\"{np.median(c_rep)} [{min(c_rep)}; {max(c_rep)}]\"))\n",
    "print('\\n')\n",
    "display(Markdown(\"<u>Median [min; max] number of sentences by challenge:\"))\n",
    "# By challenge (median [min; max] number of repeated sentences and their frequency)\n",
    "for ch in ch_counts.keys():\n",
    "    display(Markdown(f\"**{ch}**: {np.median(ch_counts[ch])} [{min(ch_counts[ch])}; {max(ch_counts[ch])}]\"))\n",
    "display(Markdown(\"<u>Median [min; max] frequency by challenge:\"))\n",
    "for ch in ch_values.keys():\n",
    "    display(Markdown(f\"**{ch}**: {np.median(ch_values[ch])} [{min(ch_values[ch])}; {max(ch_values[ch])}]\"))\n",
    "\n",
    "print('\\n')\n",
    "display(Markdown(f\"**Overall number of notes:** {wn_dt.shape[0]} ({round(wn_dt.shape[0]/len(notes)*100, 2)}%)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the within-note redundancy in the challenge notes is caused by:\n",
    "1. C/P practice for some notes (longest);\n",
    "2. Addendums (miedium length);\n",
    "3. Override notes (medium length);\n",
    "4. Repeated constant text (lab, headers) (shortest).\n",
    "\n",
    "### Strategy\n",
    "Drop all repeated sentences starting from the bottom of the note and the beginning of the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between-note investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_redundancy = namedtuple('bn_redundancy', ['sen_A', 'sen_B', 'align_score'])\n",
    "bn_list = pkl.load(open('./data/train_bn_redundancy.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_scores = []\n",
    "bn_list_rid = {}\n",
    "for k, el in bn_list.items():\n",
    "    len_a = len([a for a in el.sen_A if a!='-'])\n",
    "    len_b = len([b for b in el.sen_B if b!='-'])\n",
    "    s_len = len_a + len_b\n",
    "    bn_scores.append(el.align_score/s_len)\n",
    "    if el.align_score > 0:\n",
    "        bn_list_rid[k] = el\n",
    "    \n",
    "display(Markdown(f\"Median alignment score ([min; max]): {round(np.median(bn_scores), 2)} [{round(min(bn_scores), 2)}; {round(max(bn_scores), 2)}]\"))\n",
    "display(Markdown(f\"Median alignment score ([min; max]): {round(np.mean(bn_scores), 2)} ({round(np.std(bn_scores), 2)})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [len(el.sen_A) for el in bn_list.values()] + [len(el.sen_B) for el in bn_list.values()]\n",
    "max_len = max(seq_len)\n",
    "\n",
    "df_list = [[k] + [0]*max_len for k in bn_list.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, el in enumerate(bn_list.values()):\n",
    "    i = 0\n",
    "    for a, b in zip(el.sen_A, el.sen_B):\n",
    "        try:\n",
    "            if a==b and (a!='-' and b!='-'):\n",
    "                df_list[n][i+1] = 2\n",
    "            elif a=='-' or b=='-':\n",
    "                if df_list[n][i] == 1.5:\n",
    "                    df_list[n][i+1] = 1.25\n",
    "                else:\n",
    "                    df_list[n][i+1] = 1.5\n",
    "            else:\n",
    "                df_list[n][i] == -10\n",
    "            i += 1\n",
    "        except IndexError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_list)\n",
    "df.columns = [\"NOTE_ID\"] + [f\"W_{i}\" for i in range(max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.NOTE_ID\n",
    "df.drop('NOTE_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myColors = ((0.8, 0.0, 0.0, 1.0), (0.0, 0.8, 0.0, 1.0), (0.0, 0.0, 0.8, 1.0))\n",
    "# cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n",
    "cmap = ['white', '#b3e6b3','#66cc66','#ffc299','#ff944d','#ff6600','#ccddff','#99bbff','#4d88ff','#0044cc','#002b80']\n",
    "\n",
    "htmp = sns.heatmap(df, xticklabels=False, yticklabels=False, cmap=cmap)\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alignment investigation suggests that no clinical note, among the longitudinal, is really copied-forward. It is possible that the amount of overlap is negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bn_scores, bins=100)\n",
    "plt.title(\"Standardized alignment scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Raw alignment scores\")\n",
    "plt.hist([el.align_score for el in bn_list.values()], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's stop here the investigation and implement the transformer. I'll get back to this when I figure out what I need to fine-tune the model (also in terms of challenge tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the notes that are highly aligned (score > 0.7)\n",
    "for k, el in bn_list.items():\n",
    "    if el.align_score/len(el.sen_A) >= 0.7:\n",
    "        print(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_note = bn_list['292::1::3']\n",
    "for a, b in zip(chk_note.sen_A, chk_note.sen_B):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dict['292::01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dict['292::03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "# nlp = spacy.load('en_core_sci_md', disable=['ner'])\n",
    "doc = nlp('2094-03-14 ciao ciao. ciao ciao')\n",
    "\n",
    "print(nltk.tokenize.word_tokenize('2094-03-14'))\n",
    "print([t for t in s for s in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(nltk.tokenize.word_tokenize(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dict = {el[0]: el[1] for el in notes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dict['292::01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = sorted(bn_scores)[::-1][:3]\n",
    "for el in bn_list:\n",
    "    if el[1][-1]/len(el[1][0]) == max_val[1]:\n",
    "        doc1 = el[1][1]\n",
    "        doc2 = el[1][2]\n",
    "        print(f\"Score: {el[1][-1]/len(el[1][0])} -- Notes {el[0]} {el[1][0]}\")\n",
    "        for w1, w2 in zip(doc1, doc2):\n",
    "            print(w1, w2)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
